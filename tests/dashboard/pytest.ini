[pytest]
# Pytest Configuration for Dashboard Tests
# =========================================

# Test discovery patterns
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Test paths
testpaths = tests/dashboard

# Markers for test categorization
markers =
    unit: Unit tests for individual components
    integration: Integration tests for component workflows
    performance: Performance and benchmark tests
    visualization: Visualization rendering and data tests
    alerts: Alert engine and notification tests
    comparison: Store comparison and ranking tests
    forecast: Forecast model evaluation tests
    slow: Tests that take significant time to run

# Output options
addopts =
    -v
    --strict-markers
    --tb=short
    --cov=src/dashboard
    --cov-report=html
    --cov-report=term-missing
    --cov-report=xml
    --cov-branch
    --maxfail=1
    -ra

# Coverage options
[coverage:run]
source = src/dashboard
omit =
    */tests/*
    */test_*.py
    */__pycache__/*
    */venv/*
    */virtualenv/*

[coverage:report]
precision = 2
show_missing = True
skip_covered = False

# Ignore warnings
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::FutureWarning

# Logging
log_cli = false
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

# Timeout for tests (in seconds)
timeout = 300

# Parallel execution
# Use -n auto for automatic worker count
# pytest -n auto
