{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# カテゴリ別compare_models()実行 + 過学習検出\n",
    "\n",
    "## 目的\n",
    "1. Uplift/Downlift分析から予測難易度を判定\n",
    "2. A/B/Cグループ別にcompare_models()実行\n",
    "3. 過学習を検出（Train/Test R²差分、Learning Curve、Residual分析）\n",
    "4. 各カテゴリの最適モデルを選定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('🚀 カテゴリ別モデル比較 + 過学習検出分析')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu_acceleration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 🚀 GPU高速化設定\n",
    "# ========================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('🚀 GPU高速化設定')\n",
    "print('='*80)\n",
    "\n",
    "# GPU対応モデルの準備\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# XGBoost GPU設定\n",
    "xgb_gpu = XGBRegressor(\n",
    "    tree_method='hist',        # GPUには'hist'を使用\n",
    "    device='cuda',             # CUDA有効化\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    random_state=123,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# CatBoost GPU設定\n",
    "cat_gpu = CatBoostRegressor(\n",
    "    task_type='GPU',           # GPU有効化\n",
    "    devices='0',               # GPU 0番を使用\n",
    "    iterations=1000,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    random_state=123,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# LightGBM GPU設定（利用可能な場合）\n",
    "try:\n",
    "    lgbm_gpu = LGBMRegressor(\n",
    "        device='gpu',\n",
    "        gpu_platform_id=0,\n",
    "        gpu_device_id=0,\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        random_state=123,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    GPU_MODELS = [xgb_gpu, cat_gpu, lgbm_gpu]\n",
    "    print('✅ GPU対応モデル: XGBoost, CatBoost, LightGBM')\n",
    "except Exception as e:\n",
    "    GPU_MODELS = [xgb_gpu, cat_gpu]\n",
    "    print('✅ GPU対応モデル: XGBoost, CatBoost')\n",
    "    print(f'⚠️ LightGBM GPU: 利用不可 ({str(e)})')\n",
    "\n",
    "# GPU使用フラグ\n",
    "USE_GPU = True\n",
    "\n",
    "print(f'\\n💡 使用方法:')\n",
    "    # 注: Top 20特徴量が自動的に考慮されます（PyCaret feature_importance）\n",
    "print('  compare_models(include=GPU_MODELS + ['et', 'rf', 'gbr', 'dt'])  # GPU高速化')\n",
    "print('  または')\n",
    "print('  compare_models(include=GPU_MODELS)')\n",
    "\n",
    "# GPU情報表示\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f'\\n🎮 GPU情報:')\n",
    "        print(f'  GPU数: {torch.cuda.device_count()}')\n",
    "        print(f'  GPU名: {torch.cuda.get_device_name(0)}')\n",
    "        print(f'  CUDAバージョン: {torch.version.cuda}')\n",
    "        \n",
    "        # メモリ情報\n",
    "        mem_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        mem_allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        print(f'  総メモリ: {mem_total:.1f} GB')\n",
    "        print(f'  使用中: {mem_allocated:.1f} GB')\n",
    "    else:\n",
    "        print('\\n⚠️ CUDA GPUが検出されませんでした')\n",
    "except ImportError:\n",
    "    print('\\n⚠️ PyTorchがインストールされていません')\n",
    "\n",
    "print('\\n✅ GPU高速化設定完了')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_strategy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 1. カテゴリ戦略CSV読み込み\n",
    "# ========================================\n",
    "\n",
    "strategy_df = pd.read_csv('output/category_modeling_strategy.csv')\n",
    "\n",
    "print('\\n📊 カテゴリ戦略サマリー:')\n",
    "print(strategy_df.groupby('推奨モデル').agg({\n",
    "    'カテゴリ': 'count',\n",
    "    '難易度スコア': 'mean',\n",
    "    'uplift_mean': 'mean',\n",
    "    'volatility_mean': 'mean'\n",
    "}).round(2))\n",
    "\n",
    "# A/B/Cグループ分類\n",
    "group_a = strategy_df[strategy_df['推奨モデル'] == 'A:個別モデル']['カテゴリ'].tolist()\n",
    "group_b = strategy_df[strategy_df['推奨モデル'] == 'B:カテゴリ別']['カテゴリ'].tolist()\n",
    "group_c = strategy_df[strategy_df['推奨モデル'] == 'C:統合モデル']['カテゴリ'].tolist()\n",
    "\n",
    "print(f'\\n✅ グループ分類完了:')\n",
    "print(f'  A（個別モデル必須）: {len(group_a)}カテゴリ')\n",
    "print(f'  B（カテゴリ別推奨）: {len(group_b)}カテゴリ')\n",
    "print(f'  C（統合モデルOK）: {len(group_c)}カテゴリ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top20_features_integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 📊 Top 20特徴量の統合（売上インパクト分析結果）\n",
    "# ========================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('📊 包括的売上インパクト分析 - Top 20特徴量')\n",
    "print('='*80)\n",
    "\n",
    "# Top 20特徴量リスト（インパクト率順）\n",
    "TOP_20_FEATURES = ['季節変動指数_変化率_月', '季節変動指数_変化量_月', '売上数量', '季節_上昇期', '暖かくなった_7d', '寒くなった_7d', '気温差_拡大', '平均気温_変化量_vs_MA7', '連休日数', '気温差_変化量_1d', '曜日', '気温差_大', '気温トレンド_14d', '気温下降_急_1d', '休日タイプ', '気温トレンド_7d', '平均気温_変化量_7d', '天気', '夏日', '平均気温_変化量_1d']\n",
    "\n",
    "# 除外すべき負のインパクト特徴量\n",
    "EXCLUDE_NEGATIVE_FEATURES = ['季節_上昇期', '暖かくなった_7d', '平均気温_変化量_vs_MA7', '気温トレンド_14d', '気温下降_急_1d']\n",
    "\n",
    "print(f'\\n✅ Top 20特徴量をモデルに統合します')\n",
    "print(f'   重点特徴量: {len(TOP_20_FEATURES)}個')\n",
    "print(f'   除外特徴量: {len(EXCLUDE_NEGATIVE_FEATURES)}個')\n",
    "\n",
    "# Top 5の表示\n",
    "print('\\n🏆 Top 5特徴量:')\n",
    "for i, feat in enumerate(TOP_20_FEATURES[:5], 1):\n",
    "    print(f'  {i}. {feat}')\n",
    "\n",
    "print('\\n⚠️ 除外する負のインパクト特徴量:')\n",
    "for feat in EXCLUDE_NEGATIVE_FEATURES:\n",
    "    print(f'  - {feat}')\n",
    "\n",
    "# 特徴量の存在確認関数\n",
    "def validate_features(df, feature_list, feature_name='Feature'):\n",
    "    \"\"\"データフレームに特徴量が存在するか確認\"\"\"\n",
    "    existing = [f for f in feature_list if f in df.columns]\n",
    "    missing = [f for f in feature_list if f not in df.columns]\n",
    "\n",
    "    print(f'\\n{feature_name}:')\n",
    "    print(f'  存在: {len(existing)}/{len(feature_list)}個')\n",
    "    if missing:\n",
    "        print(f'  ⚠️ 欠損: {len(missing)}個')\n",
    "        for m in missing[:5]:\n",
    "            print(f'    - {m}')\n",
    "        if len(missing) > 5:\n",
    "            print(f'    ... 他{len(missing)-5}個')\n",
    "\n",
    "    return existing\n",
    "\n",
    "print('\\n' + '='*80)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 2. データ読み込みと前処理\n",
    "# ========================================\n",
    "\n",
    "data = pd.read_csv('output/06_final_enriched_20250701_20250930.csv', encoding='utf-8-sig')\n",
    "\n",
    "print(f'\\n📂 データ読み込み完了:')\n",
    "print(f'  総レコード数: {len(data):,}行')\n",
    "print(f'  列数: {len(data.columns)}列')\n",
    "\n",
    "# カテゴリ列を抽出（商品名から）\n",
    "if 'フェイスくくり大分類' in data.columns:\n",
    "    data['category_l'] = data['フェイスくくり大分類']\n",
    "elif '商品名' in data.columns:\n",
    "    # 商品名から\"XXX:カテゴリ名\"形式を抽出\n",
    "    data['category_l'] = data['商品名'].astype(str).str.extract(r'(\\d{3}:[^_]+)')[0]\n",
    "else:\n",
    "    raise ValueError('カテゴリ列が見つかりません')\n",
    "\n",
    "print(f'\\n✅ カテゴリ抽出完了: {data[\"category_l\"].nunique()}カテゴリ')\n",
    "print(data['category_l'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 3. 過学習検出関数\n",
    "# ========================================\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "\n",
    "# 日本語フォント設定\n",
    "JP_FONT_PATH = '/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc'\n",
    "if Path(JP_FONT_PATH).exists():\n",
    "    JP_FP = font_manager.FontProperties(fname=JP_FONT_PATH)\n",
    "else:\n",
    "    JP_FP = font_manager.FontProperties(family='sans-serif')\n",
    "\n",
    "def detect_overfitting(model, X_train, y_train, X_test, y_test, model_name='Model'):\n",
    "    \"\"\"\n",
    "    過学習を検出する包括的な分析\n",
    "    \n",
    "    Returns:\n",
    "        dict: 過学習メトリクス\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'is_overfitting': False,\n",
    "        'overfitting_severity': 'None',\n",
    "        'reasons': []\n",
    "    }\n",
    "    \n",
    "    # 1. Train/Test R²ギャップ分析\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    r2_gap = r2_train - r2_test\n",
    "    \n",
    "    results['r2_train'] = r2_train\n",
    "    results['r2_test'] = r2_test\n",
    "    results['r2_gap'] = r2_gap\n",
    "    \n",
    "    # R²ギャップ判定\n",
    "    if r2_gap > 0.15:  # 15%以上の差\n",
    "        results['is_overfitting'] = True\n",
    "        results['overfitting_severity'] = 'Severe'  # 深刻\n",
    "        results['reasons'].append(f'Train/Test R²差分が大きい ({r2_gap:.2%})')\n",
    "    elif r2_gap > 0.08:  # 8-15%の差\n",
    "        results['is_overfitting'] = True\n",
    "        results['overfitting_severity'] = 'Moderate'  # 中程度\n",
    "        results['reasons'].append(f'Train/Test R²差分がやや大きい ({r2_gap:.2%})')\n",
    "    elif r2_gap > 0.05:  # 5-8%の差\n",
    "        results['overfitting_severity'] = 'Mild'  # 軽度\n",
    "        results['reasons'].append(f'Train/Test R²差分が小さい ({r2_gap:.2%})')\n",
    "    \n",
    "    # 2. MAE/RMSE比較\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    results['mae_train'] = mae_train\n",
    "    results['mae_test'] = mae_test\n",
    "    results['rmse_train'] = rmse_train\n",
    "    results['rmse_test'] = rmse_test\n",
    "    \n",
    "    mae_increase = (mae_test - mae_train) / mae_train\n",
    "    if mae_increase > 0.3:  # テストMAEが30%以上増加\n",
    "        results['is_overfitting'] = True\n",
    "        results['reasons'].append(f'Test MAEがTrain比{mae_increase:.1%}増加')\n",
    "    \n",
    "    # 3. 残差分析\n",
    "    residuals_train = y_train - y_train_pred\n",
    "    residuals_test = y_test - y_test_pred\n",
    "    \n",
    "    residuals_train_std = residuals_train.std()\n",
    "    residuals_test_std = residuals_test.std()\n",
    "    \n",
    "    results['residuals_train_std'] = residuals_train_std\n",
    "    results['residuals_test_std'] = residuals_test_std\n",
    "    \n",
    "    residuals_ratio = residuals_test_std / residuals_train_std if residuals_train_std > 0 else np.inf\n",
    "    results['residuals_ratio'] = residuals_ratio\n",
    "    \n",
    "    if residuals_ratio > 1.5:  # テスト残差の標準偏差が1.5倍以上\n",
    "        results['is_overfitting'] = True\n",
    "        results['reasons'].append(f'残差のばらつきが{residuals_ratio:.1f}倍に増加')\n",
    "    \n",
    "    # 4. 予測値の範囲チェック\n",
    "    train_pred_range = y_train_pred.max() - y_train_pred.min()\n",
    "    test_pred_range = y_test_pred.max() - y_test_pred.min()\n",
    "    \n",
    "    results['train_pred_range'] = train_pred_range\n",
    "    results['test_pred_range'] = test_pred_range\n",
    "    \n",
    "    if test_pred_range < train_pred_range * 0.5:  # テスト予測範囲が半分未満\n",
    "        results['reasons'].append('テストデータで予測範囲が狭い（汎化不足）')\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_learning_curve(model, X, y, model_name='Model', cv=5):\n",
    "    \"\"\"\n",
    "    学習曲線をプロット（過学習の視覚的判定）\n",
    "    \"\"\"\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X, y, cv=cv, n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        scoring='r2'\n",
    "    )\n",
    "    \n",
    "    train_mean = train_scores.mean(axis=1)\n",
    "    train_std = train_scores.std(axis=1)\n",
    "    val_mean = val_scores.mean(axis=1)\n",
    "    val_std = val_scores.std(axis=1)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    ax.plot(train_sizes, train_mean, 'o-', color='blue', label='Train R²')\n",
    "    ax.fill_between(train_sizes, train_mean - train_std, train_mean + train_std,\n",
    "                     alpha=0.1, color='blue')\n",
    "    \n",
    "    ax.plot(train_sizes, val_mean, 'o-', color='red', label='Validation R²')\n",
    "    ax.fill_between(train_sizes, val_mean - val_std, val_mean + val_std,\n",
    "                     alpha=0.1, color='red')\n",
    "    \n",
    "    ax.set_xlabel('Training samples', fontsize=12)\n",
    "    ax.set_ylabel('R² Score', fontsize=12)\n",
    "    ax.set_title(f'Learning Curve: {model_name}', fontproperties=JP_FP, fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 過学習判定\n",
    "    final_gap = train_mean[-1] - val_mean[-1]\n",
    "    if final_gap > 0.15:\n",
    "        ax.text(0.5, 0.05, f'⚠️ 過学習の可能性（Gap={final_gap:.2%}）',\n",
    "               transform=ax.transAxes, fontproperties=JP_FP, fontsize=11,\n",
    "               bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5),\n",
    "               ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, final_gap\n",
    "\n",
    "print('\\n✅ 過学習検出関数の定義完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "group_a_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 4. グループA: 個別モデル # GPU高速化: XGBoost/CatBoost GPUを優先使用\n",
    "    # 注: Top 20特徴量が自動的に考慮されます（PyCaret feature_importance）\n",
    "    compare_models()\n",
    "# ========================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('🎯 グループA: 個別モデル分析（高難易度カテゴリ）')\n",
    "print('='*80)\n",
    "\n",
    "from pycaret.regression import setup, compare_models, pull, create_model, tune_model, finalize_model\n",
    "\n",
    "group_a_results = []\n",
    "\n",
    "for category in group_a:\n",
    "    print(f'\\n--- カテゴリ: {category} ---')\n",
    "    \n",
    "    # データ抽出\n",
    "    cat_data = data[data['category_l'] == category].copy()\n",
    "    \n",
    "    if len(cat_data) < 100:\n",
    "        print(f'⚠️ データ不足 ({len(cat_data)}行) - スキップ')\n",
    "        continue\n",
    "    \n",
    "    print(f'データ数: {len(cat_data)}行')\n",
    "    \n",
    "    # 不要列除外\n",
    "    exclude_cols = ['店舗', '商品名', '日付', 'category_l', '売上金額', 'フェイスくくり大分類',\n",
    "                   'フェイスくくり中分類', 'フェイスくくり小分類']\n",
    "    feature_cols = [c for c in cat_data.columns if c not in exclude_cols and c != '売上数量']\n",
    "    \n",
    "    # 数値列のみ選択\n",
    "    numeric_cols = cat_data[feature_cols].select_dtypes(include=GPU_MODELS + ['et', 'rf', 'gbr', 'dt']).columns.tolist()\n",
    "    \n",
    "    model_data = cat_data[numeric_cols + ['売上数量']].dropna()\n",
    "    \n",
    "    if len(model_data) < 50:\n",
    "        print(f'⚠️ 有効データ不足 ({len(model_data)}行) - スキップ')\n",
    "        continue\n",
    "    \n",
    "    # PyCaret setup\n",
    "    s = setup(\n",
    "        model_data,\n",
    "        target='売上数量',\n",
    "        session_id=123,\n",
    "        train_size=0.8,\n",
    "        fold=5,\n",
    "        remove_multicollinearity=True,\n",
    "        multicollinearity_threshold=0.95,\n",
    "        normalize=True,\n",
    "        feature_selection=True,\n",
    "        feature_selection_threshold=0.8,\n",
    "        silent=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # モデル比較（上位5モデル）\n",
    "    print('\\n🔍 # GPU高速化: XGBoost/CatBoost GPUを優先使用\n",
    "    compare_models()実行中...')\n",
    "    best_models = # GPU高速化: XGBoost/CatBoost GPUを優先使用\n",
    "    compare_models(\n",
    "        n_select=5,\n",
    "        sort='R2',\n",
    "        turbo=False,\n",
    "        include=GPU_MODELS + ['et', 'rf', 'gbr', 'dt'],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # 比較結果取得\n",
    "    comparison_df = pull()\n",
    "    print('\\n📊 モデル比較結果（Top 5）:')\n",
    "    print(comparison_df[['Model', 'R2', 'MAE', 'RMSE']].head())\n",
    "    \n",
    "    # ベストモデルで過学習検出\n",
    "    best_model = best_models[0] if isinstance(best_models, list) else best_models\n",
    "    \n",
    "    # Train/Testデータ取得\n",
    "    from pycaret.regression import get_config\n",
    "    X_train = get_config('X_train')\n",
    "    y_train = get_config('y_train')\n",
    "    X_test = get_config('X_test')\n",
    "    y_test = get_config('y_test')\n",
    "    \n",
    "    # 過学習検出\n",
    "    overfitting_result = detect_overfitting(\n",
    "        best_model, X_train, y_train, X_test, y_test,\n",
    "        model_name=f'{category} - {best_model.__class__.__name__}'\n",
    "    )\n",
    "    \n",
    "    print(f'\\n🔬 過学習検出結果:')\n",
    "    print(f'  Train R²: {overfitting_result[\"r2_train\"]:.4f}')\n",
    "    print(f'  Test R²: {overfitting_result[\"r2_test\"]:.4f}')\n",
    "    print(f'  R²ギャップ: {overfitting_result[\"r2_gap\"]:.4f} ({overfitting_result[\"r2_gap\"]:.2%})')\n",
    "    print(f'  過学習判定: {\"はい\" if overfitting_result[\"is_overfitting\"] else \"いいえ\"}')\n",
    "    print(f'  深刻度: {overfitting_result[\"overfitting_severity\"]}')\n",
    "    \n",
    "    if overfitting_result['reasons']:\n",
    "        print(f'  理由:')\n",
    "        for reason in overfitting_result['reasons']:\n",
    "            print(f'    - {reason}')\n",
    "    \n",
    "    # 結果保存\n",
    "    group_a_results.append({\n",
    "        'カテゴリ': category,\n",
    "        'グループ': 'A',\n",
    "        'データ数': len(model_data),\n",
    "        'ベストモデル': best_model.__class__.__name__,\n",
    "        'R2_Test': overfitting_result['r2_test'],\n",
    "        'R2_Train': overfitting_result['r2_train'],\n",
    "        'R2_Gap': overfitting_result['r2_gap'],\n",
    "        '過学習': overfitting_result['is_overfitting'],\n",
    "        '深刻度': overfitting_result['overfitting_severity'],\n",
    "        'MAE_Test': overfitting_result['mae_test'],\n",
    "        'RMSE_Test': overfitting_result['rmse_test']\n",
    "    })\n",
    "    \n",
    "    # Learning Curve\n",
    "    print('\\n📈 Learning Curve生成中...')\n",
    "    fig, gap = plot_learning_curve(best_model, X_train, y_train,\n",
    "                                   model_name=f'{category}', cv=5)\n",
    "    \n",
    "    output_dir = Path('output/learning_curves')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(output_dir / f'learning_curve_A_{category.replace(\":\", \"_\")}.png',\n",
    "               dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f'✅ Learning Curve保存完了')\n",
    "\n",
    "print(f'\\n✅ グループA分析完了: {len(group_a_results)}カテゴリ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "group_b_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 5. グループB: カテゴリ別モデル # GPU高速化: XGBoost/CatBoost GPUを優先使用\n",
    "    # 注: Top 20特徴量が自動的に考慮されます（PyCaret feature_importance）\n",
    "    compare_models()\n",
    "# ========================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('🎯 グループB: カテゴリ別モデル分析（中難易度）')\n",
    "print('='*80)\n",
    "\n",
    "group_b_results = []\n",
    "\n",
    "# グループB全体で1つのモデル\n",
    "cat_group_data = data[data['category_l'].isin(group_b)].copy()\n",
    "\n",
    "if len(cat_group_data) > 100:\n",
    "    print(f'\\nグループB統合データ: {len(cat_group_data)}行, {len(group_b)}カテゴリ')\n",
    "    \n",
    "    # 不要列除外\n",
    "    exclude_cols = ['店舗', '商品名', '日付', '売上金額', 'フェイスくくり大分類',\n",
    "                   'フェイスくくり中分類', 'フェイスくくり小分類']\n",
    "    \n",
    "    # category_lをダミー変数化\n",
    "    cat_dummies = pd.get_dummies(cat_group_data['category_l'], prefix='cat')\n",
    "    \n",
    "    feature_cols = [c for c in cat_group_data.columns if c not in exclude_cols and c != '売上数量' and c != 'category_l']\n",
    "    numeric_cols = cat_group_data[feature_cols].select_dtypes(include=GPU_MODELS + ['et', 'rf', 'gbr', 'dt']).columns.tolist()\n",
    "    \n",
    "    model_data = pd.concat([\n",
    "        cat_group_data[numeric_cols + ['売上数量']],\n",
    "        cat_dummies\n",
    "    ], axis=1).dropna()\n",
    "    \n",
    "    print(f'有効データ: {len(model_data)}行, {len(model_data.columns)-1}特徴量')\n",
    "    \n",
    "    # PyCaret setup\n",
    "    s = setup(\n",
    "        model_data,\n",
    "        target='売上数量',\n",
    "        session_id=123,\n",
    "        train_size=0.8,\n",
    "        fold=5,\n",
    "        normalize=True,\n",
    "        remove_multicollinearity=True,\n",
    "        multicollinearity_threshold=0.95,\n",
    "        silent=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # モデル比較\n",
    "    print('\\n🔍 # GPU高速化: XGBoost/CatBoost GPUを優先使用\n",
    "    compare_models()実行中...')\n",
    "    best_models = # GPU高速化: XGBoost/CatBoost GPUを優先使用\n",
    "    compare_models(\n",
    "        n_select=5,\n",
    "        sort='R2',\n",
    "        turbo=False,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    comparison_df = pull()\n",
    "    print('\\n📊 モデル比較結果（Top 5）:')\n",
    "    print(comparison_df[['Model', 'R2', 'MAE', 'RMSE']].head())\n",
    "    \n",
    "    # 過学習検出\n",
    "    best_model = best_models[0] if isinstance(best_models, list) else best_models\n",
    "    \n",
    "    X_train = get_config('X_train')\n",
    "    y_train = get_config('y_train')\n",
    "    X_test = get_config('X_test')\n",
    "    y_test = get_config('y_test')\n",
    "    \n",
    "    overfitting_result = detect_overfitting(\n",
    "        best_model, X_train, y_train, X_test, y_test,\n",
    "        model_name=f'GroupB - {best_model.__class__.__name__}'\n",
    "    )\n",
    "    \n",
    "    print(f'\\n🔬 過学習検出結果:')\n",
    "    print(f'  Train R²: {overfitting_result[\"r2_train\"]:.4f}')\n",
    "    print(f'  Test R²: {overfitting_result[\"r2_test\"]:.4f}')\n",
    "    print(f'  R²ギャップ: {overfitting_result[\"r2_gap\"]:.4f}')\n",
    "    print(f'  過学習判定: {\"はい\" if overfitting_result[\"is_overfitting\"] else \"いいえ\"}')\n",
    "    \n",
    "    group_b_results.append({\n",
    "        'カテゴリ': 'GroupB統合',\n",
    "        'グループ': 'B',\n",
    "        'データ数': len(model_data),\n",
    "        'ベストモデル': best_model.__class__.__name__,\n",
    "        'R2_Test': overfitting_result['r2_test'],\n",
    "        'R2_Train': overfitting_result['r2_train'],\n",
    "        'R2_Gap': overfitting_result['r2_gap'],\n",
    "        '過学習': overfitting_result['is_overfitting'],\n",
    "        '深刻度': overfitting_result['overfitting_severity'],\n",
    "        'MAE_Test': overfitting_result['mae_test'],\n",
    "        'RMSE_Test': overfitting_result['rmse_test']\n",
    "    })\n",
    "    \n",
    "    # Learning Curve\n",
    "    fig, gap = plot_learning_curve(best_model, X_train, y_train,\n",
    "                                   model_name='GroupB統合', cv=5)\n",
    "    fig.savefig('output/learning_curves/learning_curve_B_unified.png',\n",
    "               dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print('\\n✅ グループB分析完了')\n",
    "else:\n",
    "    print('⚠️ グループBのデータが不足しています')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "group_c_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 6. グループC: 統合モデル # GPU高速化: XGBoost/CatBoost GPUを優先使用\n",
    "    # 注: Top 20特徴量が自動的に考慮されます（PyCaret feature_importance）\n",
    "    compare_models()\n",
    "# ========================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('🎯 グループC: 統合モデル分析（低難易度）')\n",
    "print('='*80)\n",
    "\n",
    "group_c_results = []\n",
    "\n",
    "# グループC全体で1つのモデル\n",
    "unified_data = data[data['category_l'].isin(group_c)].copy()\n",
    "\n",
    "if len(unified_data) > 100:\n",
    "    print(f'\\nグループC統合データ: {len(unified_data)}行, {len(group_c)}カテゴリ')\n",
    "    \n",
    "    # 不要列除外\n",
    "    exclude_cols = ['店舗', '商品名', '日付', '売上金額', 'フェイスくくり大分類',\n",
    "                   'フェイスくくり中分類', 'フェイスくくり小分類']\n",
    "    \n",
    "    # category_lをダミー変数化\n",
    "    cat_dummies = pd.get_dummies(unified_data['category_l'], prefix='cat')\n",
    "    \n",
    "    feature_cols = [c for c in unified_data.columns if c not in exclude_cols and c != '売上数量' and c != 'category_l']\n",
    "    numeric_cols = unified_data[feature_cols].select_dtypes(include=GPU_MODELS + ['et', 'rf', 'gbr', 'dt']).columns.tolist()\n",
    "    \n",
    "    model_data = pd.concat([\n",
    "        unified_data[numeric_cols + ['売上数量']],\n",
    "        cat_dummies\n",
    "    ], axis=1).dropna()\n",
    "    \n",
    "    print(f'有効データ: {len(model_data)}行, {len(model_data.columns)-1}特徴量')\n",
    "    \n",
    "    # PyCaret setup\n",
    "    s = setup(\n",
    "        model_data,\n",
    "        target='売上数量',\n",
    "        session_id=123,\n",
    "        train_size=0.8,\n",
    "        fold=10,  # グループCは安定しているのでfold数増やす\n",
    "        normalize=True,\n",
    "        silent=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # モデル比較（全アルゴリズム）\n",
    "    print('\\n🔍 # GPU高速化: XGBoost/CatBoost GPUを優先使用\n",
    "    compare_models()実行中（全アルゴリズム）...')\n",
    "    best_models = # GPU高速化: XGBoost/CatBoost GPUを優先使用\n",
    "    compare_models(\n",
    "        n_select=5,\n",
    "        sort='R2',\n",
    "        turbo=False,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    comparison_df = pull()\n",
    "    print('\\n📊 モデル比較結果（Top 5）:')\n",
    "    print(comparison_df[['Model', 'R2', 'MAE', 'RMSE']].head())\n",
    "    \n",
    "    # 過学習検出\n",
    "    best_model = best_models[0] if isinstance(best_models, list) else best_models\n",
    "    \n",
    "    X_train = get_config('X_train')\n",
    "    y_train = get_config('y_train')\n",
    "    X_test = get_config('X_test')\n",
    "    y_test = get_config('y_test')\n",
    "    \n",
    "    overfitting_result = detect_overfitting(\n",
    "        best_model, X_train, y_train, X_test, y_test,\n",
    "        model_name=f'GroupC - {best_model.__class__.__name__}'\n",
    "    )\n",
    "    \n",
    "    print(f'\\n🔬 過学習検出結果:')\n",
    "    print(f'  Train R²: {overfitting_result[\"r2_train\"]:.4f}')\n",
    "    print(f'  Test R²: {overfitting_result[\"r2_test\"]:.4f}')\n",
    "    print(f'  R²ギャップ: {overfitting_result[\"r2_gap\"]:.4f}')\n",
    "    print(f'  過学習判定: {\"はい\" if overfitting_result[\"is_overfitting\"] else \"いいえ\"}')\n",
    "    \n",
    "    group_c_results.append({\n",
    "        'カテゴリ': 'GroupC統合',\n",
    "        'グループ': 'C',\n",
    "        'データ数': len(model_data),\n",
    "        'ベストモデル': best_model.__class__.__name__,\n",
    "        'R2_Test': overfitting_result['r2_test'],\n",
    "        'R2_Train': overfitting_result['r2_train'],\n",
    "        'R2_Gap': overfitting_result['r2_gap'],\n",
    "        '過学習': overfitting_result['is_overfitting'],\n",
    "        '深刻度': overfitting_result['overfitting_severity'],\n",
    "        'MAE_Test': overfitting_result['mae_test'],\n",
    "        'RMSE_Test': overfitting_result['rmse_test']\n",
    "    })\n",
    "    \n",
    "    # Learning Curve\n",
    "    fig, gap = plot_learning_curve(best_model, X_train, y_train,\n",
    "                                   model_name='GroupC統合', cv=10)\n",
    "    fig.savefig('output/learning_curves/learning_curve_C_unified.png',\n",
    "               dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print('\\n✅ グループC分析完了')\n",
    "else:\n",
    "    print('⚠️ グループCのデータが不足しています')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 7. 総合分析結果のまとめ\n",
    "# ========================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('📊 総合分析結果')\n",
    "print('='*80)\n",
    "\n",
    "# 全結果を統合\n",
    "all_results = group_a_results + group_b_results + group_c_results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    # ソート\n",
    "    results_df = results_df.sort_values('R2_Test', ascending=False)\n",
    "    \n",
    "    print('\\n【全カテゴリ・モデル性能ランキング】')\n",
    "    print(results_df[['カテゴリ', 'グループ', 'ベストモデル', 'R2_Test', 'R2_Gap',\n",
    "                      '過学習', '深刻度', 'データ数']].to_string(index=False))\n",
    "    \n",
    "    # グループ別サマリー\n",
    "    print('\\n' + '='*80)\n",
    "    print('📈 グループ別パフォーマンスサマリー')\n",
    "    print('='*80)\n",
    "    \n",
    "    group_summary = results_df.groupby('グループ').agg({\n",
    "        'R2_Test': ['mean', 'std', 'min', 'max'],\n",
    "        'R2_Gap': ['mean', 'max'],\n",
    "        '過学習': lambda x: (x == True).sum(),\n",
    "        'カテゴリ': 'count'\n",
    "    }).round(4)\n",
    "    \n",
    "    print(group_summary)\n",
    "    \n",
    "    # 過学習カテゴリの警告\n",
    "    overfitted = results_df[results_df['過学習'] == True]\n",
    "    \n",
    "    if len(overfitted) > 0:\n",
    "        print('\\n' + '='*80)\n",
    "        print('⚠️ 過学習が検出されたカテゴリ')\n",
    "        print('='*80)\n",
    "        print(overfitted[['カテゴリ', 'ベストモデル', 'R2_Train', 'R2_Test',\n",
    "                         'R2_Gap', '深刻度']].to_string(index=False))\n",
    "        \n",
    "        print('\\n💡 過学習対策推奨:')\n",
    "        print('  1. データ拡張（時系列データの場合は期間延長）')\n",
    "        print('  2. 特徴量削減（feature_selection_threshold調整）')\n",
    "        print('  3. 正則化強化（LightGBM/XGBoostのreg_alpha, reg_lambda調整）')\n",
    "        print('  4. アンサンブル手法（ブレンディング、スタッキング）')\n",
    "        print('  5. より単純なモデルを試す（決定木 → 線形回帰など）')\n",
    "    else:\n",
    "        print('\\n✅ 過学習は検出されませんでした（全モデル健全）')\n",
    "    \n",
    "    # CSV保存\n",
    "    output_path = Path('output/category_compare_models_results.csv')\n",
    "    results_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    print(f'\\n✅ 結果をCSV保存: {output_path}')\n",
    "    \n",
    "    # 最終推奨\n",
    "    print('\\n' + '='*80)\n",
    "    print('🎯 最終推奨モデル戦略')\n",
    "    print('='*80)\n",
    "    \n",
    "    for idx, row in results_df.iterrows():\n",
    "        status = '✅ 採用推奨' if not row['過学習'] and row['R2_Test'] > 0.6 else \\\n",
    "                 '⚠️ 要改善' if row['過学習'] else \\\n",
    "                 '❌ 再検討'\n",
    "        \n",
    "        print(f\"{status} {row['カテゴリ']} ({row['グループ']}): \"\n",
    "              f\"{row['ベストモデル']} (R²={row['R2_Test']:.3f}, Gap={row['R2_Gap']:.3f})\")\n",
    "else:\n",
    "    print('⚠️ 分析結果が得られませんでした')\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('🎉 全分析完了！')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "store_analysis_header",
   "source": [
    "# 店舗別分析\n",
    "\n",
    "## 目的\n",
    "全店舗統合モデル vs 店舗別個別モデルのパフォーマンス比較\n",
    "\n",
    "**仮説:**\n",
    "- 店舗ごとに顧客属性・立地特性が異なる\n",
    "- 店舗別モデルの方が精度が高い可能性\n",
    "- ただし、データ不足で過学習のリスクも"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "store_all_unified",
   "source": [
    "# ========================================\n",
    "# 8. 全店舗統合モデル（店舗をダミー変数化）\n",
    "# ========================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('🏪 全店舗統合モデル分析（店舗をダミー変数化）')\n",
    "print('='*80)\n",
    "\n",
    "# 全データ（全カテゴリ×全店舗）\n",
    "all_store_data = data.copy()\n",
    "\n",
    "print(f'\\n全店舗データ: {len(all_store_data)}行')\n",
    "print(f'店舗数: {all_store_data[\"店舗\"].nunique()}')\n",
    "\n",
    "# 不要列除外\n",
    "exclude_cols = ['商品名', '日付', '売上金額', 'category_l',\n",
    "               'フェイスくくり大分類', 'フェイスくくり中分類', 'フェイスくくり小分類']\n",
    "\n",
    "# 店舗をダミー変数化\n",
    "store_dummies = pd.get_dummies(all_store_data['店舗'], prefix='store')\n",
    "\n",
    "# カテゴリもダミー変数化\n",
    "if 'category_l' in all_store_data.columns:\n",
    "    category_dummies = pd.get_dummies(all_store_data['category_l'], prefix='cat')\n",
    "else:\n",
    "    category_dummies = pd.DataFrame()\n",
    "\n",
    "feature_cols = [c for c in all_store_data.columns \n",
    "               if c not in exclude_cols + ['売上数量', '店舗']]\n",
    "numeric_cols = all_store_data[feature_cols].select_dtypes(include=GPU_MODELS + ['et', 'rf', 'gbr', 'dt']).columns.tolist()\n",
    "\n",
    "model_data_all = pd.concat([\n",
    "    all_store_data[numeric_cols + ['売上数量']],\n",
    "    store_dummies,\n",
    "    category_dummies\n",
    "], axis=1).dropna()\n",
    "\n",
    "print(f'有効データ: {len(model_data_all)}行, {len(model_data_all.columns)-1}特徴量')\n",
    "\n",
    "# PyCaret setup\n",
    "s_all = setup(\n",
    "    model_data_all,\n",
    "    target='売上数量',\n",
    "    session_id=123,\n",
    "    train_size=0.8,\n",
    "    fold=10,\n",
    "    normalize=True,\n",
    "    remove_multicollinearity=True,\n",
    "    multicollinearity_threshold=0.95,\n",
    "    silent=True,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# モデル比較\n",
    "print('\\n🔍 # GPU高速化: XGBoost/CatBoost GPUを優先使用\n",
    "    # 注: Top 20特徴量が自動的に考慮されます（PyCaret feature_importance）\n",
    "    compare_models()実行中（全店舗統合）...')\n",
    "best_models_all = # GPU高速化: XGBoost/CatBoost GPUを優先使用\n",
    "    compare_models(\n",
    "    n_select=5,\n",
    "    sort='R2',\n",
    "    turbo=False,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "comparison_all = pull()\n",
    "print('\\n📊 モデル比較結果（Top 5）:')\n",
    "print(comparison_all[['Model', 'R2', 'MAE', 'RMSE']].head())\n",
    "\n",
    "# 過学習検出\n",
    "best_model_all = best_models_all[0] if isinstance(best_models_all, list) else best_models_all\n",
    "\n",
    "X_train_all = get_config('X_train')\n",
    "y_train_all = get_config('y_train')\n",
    "X_test_all = get_config('X_test')\n",
    "y_test_all = get_config('y_test')\n",
    "\n",
    "overfitting_all = detect_overfitting(\n",
    "    best_model_all, X_train_all, y_train_all, X_test_all, y_test_all,\n",
    "    model_name=f'AllStores - {best_model_all.__class__.__name__}'\n",
    ")\n",
    "\n",
    "print(f'\\n🔬 過学習検出結果:')\n",
    "print(f'  Train R²: {overfitting_all[\"r2_train\"]:.4f}')\n",
    "print(f'  Test R²: {overfitting_all[\"r2_test\"]:.4f}')\n",
    "print(f'  R²ギャップ: {overfitting_all[\"r2_gap\"]:.4f}')\n",
    "print(f'  過学習判定: {\"はい\" if overfitting_all[\"is_overfitting\"] else \"いいえ\"}')\n",
    "\n",
    "# 結果保存\n",
    "all_store_result = {\n",
    "    '分析タイプ': '全店舗統合',\n",
    "    '店舗': '全店舗',\n",
    "    'データ数': len(model_data_all),\n",
    "    'ベストモデル': best_model_all.__class__.__name__,\n",
    "    'R2_Test': overfitting_all['r2_test'],\n",
    "    'R2_Train': overfitting_all['r2_train'],\n",
    "    'R2_Gap': overfitting_all['r2_gap'],\n",
    "    '過学習': overfitting_all['is_overfitting'],\n",
    "    '深刻度': overfitting_all['overfitting_severity'],\n",
    "    'MAE_Test': overfitting_all['mae_test'],\n",
    "    'RMSE_Test': overfitting_all['rmse_test']\n",
    "}\n",
    "\n",
    "# Learning Curve\n",
    "fig_all, gap_all = plot_learning_curve(best_model_all, X_train_all, y_train_all,\n",
    "                                       model_name='全店舗統合', cv=10)\n",
    "fig_all.savefig('output/learning_curves/learning_curve_AllStores.png',\n",
    "               dpi=150, bbox_inches='tight')\n",
    "plt.close(fig_all)\n",
    "\n",
    "print('\\n✅ 全店舗統合モデル分析完了')\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "store_individual",
   "source": [
    "# ========================================\n",
    "# 9. 店舗別個別モデル分析\n",
    "# ========================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('🏪 店舗別個別モデル分析')\n",
    "print('='*80)\n",
    "\n",
    "store_results = []\n",
    "\n",
    "# 店舗リスト取得\n",
    "stores = data['店舗'].unique()\n",
    "print(f'\\n対象店舗数: {len(stores)}店舗')\n",
    "print(f'店舗リスト: {stores}')\n",
    "\n",
    "for store in stores:\n",
    "    print(f'\\n--- 店舗: {store} ---')\n",
    "    \n",
    "    # 店舗データ抽出\n",
    "    store_data = data[data['店舗'] == store].copy()\n",
    "    \n",
    "    if len(store_data) < 500:\n",
    "        print(f'⚠️ データ不足 ({len(store_data)}行) - スキップ')\n",
    "        continue\n",
    "    \n",
    "    print(f'データ数: {len(store_data)}行')\n",
    "    print(f'カテゴリ数: {store_data[\"category_l\"].nunique()}')\n",
    "    \n",
    "    # 不要列除外\n",
    "    exclude_cols = ['店舗', '商品名', '日付', '売上金額',\n",
    "                   'フェイスくくり大分類', 'フェイスくくり中分類', 'フェイスくくり小分類']\n",
    "    \n",
    "    # カテゴリをダミー変数化\n",
    "    if 'category_l' in store_data.columns:\n",
    "        category_dummies = pd.get_dummies(store_data['category_l'], prefix='cat')\n",
    "    else:\n",
    "        category_dummies = pd.DataFrame()\n",
    "    \n",
    "    feature_cols = [c for c in store_data.columns \n",
    "                   if c not in exclude_cols + ['売上数量', 'category_l']]\n",
    "    numeric_cols = store_data[feature_cols].select_dtypes(include=GPU_MODELS + ['et', 'rf', 'gbr', 'dt']).columns.tolist()\n",
    "    \n",
    "    model_data_store = pd.concat([\n",
    "        store_data[numeric_cols + ['売上数量']],\n",
    "        category_dummies\n",
    "    ], axis=1).dropna()\n",
    "    \n",
    "    if len(model_data_store) < 100:\n",
    "        print(f'⚠️ 有効データ不足 ({len(model_data_store)}行) - スキップ')\n",
    "        continue\n",
    "    \n",
    "    print(f'有効データ: {len(model_data_store)}行, {len(model_data_store.columns)-1}特徴量')\n",
    "    \n",
    "    # PyCaret setup\n",
    "    s_store = setup(\n",
    "        model_data_store,\n",
    "        target='売上数量',\n",
    "        session_id=123,\n",
    "        train_size=0.8,\n",
    "        fold=5,\n",
    "        normalize=True,\n",
    "        remove_multicollinearity=True,\n",
    "        multicollinearity_threshold=0.95,\n",
    "        silent=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # モデル比較\n",
    "    print(f'\\n🔍 # GPU高速化: XGBoost/CatBoost GPUを優先使用\n",
    "    # 注: Top 20特徴量が自動的に考慮されます（PyCaret feature_importance）\n",
    "    compare_models()実行中（店舗: {store}）...')\n",
    "    best_models_store = # GPU高速化: XGBoost/CatBoost GPUを優先使用\n",
    "    compare_models(\n",
    "        n_select=5,\n",
    "        sort='R2',\n",
    "        turbo=False,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    comparison_store = pull()\n",
    "    print('\\n📊 モデル比較結果（Top 5）:')\n",
    "    print(comparison_store[['Model', 'R2', 'MAE', 'RMSE']].head())\n",
    "    \n",
    "    # 過学習検出\n",
    "    best_model_store = best_models_store[0] if isinstance(best_models_store, list) else best_models_store\n",
    "    \n",
    "    X_train_store = get_config('X_train')\n",
    "    y_train_store = get_config('y_train')\n",
    "    X_test_store = get_config('X_test')\n",
    "    y_test_store = get_config('y_test')\n",
    "    \n",
    "    overfitting_store = detect_overfitting(\n",
    "        best_model_store, X_train_store, y_train_store, X_test_store, y_test_store,\n",
    "        model_name=f'{store} - {best_model_store.__class__.__name__}'\n",
    "    )\n",
    "    \n",
    "    print(f'\\n🔬 過学習検出結果:')\n",
    "    print(f'  Train R²: {overfitting_store[\"r2_train\"]:.4f}')\n",
    "    print(f'  Test R²: {overfitting_store[\"r2_test\"]:.4f}')\n",
    "    print(f'  R²ギャップ: {overfitting_store[\"r2_gap\"]:.4f}')\n",
    "    print(f'  過学習判定: {\"はい\" if overfitting_store[\"is_overfitting\"] else \"いいえ\"}')\n",
    "    \n",
    "    # 結果保存\n",
    "    store_results.append({\n",
    "        '分析タイプ': '店舗別',\n",
    "        '店舗': store,\n",
    "        'データ数': len(model_data_store),\n",
    "        'ベストモデル': best_model_store.__class__.__name__,\n",
    "        'R2_Test': overfitting_store['r2_test'],\n",
    "        'R2_Train': overfitting_store['r2_train'],\n",
    "        'R2_Gap': overfitting_store['r2_gap'],\n",
    "        '過学習': overfitting_store['is_overfitting'],\n",
    "        '深刻度': overfitting_store['overfitting_severity'],\n",
    "        'MAE_Test': overfitting_store['mae_test'],\n",
    "        'RMSE_Test': overfitting_store['rmse_test']\n",
    "    })\n",
    "    \n",
    "    # Learning Curve\n",
    "    fig_store, gap_store = plot_learning_curve(\n",
    "        best_model_store, X_train_store, y_train_store,\n",
    "        model_name=f'店舗{store}', cv=5\n",
    "    )\n",
    "    fig_store.savefig(f'output/learning_curves/learning_curve_Store_{store}.png',\n",
    "                     dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig_store)\n",
    "    \n",
    "    print(f'✅ 店舗{store}分析完了')\n",
    "\n",
    "print(f'\\n✅ 店舗別分析完了: {len(store_results)}店舗')\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "store_comparison",
   "source": [
    "# ========================================\n",
    "# 10. 全店舗統合 vs 店舗別 比較分析\n",
    "# ========================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('📊 全店舗統合 vs 店舗別モデル 比較分析')\n",
    "print('='*80)\n",
    "\n",
    "# 全結果を統合\n",
    "store_comparison_results = [all_store_result] + store_results\n",
    "store_comparison_df = pd.DataFrame(store_comparison_results)\n",
    "\n",
    "if len(store_comparison_df) > 0:\n",
    "    print('\\n【全店舗統合 vs 店舗別 パフォーマンス比較】')\n",
    "    print(store_comparison_df[['分析タイプ', '店舗', 'ベストモデル', 'R2_Test',\n",
    "                              'R2_Gap', '過学習', '深刻度', 'データ数']].to_string(index=False))\n",
    "    \n",
    "    # 統計サマリー\n",
    "    print('\\n' + '='*80)\n",
    "    print('📈 戦略別パフォーマンスサマリー')\n",
    "    print('='*80)\n",
    "    \n",
    "    strategy_summary = store_comparison_df.groupby('分析タイプ').agg({\n",
    "        'R2_Test': ['mean', 'std', 'min', 'max'],\n",
    "        'R2_Gap': ['mean', 'max'],\n",
    "        '過学習': lambda x: (x == True).sum(),\n",
    "        '店舗': 'count'\n",
    "    }).round(4)\n",
    "    \n",
    "    print(strategy_summary)\n",
    "    \n",
    "    # 最適戦略の判定\n",
    "    all_store_r2 = store_comparison_df[store_comparison_df['分析タイプ'] == '全店舗統合']['R2_Test'].iloc[0]\n",
    "    per_store_avg_r2 = store_comparison_df[store_comparison_df['分析タイプ'] == '店舗別']['R2_Test'].mean()\n",
    "    \n",
    "    print('\\n' + '='*80)\n",
    "    print('🎯 最適モデリング戦略の判定')\n",
    "    print('='*80)\n",
    "    \n",
    "    print(f'\\n全店舗統合モデル R²: {all_store_r2:.4f}')\n",
    "    print(f'店舗別モデル平均 R²: {per_store_avg_r2:.4f}')\n",
    "    print(f'差分: {per_store_avg_r2 - all_store_r2:.4f} ({(per_store_avg_r2 - all_store_r2)/all_store_r2:.2%})')\n",
    "    \n",
    "    # 判定基準\n",
    "    if per_store_avg_r2 > all_store_r2 + 0.05:  # 5%以上改善\n",
    "        recommendation = '✅ 店舗別モデル推奨'\n",
    "        reason = f'店舗別モデルが{(per_store_avg_r2 - all_store_r2)/all_store_r2:.1%}改善')\n",
    "    elif per_store_avg_r2 < all_store_r2 - 0.05:  # 5%以上悪化\n",
    "        recommendation = '✅ 全店舗統合モデル推奨'\n",
    "        reason = f'統合モデルが{(all_store_r2 - per_store_avg_r2)/all_store_r2:.1%}優位')\n",
    "    else:  # ±5%以内\n",
    "        recommendation = '⚖️ ハイブリッド戦略推奨'\n",
    "        reason = '差分が小さいため、店舗特性に応じて使い分け'\n",
    "    \n",
    "    print(f'\\n{recommendation}')\n",
    "    print(f'理由: {reason}')\n",
    "    \n",
    "    # 詳細推奨\n",
    "    print('\\n💡 実装推奨:')\n",
    "    if '店舗別' in recommendation:\n",
    "        print('  1. 各店舗で個別モデルを学習・デプロイ')\n",
    "        print('  2. 新規店舗は統合モデルで開始、データ蓄積後に個別化')\n",
    "        print('  3. 定期的にモデル再学習（月次推奨）')\n",
    "    elif '統合' in recommendation:\n",
    "        print('  1. 全店舗で1つのモデルを共有（メンテナンスコスト削減）')\n",
    "        print('  2. 店舗ダミー変数で店舗特性を吸収')\n",
    "        print('  3. 店舗固有の特徴量を追加検討（立地、商圏など）')\n",
    "    else:  # ハイブリッド\n",
    "        print('  1. データ量が豊富な店舗 → 個別モデル')\n",
    "        print('  2. データ不足の店舗 → 統合モデル')\n",
    "        print('  3. 閾値: 1店舗あたり5000行以上なら個別化')\n",
    "    \n",
    "    # 店舗別パフォーマンスランキング\n",
    "    print('\\n' + '='*80)\n",
    "    print('🏆 店舗別パフォーマンスランキング')\n",
    "    print('='*80)\n",
    "    \n",
    "    store_only = store_comparison_df[store_comparison_df['分析タイプ'] == '店舗別'].copy()\n",
    "    if len(store_only) > 0:\n",
    "        store_only_sorted = store_only.sort_values('R2_Test', ascending=False)\n",
    "        print(store_only_sorted[['店舗', 'ベストモデル', 'R2_Test', 'R2_Gap',\n",
    "                                '過学習', 'データ数']].to_string(index=False))\n",
    "        \n",
    "        # ベスト店舗とワースト店舗\n",
    "        best_store = store_only_sorted.iloc[0]\n",
    "        worst_store = store_only_sorted.iloc[-1]\n",
    "        \n",
    "        print(f'\\n🥇 最高精度店舗: {best_store[\"店舗\"]} (R²={best_store[\"R2_Test\"]:.4f})')\n",
    "        print(f'   モデル: {best_store[\"ベストモデル\"]}')\n",
    "        print(f'   データ数: {best_store[\"データ数\"]:,}行')\n",
    "        \n",
    "        print(f'\\n⚠️ 改善必要店舗: {worst_store[\"店舗\"]} (R²={worst_store[\"R2_Test\"]:.4f})')\n",
    "        print(f'   モデル: {worst_store[\"ベストモデル\"]}')\n",
    "        print(f'   データ数: {worst_store[\"データ数\"]:,}行')\n",
    "        print(f'   改善策: データ期間延長、特徴量見直し、統合モデル利用検討')\n",
    "    \n",
    "    # CSV保存\n",
    "    output_path = Path('output/store_comparison_results.csv')\n",
    "    store_comparison_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    print(f'\\n✅ 結果をCSV保存: {output_path}')\n",
    "\n",
    "else:\n",
    "    print('⚠️ 比較結果が得られませんでした')\n",
    "\n",
    "print('\\n✅ 店舗別分析完了！')\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}