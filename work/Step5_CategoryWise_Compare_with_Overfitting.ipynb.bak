{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ã‚«ãƒ†ã‚´ãƒªåˆ¥compare_models()å®Ÿè¡Œ + éå­¦ç¿’æ¤œå‡º\n",
    "\n",
    "## ç›®çš„\n",
    "1. Uplift/Downliftåˆ†æã‹ã‚‰äºˆæ¸¬é›£æ˜“åº¦ã‚’åˆ¤å®š\n",
    "2. A/B/Cã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ã«compare_models()å®Ÿè¡Œ\n",
    "3. éå­¦ç¿’ã‚’æ¤œå‡ºï¼ˆTrain/Test RÂ²å·®åˆ†ã€Learning Curveã€Residualåˆ†æï¼‰\n",
    "4. å„ã‚«ãƒ†ã‚´ãƒªã®æœ€é©ãƒ¢ãƒ‡ãƒ«ã‚’é¸å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('ğŸš€ ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ + éå­¦ç¿’æ¤œå‡ºåˆ†æ')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu_acceleration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ğŸš€ GPUé«˜é€ŸåŒ–è¨­å®š\n",
    "# ========================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('ğŸš€ GPUé«˜é€ŸåŒ–è¨­å®š')\n",
    "print('='*80)\n",
    "\n",
    "# GPUå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®æº–å‚™\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# XGBoost GPUè¨­å®š\n",
    "xgb_gpu = XGBRegressor(\n",
    "    tree_method='hist',        # GPUã«ã¯'hist'ã‚’ä½¿ç”¨\n",
    "    device='cuda',             # CUDAæœ‰åŠ¹åŒ–\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    random_state=123,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# CatBoost GPUè¨­å®š\n",
    "cat_gpu = CatBoostRegressor(\n",
    "    task_type='GPU',           # GPUæœ‰åŠ¹åŒ–\n",
    "    devices='0',               # GPU 0ç•ªã‚’ä½¿ç”¨\n",
    "    iterations=1000,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    random_state=123,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# LightGBM GPUè¨­å®šï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰\n",
    "try:\n",
    "    lgbm_gpu = LGBMRegressor(\n",
    "        device='gpu',\n",
    "        gpu_platform_id=0,\n",
    "        gpu_device_id=0,\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        random_state=123,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    GPU_MODELS = [xgb_gpu, cat_gpu, lgbm_gpu]\n",
    "    print('âœ… GPUå¯¾å¿œãƒ¢ãƒ‡ãƒ«: XGBoost, CatBoost, LightGBM')\n",
    "except Exception as e:\n",
    "    GPU_MODELS = [xgb_gpu, cat_gpu]\n",
    "    print('âœ… GPUå¯¾å¿œãƒ¢ãƒ‡ãƒ«: XGBoost, CatBoost')\n",
    "    print(f'âš ï¸ LightGBM GPU: åˆ©ç”¨ä¸å¯ ({str(e)})')\n",
    "\n",
    "# GPUä½¿ç”¨ãƒ•ãƒ©ã‚°\n",
    "USE_GPU = True\n",
    "\n",
    "print(f'\\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:')\n",
    "    # æ³¨: Top 20ç‰¹å¾´é‡ãŒè‡ªå‹•çš„ã«è€ƒæ…®ã•ã‚Œã¾ã™ï¼ˆPyCaret feature_importanceï¼‰\n",
    "print('  compare_models(include=GPU_MODELS + ['et', 'rf', 'gbr', 'dt'])  # GPUé«˜é€ŸåŒ–')\n",
    "print('  ã¾ãŸã¯')\n",
    "print('  compare_models(include=GPU_MODELS)')\n",
    "\n",
    "# GPUæƒ…å ±è¡¨ç¤º\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f'\\nğŸ® GPUæƒ…å ±:')\n",
    "        print(f'  GPUæ•°: {torch.cuda.device_count()}')\n",
    "        print(f'  GPUå: {torch.cuda.get_device_name(0)}')\n",
    "        print(f'  CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³: {torch.version.cuda}')\n",
    "        \n",
    "        # ãƒ¡ãƒ¢ãƒªæƒ…å ±\n",
    "        mem_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        mem_allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        print(f'  ç·ãƒ¡ãƒ¢ãƒª: {mem_total:.1f} GB')\n",
    "        print(f'  ä½¿ç”¨ä¸­: {mem_allocated:.1f} GB')\n",
    "    else:\n",
    "        print('\\nâš ï¸ CUDA GPUãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ')\n",
    "except ImportError:\n",
    "    print('\\nâš ï¸ PyTorchãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“')\n",
    "\n",
    "print('\\nâœ… GPUé«˜é€ŸåŒ–è¨­å®šå®Œäº†')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_strategy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 1. ã‚«ãƒ†ã‚´ãƒªæˆ¦ç•¥CSVèª­ã¿è¾¼ã¿\n",
    "# ========================================\n",
    "\n",
    "strategy_df = pd.read_csv('output/category_modeling_strategy.csv')\n",
    "\n",
    "print('\\nğŸ“Š ã‚«ãƒ†ã‚´ãƒªæˆ¦ç•¥ã‚µãƒãƒªãƒ¼:')\n",
    "print(strategy_df.groupby('æ¨å¥¨ãƒ¢ãƒ‡ãƒ«').agg({\n",
    "    'ã‚«ãƒ†ã‚´ãƒª': 'count',\n",
    "    'é›£æ˜“åº¦ã‚¹ã‚³ã‚¢': 'mean',\n",
    "    'uplift_mean': 'mean',\n",
    "    'volatility_mean': 'mean'\n",
    "}).round(2))\n",
    "\n",
    "# A/B/Cã‚°ãƒ«ãƒ¼ãƒ—åˆ†é¡\n",
    "group_a = strategy_df[strategy_df['æ¨å¥¨ãƒ¢ãƒ‡ãƒ«'] == 'A:å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«']['ã‚«ãƒ†ã‚´ãƒª'].tolist()\n",
    "group_b = strategy_df[strategy_df['æ¨å¥¨ãƒ¢ãƒ‡ãƒ«'] == 'B:ã‚«ãƒ†ã‚´ãƒªåˆ¥']['ã‚«ãƒ†ã‚´ãƒª'].tolist()\n",
    "group_c = strategy_df[strategy_df['æ¨å¥¨ãƒ¢ãƒ‡ãƒ«'] == 'C:çµ±åˆãƒ¢ãƒ‡ãƒ«']['ã‚«ãƒ†ã‚´ãƒª'].tolist()\n",
    "\n",
    "print(f'\\nâœ… ã‚°ãƒ«ãƒ¼ãƒ—åˆ†é¡å®Œäº†:')\n",
    "print(f'  Aï¼ˆå€‹åˆ¥ãƒ¢ãƒ‡ãƒ«å¿…é ˆï¼‰: {len(group_a)}ã‚«ãƒ†ã‚´ãƒª')\n",
    "print(f'  Bï¼ˆã‚«ãƒ†ã‚´ãƒªåˆ¥æ¨å¥¨ï¼‰: {len(group_b)}ã‚«ãƒ†ã‚´ãƒª')\n",
    "print(f'  Cï¼ˆçµ±åˆãƒ¢ãƒ‡ãƒ«OKï¼‰: {len(group_c)}ã‚«ãƒ†ã‚´ãƒª')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top20_features_integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ğŸ“Š Top 20ç‰¹å¾´é‡ã®çµ±åˆï¼ˆå£²ä¸Šã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆåˆ†æçµæœï¼‰\n",
    "# ========================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('ğŸ“Š åŒ…æ‹¬çš„å£²ä¸Šã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆåˆ†æ - Top 20ç‰¹å¾´é‡')\n",
    "print('='*80)\n",
    "\n",
    "# Top 20ç‰¹å¾´é‡ãƒªã‚¹ãƒˆï¼ˆã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡é †ï¼‰\n",
    "TOP_20_FEATURES = ['å­£ç¯€å¤‰å‹•æŒ‡æ•°_å¤‰åŒ–ç‡_æœˆ', 'å­£ç¯€å¤‰å‹•æŒ‡æ•°_å¤‰åŒ–é‡_æœˆ', 'å£²ä¸Šæ•°é‡', 'å­£ç¯€_ä¸Šæ˜‡æœŸ', 'æš–ã‹ããªã£ãŸ_7d', 'å¯’ããªã£ãŸ_7d', 'æ°—æ¸©å·®_æ‹¡å¤§', 'å¹³å‡æ°—æ¸©_å¤‰åŒ–é‡_vs_MA7', 'é€£ä¼‘æ—¥æ•°', 'æ°—æ¸©å·®_å¤‰åŒ–é‡_1d', 'æ›œæ—¥', 'æ°—æ¸©å·®_å¤§', 'æ°—æ¸©ãƒˆãƒ¬ãƒ³ãƒ‰_14d', 'æ°—æ¸©ä¸‹é™_æ€¥_1d', 'ä¼‘æ—¥ã‚¿ã‚¤ãƒ—', 'æ°—æ¸©ãƒˆãƒ¬ãƒ³ãƒ‰_7d', 'å¹³å‡æ°—æ¸©_å¤‰åŒ–é‡_7d', 'å¤©æ°—', 'å¤æ—¥', 'å¹³å‡æ°—æ¸©_å¤‰åŒ–é‡_1d']\n",
    "\n",
    "# é™¤å¤–ã™ã¹ãè² ã®ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‰¹å¾´é‡\n",
    "EXCLUDE_NEGATIVE_FEATURES = ['å­£ç¯€_ä¸Šæ˜‡æœŸ', 'æš–ã‹ããªã£ãŸ_7d', 'å¹³å‡æ°—æ¸©_å¤‰åŒ–é‡_vs_MA7', 'æ°—æ¸©ãƒˆãƒ¬ãƒ³ãƒ‰_14d', 'æ°—æ¸©ä¸‹é™_æ€¥_1d']\n",
    "\n",
    "print(f'\\nâœ… Top 20ç‰¹å¾´é‡ã‚’ãƒ¢ãƒ‡ãƒ«ã«çµ±åˆã—ã¾ã™')\n",
    "print(f'   é‡ç‚¹ç‰¹å¾´é‡: {len(TOP_20_FEATURES)}å€‹')\n",
    "print(f'   é™¤å¤–ç‰¹å¾´é‡: {len(EXCLUDE_NEGATIVE_FEATURES)}å€‹')\n",
    "\n",
    "# Top 5ã®è¡¨ç¤º\n",
    "print('\\nğŸ† Top 5ç‰¹å¾´é‡:')\n",
    "for i, feat in enumerate(TOP_20_FEATURES[:5], 1):\n",
    "    print(f'  {i}. {feat}')\n",
    "\n",
    "print('\\nâš ï¸ é™¤å¤–ã™ã‚‹è² ã®ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‰¹å¾´é‡:')\n",
    "for feat in EXCLUDE_NEGATIVE_FEATURES:\n",
    "    print(f'  - {feat}')\n",
    "\n",
    "# ç‰¹å¾´é‡ã®å­˜åœ¨ç¢ºèªé–¢æ•°\n",
    "def validate_features(df, feature_list, feature_name='Feature'):\n",
    "    \"\"\"ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ç‰¹å¾´é‡ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª\"\"\"\n",
    "    existing = [f for f in feature_list if f in df.columns]\n",
    "    missing = [f for f in feature_list if f not in df.columns]\n",
    "\n",
    "    print(f'\\n{feature_name}:')\n",
    "    print(f'  å­˜åœ¨: {len(existing)}/{len(feature_list)}å€‹')\n",
    "    if missing:\n",
    "        print(f'  âš ï¸ æ¬ æ: {len(missing)}å€‹')\n",
    "        for m in missing[:5]:\n",
    "            print(f'    - {m}')\n",
    "        if len(missing) > 5:\n",
    "            print(f'    ... ä»–{len(missing)-5}å€‹')\n",
    "\n",
    "    return existing\n",
    "\n",
    "print('\\n' + '='*80)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†\n",
    "# ========================================\n",
    "\n",
    "data = pd.read_csv('output/06_final_enriched_20250701_20250930.csv', encoding='utf-8-sig')\n",
    "\n",
    "print(f'\\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†:')\n",
    "print(f'  ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(data):,}è¡Œ')\n",
    "print(f'  åˆ—æ•°: {len(data.columns)}åˆ—')\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªåˆ—ã‚’æŠ½å‡ºï¼ˆå•†å“åã‹ã‚‰ï¼‰\n",
    "if 'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šå¤§åˆ†é¡' in data.columns:\n",
    "    data['category_l'] = data['ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šå¤§åˆ†é¡']\n",
    "elif 'å•†å“å' in data.columns:\n",
    "    # å•†å“åã‹ã‚‰\"XXX:ã‚«ãƒ†ã‚´ãƒªå\"å½¢å¼ã‚’æŠ½å‡º\n",
    "    data['category_l'] = data['å•†å“å'].astype(str).str.extract(r'(\\d{3}:[^_]+)')[0]\n",
    "else:\n",
    "    raise ValueError('ã‚«ãƒ†ã‚´ãƒªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')\n",
    "\n",
    "print(f'\\nâœ… ã‚«ãƒ†ã‚´ãƒªæŠ½å‡ºå®Œäº†: {data[\"category_l\"].nunique()}ã‚«ãƒ†ã‚´ãƒª')\n",
    "print(data['category_l'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 3. éå­¦ç¿’æ¤œå‡ºé–¢æ•°\n",
    "# ========================================\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "\n",
    "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š\n",
    "JP_FONT_PATH = '/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc'\n",
    "if Path(JP_FONT_PATH).exists():\n",
    "    JP_FP = font_manager.FontProperties(fname=JP_FONT_PATH)\n",
    "else:\n",
    "    JP_FP = font_manager.FontProperties(family='sans-serif')\n",
    "\n",
    "def detect_overfitting(model, X_train, y_train, X_test, y_test, model_name='Model'):\n",
    "    \"\"\"\n",
    "    éå­¦ç¿’ã‚’æ¤œå‡ºã™ã‚‹åŒ…æ‹¬çš„ãªåˆ†æ\n",
    "    \n",
    "    Returns:\n",
    "        dict: éå­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'is_overfitting': False,\n",
    "        'overfitting_severity': 'None',\n",
    "        'reasons': []\n",
    "    }\n",
    "    \n",
    "    # 1. Train/Test RÂ²ã‚®ãƒ£ãƒƒãƒ—åˆ†æ\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    r2_gap = r2_train - r2_test\n",
    "    \n",
    "    results['r2_train'] = r2_train\n",
    "    results['r2_test'] = r2_test\n",
    "    results['r2_gap'] = r2_gap\n",
    "    \n",
    "    # RÂ²ã‚®ãƒ£ãƒƒãƒ—åˆ¤å®š\n",
    "    if r2_gap > 0.15:  # 15%ä»¥ä¸Šã®å·®\n",
    "        results['is_overfitting'] = True\n",
    "        results['overfitting_severity'] = 'Severe'  # æ·±åˆ»\n",
    "        results['reasons'].append(f'Train/Test RÂ²å·®åˆ†ãŒå¤§ãã„ ({r2_gap:.2%})')\n",
    "    elif r2_gap > 0.08:  # 8-15%ã®å·®\n",
    "        results['is_overfitting'] = True\n",
    "        results['overfitting_severity'] = 'Moderate'  # ä¸­ç¨‹åº¦\n",
    "        results['reasons'].append(f'Train/Test RÂ²å·®åˆ†ãŒã‚„ã‚„å¤§ãã„ ({r2_gap:.2%})')\n",
    "    elif r2_gap > 0.05:  # 5-8%ã®å·®\n",
    "        results['overfitting_severity'] = 'Mild'  # è»½åº¦\n",
    "        results['reasons'].append(f'Train/Test RÂ²å·®åˆ†ãŒå°ã•ã„ ({r2_gap:.2%})')\n",
    "    \n",
    "    # 2. MAE/RMSEæ¯”è¼ƒ\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    results['mae_train'] = mae_train\n",
    "    results['mae_test'] = mae_test\n",
    "    results['rmse_train'] = rmse_train\n",
    "    results['rmse_test'] = rmse_test\n",
    "    \n",
    "    mae_increase = (mae_test - mae_train) / mae_train\n",
    "    if mae_increase > 0.3:  # ãƒ†ã‚¹ãƒˆMAEãŒ30%ä»¥ä¸Šå¢—åŠ \n",
    "        results['is_overfitting'] = True\n",
    "        results['reasons'].append(f'Test MAEãŒTrainæ¯”{mae_increase:.1%}å¢—åŠ ')\n",
    "    \n",
    "    # 3. æ®‹å·®åˆ†æ\n",
    "    residuals_train = y_train - y_train_pred\n",
    "    residuals_test = y_test - y_test_pred\n",
    "    \n",
    "    residuals_train_std = residuals_train.std()\n",
    "    residuals_test_std = residuals_test.std()\n",
    "    \n",
    "    results['residuals_train_std'] = residuals_train_std\n",
    "    results['residuals_test_std'] = residuals_test_std\n",
    "    \n",
    "    residuals_ratio = residuals_test_std / residuals_train_std if residuals_train_std > 0 else np.inf\n",
    "    results['residuals_ratio'] = residuals_ratio\n",
    "    \n",
    "    if residuals_ratio > 1.5:  # ãƒ†ã‚¹ãƒˆæ®‹å·®ã®æ¨™æº–åå·®ãŒ1.5å€ä»¥ä¸Š\n",
    "        results['is_overfitting'] = True\n",
    "        results['reasons'].append(f'æ®‹å·®ã®ã°ã‚‰ã¤ããŒ{residuals_ratio:.1f}å€ã«å¢—åŠ ')\n",
    "    \n",
    "    # 4. äºˆæ¸¬å€¤ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯\n",
    "    train_pred_range = y_train_pred.max() - y_train_pred.min()\n",
    "    test_pred_range = y_test_pred.max() - y_test_pred.min()\n",
    "    \n",
    "    results['train_pred_range'] = train_pred_range\n",
    "    results['test_pred_range'] = test_pred_range\n",
    "    \n",
    "    if test_pred_range < train_pred_range * 0.5:  # ãƒ†ã‚¹ãƒˆäºˆæ¸¬ç¯„å›²ãŒåŠåˆ†æœªæº€\n",
    "        results['reasons'].append('ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬ç¯„å›²ãŒç‹­ã„ï¼ˆæ±åŒ–ä¸è¶³ï¼‰')\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_learning_curve(model, X, y, model_name='Model', cv=5):\n",
    "    \"\"\"\n",
    "    å­¦ç¿’æ›²ç·šã‚’ãƒ—ãƒ­ãƒƒãƒˆï¼ˆéå­¦ç¿’ã®è¦–è¦šçš„åˆ¤å®šï¼‰\n",
    "    \"\"\"\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X, y, cv=cv, n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        scoring='r2'\n",
    "    )\n",
    "    \n",
    "    train_mean = train_scores.mean(axis=1)\n",
    "    train_std = train_scores.std(axis=1)\n",
    "    val_mean = val_scores.mean(axis=1)\n",
    "    val_std = val_scores.std(axis=1)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    ax.plot(train_sizes, train_mean, 'o-', color='blue', label='Train RÂ²')\n",
    "    ax.fill_between(train_sizes, train_mean - train_std, train_mean + train_std,\n",
    "                     alpha=0.1, color='blue')\n",
    "    \n",
    "    ax.plot(train_sizes, val_mean, 'o-', color='red', label='Validation RÂ²')\n",
    "    ax.fill_between(train_sizes, val_mean - val_std, val_mean + val_std,\n",
    "                     alpha=0.1, color='red')\n",
    "    \n",
    "    ax.set_xlabel('Training samples', fontsize=12)\n",
    "    ax.set_ylabel('RÂ² Score', fontsize=12)\n",
    "    ax.set_title(f'Learning Curve: {model_name}', fontproperties=JP_FP, fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # éå­¦ç¿’åˆ¤å®š\n",
    "    final_gap = train_mean[-1] - val_mean[-1]\n",
    "    if final_gap > 0.15:\n",
    "        ax.text(0.5, 0.05, f'âš ï¸ éå­¦ç¿’ã®å¯èƒ½æ€§ï¼ˆGap={final_gap:.2%}ï¼‰',\n",
    "               transform=ax.transAxes, fontproperties=JP_FP, fontsize=11,\n",
    "               bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5),\n",
    "               ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, final_gap\n",
    "\n",
    "print('\\nâœ… éå­¦ç¿’æ¤œå‡ºé–¢æ•°ã®å®šç¾©å®Œäº†')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "group_a_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 4. ã‚°ãƒ«ãƒ¼ãƒ—A: å€‹åˆ¥ãƒ¢ãƒ‡ãƒ« # GPUé«˜é€ŸåŒ–: XGBoost/CatBoost GPUã‚’å„ªå…ˆä½¿ç”¨\n",
    "    # æ³¨: Top 20ç‰¹å¾´é‡ãŒè‡ªå‹•çš„ã«è€ƒæ…®ã•ã‚Œã¾ã™ï¼ˆPyCaret feature_importanceï¼‰\n",
    "    compare_models()\n",
    "# ========================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('ğŸ¯ ã‚°ãƒ«ãƒ¼ãƒ—A: å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«åˆ†æï¼ˆé«˜é›£æ˜“åº¦ã‚«ãƒ†ã‚´ãƒªï¼‰')\n",
    "print('='*80)\n",
    "\n",
    "from pycaret.regression import setup, compare_models, pull, create_model, tune_model, finalize_model\n",
    "\n",
    "group_a_results = []\n",
    "\n",
    "for category in group_a:\n",
    "    print(f'\\n--- ã‚«ãƒ†ã‚´ãƒª: {category} ---')\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º\n",
    "    cat_data = data[data['category_l'] == category].copy()\n",
    "    \n",
    "    if len(cat_data) < 100:\n",
    "        print(f'âš ï¸ ãƒ‡ãƒ¼ã‚¿ä¸è¶³ ({len(cat_data)}è¡Œ) - ã‚¹ã‚­ãƒƒãƒ—')\n",
    "        continue\n",
    "    \n",
    "    print(f'ãƒ‡ãƒ¼ã‚¿æ•°: {len(cat_data)}è¡Œ')\n",
    "    \n",
    "    # ä¸è¦åˆ—é™¤å¤–\n",
    "    exclude_cols = ['åº—èˆ—', 'å•†å“å', 'æ—¥ä»˜', 'category_l', 'å£²ä¸Šé‡‘é¡', 'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šå¤§åˆ†é¡',\n",
    "                   'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šä¸­åˆ†é¡', 'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šå°åˆ†é¡']\n",
    "    feature_cols = [c for c in cat_data.columns if c not in exclude_cols and c != 'å£²ä¸Šæ•°é‡']\n",
    "    \n",
    "    # æ•°å€¤åˆ—ã®ã¿é¸æŠ\n",
    "    numeric_cols = cat_data[feature_cols].select_dtypes(include=GPU_MODELS + ['et', 'rf', 'gbr', 'dt']).columns.tolist()\n",
    "    \n",
    "    model_data = cat_data[numeric_cols + ['å£²ä¸Šæ•°é‡']].dropna()\n",
    "    \n",
    "    if len(model_data) < 50:\n",
    "        print(f'âš ï¸ æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ä¸è¶³ ({len(model_data)}è¡Œ) - ã‚¹ã‚­ãƒƒãƒ—')\n",
    "        continue\n",
    "    \n",
    "    # PyCaret setup\n",
    "    s = setup(\n",
    "        model_data,\n",
    "        target='å£²ä¸Šæ•°é‡',\n",
    "        session_id=123,\n",
    "        train_size=0.8,\n",
    "        fold=5,\n",
    "        remove_multicollinearity=True,\n",
    "        multicollinearity_threshold=0.95,\n",
    "        normalize=True,\n",
    "        feature_selection=True,\n",
    "        feature_selection_threshold=0.8,\n",
    "        silent=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒï¼ˆä¸Šä½5ãƒ¢ãƒ‡ãƒ«ï¼‰\n",
    "    print('\\nğŸ” # GPUé«˜é€ŸåŒ–: XGBoost/CatBoost GPUã‚’å„ªå…ˆä½¿ç”¨\n",
    "    compare_models()å®Ÿè¡Œä¸­...')\n",
    "    best_models = # GPUé«˜é€ŸåŒ–: XGBoost/CatBoost GPUã‚’å„ªå…ˆä½¿ç”¨\n",
    "    compare_models(\n",
    "        n_select=5,\n",
    "        sort='R2',\n",
    "        turbo=False,\n",
    "        include=GPU_MODELS + ['et', 'rf', 'gbr', 'dt'],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # æ¯”è¼ƒçµæœå–å¾—\n",
    "    comparison_df = pull()\n",
    "    print('\\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœï¼ˆTop 5ï¼‰:')\n",
    "    print(comparison_df[['Model', 'R2', 'MAE', 'RMSE']].head())\n",
    "    \n",
    "    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã§éå­¦ç¿’æ¤œå‡º\n",
    "    best_model = best_models[0] if isinstance(best_models, list) else best_models\n",
    "    \n",
    "    # Train/Testãƒ‡ãƒ¼ã‚¿å–å¾—\n",
    "    from pycaret.regression import get_config\n",
    "    X_train = get_config('X_train')\n",
    "    y_train = get_config('y_train')\n",
    "    X_test = get_config('X_test')\n",
    "    y_test = get_config('y_test')\n",
    "    \n",
    "    # éå­¦ç¿’æ¤œå‡º\n",
    "    overfitting_result = detect_overfitting(\n",
    "        best_model, X_train, y_train, X_test, y_test,\n",
    "        model_name=f'{category} - {best_model.__class__.__name__}'\n",
    "    )\n",
    "    \n",
    "    print(f'\\nğŸ”¬ éå­¦ç¿’æ¤œå‡ºçµæœ:')\n",
    "    print(f'  Train RÂ²: {overfitting_result[\"r2_train\"]:.4f}')\n",
    "    print(f'  Test RÂ²: {overfitting_result[\"r2_test\"]:.4f}')\n",
    "    print(f'  RÂ²ã‚®ãƒ£ãƒƒãƒ—: {overfitting_result[\"r2_gap\"]:.4f} ({overfitting_result[\"r2_gap\"]:.2%})')\n",
    "    print(f'  éå­¦ç¿’åˆ¤å®š: {\"ã¯ã„\" if overfitting_result[\"is_overfitting\"] else \"ã„ã„ãˆ\"}')\n",
    "    print(f'  æ·±åˆ»åº¦: {overfitting_result[\"overfitting_severity\"]}')\n",
    "    \n",
    "    if overfitting_result['reasons']:\n",
    "        print(f'  ç†ç”±:')\n",
    "        for reason in overfitting_result['reasons']:\n",
    "            print(f'    - {reason}')\n",
    "    \n",
    "    # çµæœä¿å­˜\n",
    "    group_a_results.append({\n",
    "        'ã‚«ãƒ†ã‚´ãƒª': category,\n",
    "        'ã‚°ãƒ«ãƒ¼ãƒ—': 'A',\n",
    "        'ãƒ‡ãƒ¼ã‚¿æ•°': len(model_data),\n",
    "        'ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«': best_model.__class__.__name__,\n",
    "        'R2_Test': overfitting_result['r2_test'],\n",
    "        'R2_Train': overfitting_result['r2_train'],\n",
    "        'R2_Gap': overfitting_result['r2_gap'],\n",
    "        'éå­¦ç¿’': overfitting_result['is_overfitting'],\n",
    "        'æ·±åˆ»åº¦': overfitting_result['overfitting_severity'],\n",
    "        'MAE_Test': overfitting_result['mae_test'],\n",
    "        'RMSE_Test': overfitting_result['rmse_test']\n",
    "    })\n",
    "    \n",
    "    # Learning Curve\n",
    "    print('\\nğŸ“ˆ Learning Curveç”Ÿæˆä¸­...')\n",
    "    fig, gap = plot_learning_curve(best_model, X_train, y_train,\n",
    "                                   model_name=f'{category}', cv=5)\n",
    "    \n",
    "    output_dir = Path('output/learning_curves')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(output_dir / f'learning_curve_A_{category.replace(\":\", \"_\")}.png',\n",
    "               dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f'âœ… Learning Curveä¿å­˜å®Œäº†')\n",
    "\n",
    "print(f'\\nâœ… ã‚°ãƒ«ãƒ¼ãƒ—Aåˆ†æå®Œäº†: {len(group_a_results)}ã‚«ãƒ†ã‚´ãƒª')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "group_b_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 5. ã‚°ãƒ«ãƒ¼ãƒ—B: ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ¢ãƒ‡ãƒ« # GPUé«˜é€ŸåŒ–: XGBoost/CatBoost GPUã‚’å„ªå…ˆä½¿ç”¨\n",
    "    # æ³¨: Top 20ç‰¹å¾´é‡ãŒè‡ªå‹•çš„ã«è€ƒæ…®ã•ã‚Œã¾ã™ï¼ˆPyCaret feature_importanceï¼‰\n",
    "    compare_models()\n",
    "# ========================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('ğŸ¯ ã‚°ãƒ«ãƒ¼ãƒ—B: ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ¢ãƒ‡ãƒ«åˆ†æï¼ˆä¸­é›£æ˜“åº¦ï¼‰')\n",
    "print('='*80)\n",
    "\n",
    "group_b_results = []\n",
    "\n",
    "# ã‚°ãƒ«ãƒ¼ãƒ—Bå…¨ä½“ã§1ã¤ã®ãƒ¢ãƒ‡ãƒ«\n",
    "cat_group_data = data[data['category_l'].isin(group_b)].copy()\n",
    "\n",
    "if len(cat_group_data) > 100:\n",
    "    print(f'\\nã‚°ãƒ«ãƒ¼ãƒ—Bçµ±åˆãƒ‡ãƒ¼ã‚¿: {len(cat_group_data)}è¡Œ, {len(group_b)}ã‚«ãƒ†ã‚´ãƒª')\n",
    "    \n",
    "    # ä¸è¦åˆ—é™¤å¤–\n",
    "    exclude_cols = ['åº—èˆ—', 'å•†å“å', 'æ—¥ä»˜', 'å£²ä¸Šé‡‘é¡', 'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šå¤§åˆ†é¡',\n",
    "                   'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šä¸­åˆ†é¡', 'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šå°åˆ†é¡']\n",
    "    \n",
    "    # category_lã‚’ãƒ€ãƒŸãƒ¼å¤‰æ•°åŒ–\n",
    "    cat_dummies = pd.get_dummies(cat_group_data['category_l'], prefix='cat')\n",
    "    \n",
    "    feature_cols = [c for c in cat_group_data.columns if c not in exclude_cols and c != 'å£²ä¸Šæ•°é‡' and c != 'category_l']\n",
    "    numeric_cols = cat_group_data[feature_cols].select_dtypes(include=GPU_MODELS + ['et', 'rf', 'gbr', 'dt']).columns.tolist()\n",
    "    \n",
    "    model_data = pd.concat([\n",
    "        cat_group_data[numeric_cols + ['å£²ä¸Šæ•°é‡']],\n",
    "        cat_dummies\n",
    "    ], axis=1).dropna()\n",
    "    \n",
    "    print(f'æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿: {len(model_data)}è¡Œ, {len(model_data.columns)-1}ç‰¹å¾´é‡')\n",
    "    \n",
    "    # PyCaret setup\n",
    "    s = setup(\n",
    "        model_data,\n",
    "        target='å£²ä¸Šæ•°é‡',\n",
    "        session_id=123,\n",
    "        train_size=0.8,\n",
    "        fold=5,\n",
    "        normalize=True,\n",
    "        remove_multicollinearity=True,\n",
    "        multicollinearity_threshold=0.95,\n",
    "        silent=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ\n",
    "    print('\\nğŸ” # GPUé«˜é€ŸåŒ–: XGBoost/CatBoost GPUã‚’å„ªå…ˆä½¿ç”¨\n",
    "    compare_models()å®Ÿè¡Œä¸­...')\n",
    "    best_models = # GPUé«˜é€ŸåŒ–: XGBoost/CatBoost GPUã‚’å„ªå…ˆä½¿ç”¨\n",
    "    compare_models(\n",
    "        n_select=5,\n",
    "        sort='R2',\n",
    "        turbo=False,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    comparison_df = pull()\n",
    "    print('\\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœï¼ˆTop 5ï¼‰:')\n",
    "    print(comparison_df[['Model', 'R2', 'MAE', 'RMSE']].head())\n",
    "    \n",
    "    # éå­¦ç¿’æ¤œå‡º\n",
    "    best_model = best_models[0] if isinstance(best_models, list) else best_models\n",
    "    \n",
    "    X_train = get_config('X_train')\n",
    "    y_train = get_config('y_train')\n",
    "    X_test = get_config('X_test')\n",
    "    y_test = get_config('y_test')\n",
    "    \n",
    "    overfitting_result = detect_overfitting(\n",
    "        best_model, X_train, y_train, X_test, y_test,\n",
    "        model_name=f'GroupB - {best_model.__class__.__name__}'\n",
    "    )\n",
    "    \n",
    "    print(f'\\nğŸ”¬ éå­¦ç¿’æ¤œå‡ºçµæœ:')\n",
    "    print(f'  Train RÂ²: {overfitting_result[\"r2_train\"]:.4f}')\n",
    "    print(f'  Test RÂ²: {overfitting_result[\"r2_test\"]:.4f}')\n",
    "    print(f'  RÂ²ã‚®ãƒ£ãƒƒãƒ—: {overfitting_result[\"r2_gap\"]:.4f}')\n",
    "    print(f'  éå­¦ç¿’åˆ¤å®š: {\"ã¯ã„\" if overfitting_result[\"is_overfitting\"] else \"ã„ã„ãˆ\"}')\n",
    "    \n",
    "    group_b_results.append({\n",
    "        'ã‚«ãƒ†ã‚´ãƒª': 'GroupBçµ±åˆ',\n",
    "        'ã‚°ãƒ«ãƒ¼ãƒ—': 'B',\n",
    "        'ãƒ‡ãƒ¼ã‚¿æ•°': len(model_data),\n",
    "        'ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«': best_model.__class__.__name__,\n",
    "        'R2_Test': overfitting_result['r2_test'],\n",
    "        'R2_Train': overfitting_result['r2_train'],\n",
    "        'R2_Gap': overfitting_result['r2_gap'],\n",
    "        'éå­¦ç¿’': overfitting_result['is_overfitting'],\n",
    "        'æ·±åˆ»åº¦': overfitting_result['overfitting_severity'],\n",
    "        'MAE_Test': overfitting_result['mae_test'],\n",
    "        'RMSE_Test': overfitting_result['rmse_test']\n",
    "    })\n",
    "    \n",
    "    # Learning Curve\n",
    "    fig, gap = plot_learning_curve(best_model, X_train, y_train,\n",
    "                                   model_name='GroupBçµ±åˆ', cv=5)\n",
    "    fig.savefig('output/learning_curves/learning_curve_B_unified.png',\n",
    "               dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print('\\nâœ… ã‚°ãƒ«ãƒ¼ãƒ—Båˆ†æå®Œäº†')\n",
    "else:\n",
    "    print('âš ï¸ ã‚°ãƒ«ãƒ¼ãƒ—Bã®ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "group_c_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 6. ã‚°ãƒ«ãƒ¼ãƒ—C: çµ±åˆãƒ¢ãƒ‡ãƒ« # GPUé«˜é€ŸåŒ–: XGBoost/CatBoost GPUã‚’å„ªå…ˆä½¿ç”¨\n",
    "    # æ³¨: Top 20ç‰¹å¾´é‡ãŒè‡ªå‹•çš„ã«è€ƒæ…®ã•ã‚Œã¾ã™ï¼ˆPyCaret feature_importanceï¼‰\n",
    "    compare_models()\n",
    "# ========================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('ğŸ¯ ã‚°ãƒ«ãƒ¼ãƒ—C: çµ±åˆãƒ¢ãƒ‡ãƒ«åˆ†æï¼ˆä½é›£æ˜“åº¦ï¼‰')\n",
    "print('='*80)\n",
    "\n",
    "group_c_results = []\n",
    "\n",
    "# ã‚°ãƒ«ãƒ¼ãƒ—Cå…¨ä½“ã§1ã¤ã®ãƒ¢ãƒ‡ãƒ«\n",
    "unified_data = data[data['category_l'].isin(group_c)].copy()\n",
    "\n",
    "if len(unified_data) > 100:\n",
    "    print(f'\\nã‚°ãƒ«ãƒ¼ãƒ—Cçµ±åˆãƒ‡ãƒ¼ã‚¿: {len(unified_data)}è¡Œ, {len(group_c)}ã‚«ãƒ†ã‚´ãƒª')\n",
    "    \n",
    "    # ä¸è¦åˆ—é™¤å¤–\n",
    "    exclude_cols = ['åº—èˆ—', 'å•†å“å', 'æ—¥ä»˜', 'å£²ä¸Šé‡‘é¡', 'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šå¤§åˆ†é¡',\n",
    "                   'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šä¸­åˆ†é¡', 'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šå°åˆ†é¡']\n",
    "    \n",
    "    # category_lã‚’ãƒ€ãƒŸãƒ¼å¤‰æ•°åŒ–\n",
    "    cat_dummies = pd.get_dummies(unified_data['category_l'], prefix='cat')\n",
    "    \n",
    "    feature_cols = [c for c in unified_data.columns if c not in exclude_cols and c != 'å£²ä¸Šæ•°é‡' and c != 'category_l']\n",
    "    numeric_cols = unified_data[feature_cols].select_dtypes(include=GPU_MODELS + ['et', 'rf', 'gbr', 'dt']).columns.tolist()\n",
    "    \n",
    "    model_data = pd.concat([\n",
    "        unified_data[numeric_cols + ['å£²ä¸Šæ•°é‡']],\n",
    "        cat_dummies\n",
    "    ], axis=1).dropna()\n",
    "    \n",
    "    print(f'æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿: {len(model_data)}è¡Œ, {len(model_data.columns)-1}ç‰¹å¾´é‡')\n",
    "    \n",
    "    # PyCaret setup\n",
    "    s = setup(\n",
    "        model_data,\n",
    "        target='å£²ä¸Šæ•°é‡',\n",
    "        session_id=123,\n",
    "        train_size=0.8,\n",
    "        fold=10,  # ã‚°ãƒ«ãƒ¼ãƒ—Cã¯å®‰å®šã—ã¦ã„ã‚‹ã®ã§foldæ•°å¢—ã‚„ã™\n",
    "        normalize=True,\n",
    "        silent=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒï¼ˆå…¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰\n",
    "    print('\\nğŸ” # GPUé«˜é€ŸåŒ–: XGBoost/CatBoost GPUã‚’å„ªå…ˆä½¿ç”¨\n",
    "    compare_models()å®Ÿè¡Œä¸­ï¼ˆå…¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰...')\n",
    "    best_models = # GPUé«˜é€ŸåŒ–: XGBoost/CatBoost GPUã‚’å„ªå…ˆä½¿ç”¨\n",
    "    compare_models(\n",
    "        n_select=5,\n",
    "        sort='R2',\n",
    "        turbo=False,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    comparison_df = pull()\n",
    "    print('\\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœï¼ˆTop 5ï¼‰:')\n",
    "    print(comparison_df[['Model', 'R2', 'MAE', 'RMSE']].head())\n",
    "    \n",
    "    # éå­¦ç¿’æ¤œå‡º\n",
    "    best_model = best_models[0] if isinstance(best_models, list) else best_models\n",
    "    \n",
    "    X_train = get_config('X_train')\n",
    "    y_train = get_config('y_train')\n",
    "    X_test = get_config('X_test')\n",
    "    y_test = get_config('y_test')\n",
    "    \n",
    "    overfitting_result = detect_overfitting(\n",
    "        best_model, X_train, y_train, X_test, y_test,\n",
    "        model_name=f'GroupC - {best_model.__class__.__name__}'\n",
    "    )\n",
    "    \n",
    "    print(f'\\nğŸ”¬ éå­¦ç¿’æ¤œå‡ºçµæœ:')\n",
    "    print(f'  Train RÂ²: {overfitting_result[\"r2_train\"]:.4f}')\n",
    "    print(f'  Test RÂ²: {overfitting_result[\"r2_test\"]:.4f}')\n",
    "    print(f'  RÂ²ã‚®ãƒ£ãƒƒãƒ—: {overfitting_result[\"r2_gap\"]:.4f}')\n",
    "    print(f'  éå­¦ç¿’åˆ¤å®š: {\"ã¯ã„\" if overfitting_result[\"is_overfitting\"] else \"ã„ã„ãˆ\"}')\n",
    "    \n",
    "    group_c_results.append({\n",
    "        'ã‚«ãƒ†ã‚´ãƒª': 'GroupCçµ±åˆ',\n",
    "        'ã‚°ãƒ«ãƒ¼ãƒ—': 'C',\n",
    "        'ãƒ‡ãƒ¼ã‚¿æ•°': len(model_data),\n",
    "        'ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«': best_model.__class__.__name__,\n",
    "        'R2_Test': overfitting_result['r2_test'],\n",
    "        'R2_Train': overfitting_result['r2_train'],\n",
    "        'R2_Gap': overfitting_result['r2_gap'],\n",
    "        'éå­¦ç¿’': overfitting_result['is_overfitting'],\n",
    "        'æ·±åˆ»åº¦': overfitting_result['overfitting_severity'],\n",
    "        'MAE_Test': overfitting_result['mae_test'],\n",
    "        'RMSE_Test': overfitting_result['rmse_test']\n",
    "    })\n",
    "    \n",
    "    # Learning Curve\n",
    "    fig, gap = plot_learning_curve(best_model, X_train, y_train,\n",
    "                                   model_name='GroupCçµ±åˆ', cv=10)\n",
    "    fig.savefig('output/learning_curves/learning_curve_C_unified.png',\n",
    "               dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print('\\nâœ… ã‚°ãƒ«ãƒ¼ãƒ—Cåˆ†æå®Œäº†')\n",
    "else:\n",
    "    print('âš ï¸ ã‚°ãƒ«ãƒ¼ãƒ—Cã®ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 7. ç·åˆåˆ†æçµæœã®ã¾ã¨ã‚\n",
    "# ========================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('ğŸ“Š ç·åˆåˆ†æçµæœ')\n",
    "print('='*80)\n",
    "\n",
    "# å…¨çµæœã‚’çµ±åˆ\n",
    "all_results = group_a_results + group_b_results + group_c_results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    # ã‚½ãƒ¼ãƒˆ\n",
    "    results_df = results_df.sort_values('R2_Test', ascending=False)\n",
    "    \n",
    "    print('\\nã€å…¨ã‚«ãƒ†ã‚´ãƒªãƒ»ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€‘')\n",
    "    print(results_df[['ã‚«ãƒ†ã‚´ãƒª', 'ã‚°ãƒ«ãƒ¼ãƒ—', 'ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«', 'R2_Test', 'R2_Gap',\n",
    "                      'éå­¦ç¿’', 'æ·±åˆ»åº¦', 'ãƒ‡ãƒ¼ã‚¿æ•°']].to_string(index=False))\n",
    "    \n",
    "    # ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ã‚µãƒãƒªãƒ¼\n",
    "    print('\\n' + '='*80)\n",
    "    print('ğŸ“ˆ ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼')\n",
    "    print('='*80)\n",
    "    \n",
    "    group_summary = results_df.groupby('ã‚°ãƒ«ãƒ¼ãƒ—').agg({\n",
    "        'R2_Test': ['mean', 'std', 'min', 'max'],\n",
    "        'R2_Gap': ['mean', 'max'],\n",
    "        'éå­¦ç¿’': lambda x: (x == True).sum(),\n",
    "        'ã‚«ãƒ†ã‚´ãƒª': 'count'\n",
    "    }).round(4)\n",
    "    \n",
    "    print(group_summary)\n",
    "    \n",
    "    # éå­¦ç¿’ã‚«ãƒ†ã‚´ãƒªã®è­¦å‘Š\n",
    "    overfitted = results_df[results_df['éå­¦ç¿’'] == True]\n",
    "    \n",
    "    if len(overfitted) > 0:\n",
    "        print('\\n' + '='*80)\n",
    "        print('âš ï¸ éå­¦ç¿’ãŒæ¤œå‡ºã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒª')\n",
    "        print('='*80)\n",
    "        print(overfitted[['ã‚«ãƒ†ã‚´ãƒª', 'ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«', 'R2_Train', 'R2_Test',\n",
    "                         'R2_Gap', 'æ·±åˆ»åº¦']].to_string(index=False))\n",
    "        \n",
    "        print('\\nğŸ’¡ éå­¦ç¿’å¯¾ç­–æ¨å¥¨:')\n",
    "        print('  1. ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯æœŸé–“å»¶é•·ï¼‰')\n",
    "        print('  2. ç‰¹å¾´é‡å‰Šæ¸›ï¼ˆfeature_selection_thresholdèª¿æ•´ï¼‰')\n",
    "        print('  3. æ­£å‰‡åŒ–å¼·åŒ–ï¼ˆLightGBM/XGBoostã®reg_alpha, reg_lambdaèª¿æ•´ï¼‰')\n",
    "        print('  4. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ï¼ˆãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã€ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ï¼‰')\n",
    "        print('  5. ã‚ˆã‚Šå˜ç´”ãªãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™ï¼ˆæ±ºå®šæœ¨ â†’ ç·šå½¢å›å¸°ãªã©ï¼‰')\n",
    "    else:\n",
    "        print('\\nâœ… éå­¦ç¿’ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«å¥å…¨ï¼‰')\n",
    "    \n",
    "    # CSVä¿å­˜\n",
    "    output_path = Path('output/category_compare_models_results.csv')\n",
    "    results_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    print(f'\\nâœ… çµæœã‚’CSVä¿å­˜: {output_path}')\n",
    "    \n",
    "    # æœ€çµ‚æ¨å¥¨\n",
    "    print('\\n' + '='*80)\n",
    "    print('ğŸ¯ æœ€çµ‚æ¨å¥¨ãƒ¢ãƒ‡ãƒ«æˆ¦ç•¥')\n",
    "    print('='*80)\n",
    "    \n",
    "    for idx, row in results_df.iterrows():\n",
    "        status = 'âœ… æ¡ç”¨æ¨å¥¨' if not row['éå­¦ç¿’'] and row['R2_Test'] > 0.6 else \\\n",
    "                 'âš ï¸ è¦æ”¹å–„' if row['éå­¦ç¿’'] else \\\n",
    "                 'âŒ å†æ¤œè¨'\n",
    "        \n",
    "        print(f\"{status} {row['ã‚«ãƒ†ã‚´ãƒª']} ({row['ã‚°ãƒ«ãƒ¼ãƒ—']}): \"\n",
    "              f\"{row['ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«']} (RÂ²={row['R2_Test']:.3f}, Gap={row['R2_Gap']:.3f})\")\n",
    "else:\n",
    "    print('âš ï¸ åˆ†æçµæœãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸ')\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('ğŸ‰ å…¨åˆ†æå®Œäº†ï¼')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "store_analysis_header",
   "source": [
    "# åº—èˆ—åˆ¥åˆ†æ\n",
    "\n",
    "## ç›®çš„\n",
    "å…¨åº—èˆ—çµ±åˆãƒ¢ãƒ‡ãƒ« vs åº—èˆ—åˆ¥å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ\n",
    "\n",
    "**ä»®èª¬:**\n",
    "- åº—èˆ—ã”ã¨ã«é¡§å®¢å±æ€§ãƒ»ç«‹åœ°ç‰¹æ€§ãŒç•°ãªã‚‹\n",
    "- åº—èˆ—åˆ¥ãƒ¢ãƒ‡ãƒ«ã®æ–¹ãŒç²¾åº¦ãŒé«˜ã„å¯èƒ½æ€§\n",
    "- ãŸã ã—ã€ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã§éå­¦ç¿’ã®ãƒªã‚¹ã‚¯ã‚‚"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "store_all_unified",
   "source": [
    "# ========================================\n",
    "# 8. å…¨åº—èˆ—çµ±åˆãƒ¢ãƒ‡ãƒ«ï¼ˆåº—èˆ—ã‚’ãƒ€ãƒŸãƒ¼å¤‰æ•°åŒ–ï¼‰\n",
    "# ========================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('ğŸª å…¨åº—èˆ—çµ±åˆãƒ¢ãƒ‡ãƒ«åˆ†æï¼ˆåº—èˆ—ã‚’ãƒ€ãƒŸãƒ¼å¤‰æ•°åŒ–ï¼‰')\n",
    "print('='*80)\n",
    "\n",
    "# å…¨ãƒ‡ãƒ¼ã‚¿ï¼ˆå…¨ã‚«ãƒ†ã‚´ãƒªÃ—å…¨åº—èˆ—ï¼‰\n",
    "all_store_data = data.copy()\n",
    "\n",
    "print(f'\\nå…¨åº—èˆ—ãƒ‡ãƒ¼ã‚¿: {len(all_store_data)}è¡Œ')\n",
    "print(f'åº—èˆ—æ•°: {all_store_data[\"åº—èˆ—\"].nunique()}')\n",
    "\n",
    "# ä¸è¦åˆ—é™¤å¤–\n",
    "exclude_cols = ['å•†å“å', 'æ—¥ä»˜', 'å£²ä¸Šé‡‘é¡', 'category_l',\n",
    "               'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šå¤§åˆ†é¡', 'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šä¸­åˆ†é¡', 'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šå°åˆ†é¡']\n",
    "\n",
    "# åº—èˆ—ã‚’ãƒ€ãƒŸãƒ¼å¤‰æ•°åŒ–\n",
    "store_dummies = pd.get_dummies(all_store_data['åº—èˆ—'], prefix='store')\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªã‚‚ãƒ€ãƒŸãƒ¼å¤‰æ•°åŒ–\n",
    "if 'category_l' in all_store_data.columns:\n",
    "    category_dummies = pd.get_dummies(all_store_data['category_l'], prefix='cat')\n",
    "else:\n",
    "    category_dummies = pd.DataFrame()\n",
    "\n",
    "feature_cols = [c for c in all_store_data.columns \n",
    "               if c not in exclude_cols + ['å£²ä¸Šæ•°é‡', 'åº—èˆ—']]\n",
    "numeric_cols = all_store_data[feature_cols].select_dtypes(include=GPU_MODELS + ['et', 'rf', 'gbr', 'dt']).columns.tolist()\n",
    "\n",
    "model_data_all = pd.concat([\n",
    "    all_store_data[numeric_cols + ['å£²ä¸Šæ•°é‡']],\n",
    "    store_dummies,\n",
    "    category_dummies\n",
    "], axis=1).dropna()\n",
    "\n",
    "print(f'æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿: {len(model_data_all)}è¡Œ, {len(model_data_all.columns)-1}ç‰¹å¾´é‡')\n",
    "\n",
    "# PyCaret setup\n",
    "s_all = setup(\n",
    "    model_data_all,\n",
    "    target='å£²ä¸Šæ•°é‡',\n",
    "    session_id=123,\n",
    "    train_size=0.8,\n",
    "    fold=10,\n",
    "    normalize=True,\n",
    "    remove_multicollinearity=True,\n",
    "    multicollinearity_threshold=0.95,\n",
    "    silent=True,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ\n",
    "print('\\nğŸ” # GPUé«˜é€ŸåŒ–: XGBoost/CatBoost GPUã‚’å„ªå…ˆä½¿ç”¨\n",
    "    # æ³¨: Top 20ç‰¹å¾´é‡ãŒè‡ªå‹•çš„ã«è€ƒæ…®ã•ã‚Œã¾ã™ï¼ˆPyCaret feature_importanceï¼‰\n",
    "    compare_models()å®Ÿè¡Œä¸­ï¼ˆå…¨åº—èˆ—çµ±åˆï¼‰...')\n",
    "best_models_all = # GPUé«˜é€ŸåŒ–: XGBoost/CatBoost GPUã‚’å„ªå…ˆä½¿ç”¨\n",
    "    compare_models(\n",
    "    n_select=5,\n",
    "    sort='R2',\n",
    "    turbo=False,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "comparison_all = pull()\n",
    "print('\\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœï¼ˆTop 5ï¼‰:')\n",
    "print(comparison_all[['Model', 'R2', 'MAE', 'RMSE']].head())\n",
    "\n",
    "# éå­¦ç¿’æ¤œå‡º\n",
    "best_model_all = best_models_all[0] if isinstance(best_models_all, list) else best_models_all\n",
    "\n",
    "X_train_all = get_config('X_train')\n",
    "y_train_all = get_config('y_train')\n",
    "X_test_all = get_config('X_test')\n",
    "y_test_all = get_config('y_test')\n",
    "\n",
    "overfitting_all = detect_overfitting(\n",
    "    best_model_all, X_train_all, y_train_all, X_test_all, y_test_all,\n",
    "    model_name=f'AllStores - {best_model_all.__class__.__name__}'\n",
    ")\n",
    "\n",
    "print(f'\\nğŸ”¬ éå­¦ç¿’æ¤œå‡ºçµæœ:')\n",
    "print(f'  Train RÂ²: {overfitting_all[\"r2_train\"]:.4f}')\n",
    "print(f'  Test RÂ²: {overfitting_all[\"r2_test\"]:.4f}')\n",
    "print(f'  RÂ²ã‚®ãƒ£ãƒƒãƒ—: {overfitting_all[\"r2_gap\"]:.4f}')\n",
    "print(f'  éå­¦ç¿’åˆ¤å®š: {\"ã¯ã„\" if overfitting_all[\"is_overfitting\"] else \"ã„ã„ãˆ\"}')\n",
    "\n",
    "# çµæœä¿å­˜\n",
    "all_store_result = {\n",
    "    'åˆ†æã‚¿ã‚¤ãƒ—': 'å…¨åº—èˆ—çµ±åˆ',\n",
    "    'åº—èˆ—': 'å…¨åº—èˆ—',\n",
    "    'ãƒ‡ãƒ¼ã‚¿æ•°': len(model_data_all),\n",
    "    'ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«': best_model_all.__class__.__name__,\n",
    "    'R2_Test': overfitting_all['r2_test'],\n",
    "    'R2_Train': overfitting_all['r2_train'],\n",
    "    'R2_Gap': overfitting_all['r2_gap'],\n",
    "    'éå­¦ç¿’': overfitting_all['is_overfitting'],\n",
    "    'æ·±åˆ»åº¦': overfitting_all['overfitting_severity'],\n",
    "    'MAE_Test': overfitting_all['mae_test'],\n",
    "    'RMSE_Test': overfitting_all['rmse_test']\n",
    "}\n",
    "\n",
    "# Learning Curve\n",
    "fig_all, gap_all = plot_learning_curve(best_model_all, X_train_all, y_train_all,\n",
    "                                       model_name='å…¨åº—èˆ—çµ±åˆ', cv=10)\n",
    "fig_all.savefig('output/learning_curves/learning_curve_AllStores.png',\n",
    "               dpi=150, bbox_inches='tight')\n",
    "plt.close(fig_all)\n",
    "\n",
    "print('\\nâœ… å…¨åº—èˆ—çµ±åˆãƒ¢ãƒ‡ãƒ«åˆ†æå®Œäº†')\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "store_individual",
   "source": [
    "# ========================================\n",
    "# 9. åº—èˆ—åˆ¥å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«åˆ†æ\n",
    "# ========================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('ğŸª åº—èˆ—åˆ¥å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«åˆ†æ')\n",
    "print('='*80)\n",
    "\n",
    "store_results = []\n",
    "\n",
    "# åº—èˆ—ãƒªã‚¹ãƒˆå–å¾—\n",
    "stores = data['åº—èˆ—'].unique()\n",
    "print(f'\\nå¯¾è±¡åº—èˆ—æ•°: {len(stores)}åº—èˆ—')\n",
    "print(f'åº—èˆ—ãƒªã‚¹ãƒˆ: {stores}')\n",
    "\n",
    "for store in stores:\n",
    "    print(f'\\n--- åº—èˆ—: {store} ---')\n",
    "    \n",
    "    # åº—èˆ—ãƒ‡ãƒ¼ã‚¿æŠ½å‡º\n",
    "    store_data = data[data['åº—èˆ—'] == store].copy()\n",
    "    \n",
    "    if len(store_data) < 500:\n",
    "        print(f'âš ï¸ ãƒ‡ãƒ¼ã‚¿ä¸è¶³ ({len(store_data)}è¡Œ) - ã‚¹ã‚­ãƒƒãƒ—')\n",
    "        continue\n",
    "    \n",
    "    print(f'ãƒ‡ãƒ¼ã‚¿æ•°: {len(store_data)}è¡Œ')\n",
    "    print(f'ã‚«ãƒ†ã‚´ãƒªæ•°: {store_data[\"category_l\"].nunique()}')\n",
    "    \n",
    "    # ä¸è¦åˆ—é™¤å¤–\n",
    "    exclude_cols = ['åº—èˆ—', 'å•†å“å', 'æ—¥ä»˜', 'å£²ä¸Šé‡‘é¡',\n",
    "                   'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šå¤§åˆ†é¡', 'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šä¸­åˆ†é¡', 'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šå°åˆ†é¡']\n",
    "    \n",
    "    # ã‚«ãƒ†ã‚´ãƒªã‚’ãƒ€ãƒŸãƒ¼å¤‰æ•°åŒ–\n",
    "    if 'category_l' in store_data.columns:\n",
    "        category_dummies = pd.get_dummies(store_data['category_l'], prefix='cat')\n",
    "    else:\n",
    "        category_dummies = pd.DataFrame()\n",
    "    \n",
    "    feature_cols = [c for c in store_data.columns \n",
    "                   if c not in exclude_cols + ['å£²ä¸Šæ•°é‡', 'category_l']]\n",
    "    numeric_cols = store_data[feature_cols].select_dtypes(include=GPU_MODELS + ['et', 'rf', 'gbr', 'dt']).columns.tolist()\n",
    "    \n",
    "    model_data_store = pd.concat([\n",
    "        store_data[numeric_cols + ['å£²ä¸Šæ•°é‡']],\n",
    "        category_dummies\n",
    "    ], axis=1).dropna()\n",
    "    \n",
    "    if len(model_data_store) < 100:\n",
    "        print(f'âš ï¸ æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ä¸è¶³ ({len(model_data_store)}è¡Œ) - ã‚¹ã‚­ãƒƒãƒ—')\n",
    "        continue\n",
    "    \n",
    "    print(f'æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿: {len(model_data_store)}è¡Œ, {len(model_data_store.columns)-1}ç‰¹å¾´é‡')\n",
    "    \n",
    "    # PyCaret setup\n",
    "    s_store = setup(\n",
    "        model_data_store,\n",
    "        target='å£²ä¸Šæ•°é‡',\n",
    "        session_id=123,\n",
    "        train_size=0.8,\n",
    "        fold=5,\n",
    "        normalize=True,\n",
    "        remove_multicollinearity=True,\n",
    "        multicollinearity_threshold=0.95,\n",
    "        silent=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ\n",
    "    print(f'\\nğŸ” # GPUé«˜é€ŸåŒ–: XGBoost/CatBoost GPUã‚’å„ªå…ˆä½¿ç”¨\n",
    "    # æ³¨: Top 20ç‰¹å¾´é‡ãŒè‡ªå‹•çš„ã«è€ƒæ…®ã•ã‚Œã¾ã™ï¼ˆPyCaret feature_importanceï¼‰\n",
    "    compare_models()å®Ÿè¡Œä¸­ï¼ˆåº—èˆ—: {store}ï¼‰...')\n",
    "    best_models_store = # GPUé«˜é€ŸåŒ–: XGBoost/CatBoost GPUã‚’å„ªå…ˆä½¿ç”¨\n",
    "    compare_models(\n",
    "        n_select=5,\n",
    "        sort='R2',\n",
    "        turbo=False,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    comparison_store = pull()\n",
    "    print('\\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœï¼ˆTop 5ï¼‰:')\n",
    "    print(comparison_store[['Model', 'R2', 'MAE', 'RMSE']].head())\n",
    "    \n",
    "    # éå­¦ç¿’æ¤œå‡º\n",
    "    best_model_store = best_models_store[0] if isinstance(best_models_store, list) else best_models_store\n",
    "    \n",
    "    X_train_store = get_config('X_train')\n",
    "    y_train_store = get_config('y_train')\n",
    "    X_test_store = get_config('X_test')\n",
    "    y_test_store = get_config('y_test')\n",
    "    \n",
    "    overfitting_store = detect_overfitting(\n",
    "        best_model_store, X_train_store, y_train_store, X_test_store, y_test_store,\n",
    "        model_name=f'{store} - {best_model_store.__class__.__name__}'\n",
    "    )\n",
    "    \n",
    "    print(f'\\nğŸ”¬ éå­¦ç¿’æ¤œå‡ºçµæœ:')\n",
    "    print(f'  Train RÂ²: {overfitting_store[\"r2_train\"]:.4f}')\n",
    "    print(f'  Test RÂ²: {overfitting_store[\"r2_test\"]:.4f}')\n",
    "    print(f'  RÂ²ã‚®ãƒ£ãƒƒãƒ—: {overfitting_store[\"r2_gap\"]:.4f}')\n",
    "    print(f'  éå­¦ç¿’åˆ¤å®š: {\"ã¯ã„\" if overfitting_store[\"is_overfitting\"] else \"ã„ã„ãˆ\"}')\n",
    "    \n",
    "    # çµæœä¿å­˜\n",
    "    store_results.append({\n",
    "        'åˆ†æã‚¿ã‚¤ãƒ—': 'åº—èˆ—åˆ¥',\n",
    "        'åº—èˆ—': store,\n",
    "        'ãƒ‡ãƒ¼ã‚¿æ•°': len(model_data_store),\n",
    "        'ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«': best_model_store.__class__.__name__,\n",
    "        'R2_Test': overfitting_store['r2_test'],\n",
    "        'R2_Train': overfitting_store['r2_train'],\n",
    "        'R2_Gap': overfitting_store['r2_gap'],\n",
    "        'éå­¦ç¿’': overfitting_store['is_overfitting'],\n",
    "        'æ·±åˆ»åº¦': overfitting_store['overfitting_severity'],\n",
    "        'MAE_Test': overfitting_store['mae_test'],\n",
    "        'RMSE_Test': overfitting_store['rmse_test']\n",
    "    })\n",
    "    \n",
    "    # Learning Curve\n",
    "    fig_store, gap_store = plot_learning_curve(\n",
    "        best_model_store, X_train_store, y_train_store,\n",
    "        model_name=f'åº—èˆ—{store}', cv=5\n",
    "    )\n",
    "    fig_store.savefig(f'output/learning_curves/learning_curve_Store_{store}.png',\n",
    "                     dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig_store)\n",
    "    \n",
    "    print(f'âœ… åº—èˆ—{store}åˆ†æå®Œäº†')\n",
    "\n",
    "print(f'\\nâœ… åº—èˆ—åˆ¥åˆ†æå®Œäº†: {len(store_results)}åº—èˆ—')\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "store_comparison",
   "source": [
    "# ========================================\n",
    "# 10. å…¨åº—èˆ—çµ±åˆ vs åº—èˆ—åˆ¥ æ¯”è¼ƒåˆ†æ\n",
    "# ========================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('ğŸ“Š å…¨åº—èˆ—çµ±åˆ vs åº—èˆ—åˆ¥ãƒ¢ãƒ‡ãƒ« æ¯”è¼ƒåˆ†æ')\n",
    "print('='*80)\n",
    "\n",
    "# å…¨çµæœã‚’çµ±åˆ\n",
    "store_comparison_results = [all_store_result] + store_results\n",
    "store_comparison_df = pd.DataFrame(store_comparison_results)\n",
    "\n",
    "if len(store_comparison_df) > 0:\n",
    "    print('\\nã€å…¨åº—èˆ—çµ±åˆ vs åº—èˆ—åˆ¥ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã€‘')\n",
    "    print(store_comparison_df[['åˆ†æã‚¿ã‚¤ãƒ—', 'åº—èˆ—', 'ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«', 'R2_Test',\n",
    "                              'R2_Gap', 'éå­¦ç¿’', 'æ·±åˆ»åº¦', 'ãƒ‡ãƒ¼ã‚¿æ•°']].to_string(index=False))\n",
    "    \n",
    "    # çµ±è¨ˆã‚µãƒãƒªãƒ¼\n",
    "    print('\\n' + '='*80)\n",
    "    print('ğŸ“ˆ æˆ¦ç•¥åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼')\n",
    "    print('='*80)\n",
    "    \n",
    "    strategy_summary = store_comparison_df.groupby('åˆ†æã‚¿ã‚¤ãƒ—').agg({\n",
    "        'R2_Test': ['mean', 'std', 'min', 'max'],\n",
    "        'R2_Gap': ['mean', 'max'],\n",
    "        'éå­¦ç¿’': lambda x: (x == True).sum(),\n",
    "        'åº—èˆ—': 'count'\n",
    "    }).round(4)\n",
    "    \n",
    "    print(strategy_summary)\n",
    "    \n",
    "    # æœ€é©æˆ¦ç•¥ã®åˆ¤å®š\n",
    "    all_store_r2 = store_comparison_df[store_comparison_df['åˆ†æã‚¿ã‚¤ãƒ—'] == 'å…¨åº—èˆ—çµ±åˆ']['R2_Test'].iloc[0]\n",
    "    per_store_avg_r2 = store_comparison_df[store_comparison_df['åˆ†æã‚¿ã‚¤ãƒ—'] == 'åº—èˆ—åˆ¥']['R2_Test'].mean()\n",
    "    \n",
    "    print('\\n' + '='*80)\n",
    "    print('ğŸ¯ æœ€é©ãƒ¢ãƒ‡ãƒªãƒ³ã‚°æˆ¦ç•¥ã®åˆ¤å®š')\n",
    "    print('='*80)\n",
    "    \n",
    "    print(f'\\nå…¨åº—èˆ—çµ±åˆãƒ¢ãƒ‡ãƒ« RÂ²: {all_store_r2:.4f}')\n",
    "    print(f'åº—èˆ—åˆ¥ãƒ¢ãƒ‡ãƒ«å¹³å‡ RÂ²: {per_store_avg_r2:.4f}')\n",
    "    print(f'å·®åˆ†: {per_store_avg_r2 - all_store_r2:.4f} ({(per_store_avg_r2 - all_store_r2)/all_store_r2:.2%})')\n",
    "    \n",
    "    # åˆ¤å®šåŸºæº–\n",
    "    if per_store_avg_r2 > all_store_r2 + 0.05:  # 5%ä»¥ä¸Šæ”¹å–„\n",
    "        recommendation = 'âœ… åº—èˆ—åˆ¥ãƒ¢ãƒ‡ãƒ«æ¨å¥¨'\n",
    "        reason = f'åº—èˆ—åˆ¥ãƒ¢ãƒ‡ãƒ«ãŒ{(per_store_avg_r2 - all_store_r2)/all_store_r2:.1%}æ”¹å–„')\n",
    "    elif per_store_avg_r2 < all_store_r2 - 0.05:  # 5%ä»¥ä¸Šæ‚ªåŒ–\n",
    "        recommendation = 'âœ… å…¨åº—èˆ—çµ±åˆãƒ¢ãƒ‡ãƒ«æ¨å¥¨'\n",
    "        reason = f'çµ±åˆãƒ¢ãƒ‡ãƒ«ãŒ{(all_store_r2 - per_store_avg_r2)/all_store_r2:.1%}å„ªä½')\n",
    "    else:  # Â±5%ä»¥å†…\n",
    "        recommendation = 'âš–ï¸ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æˆ¦ç•¥æ¨å¥¨'\n",
    "        reason = 'å·®åˆ†ãŒå°ã•ã„ãŸã‚ã€åº—èˆ—ç‰¹æ€§ã«å¿œã˜ã¦ä½¿ã„åˆ†ã‘'\n",
    "    \n",
    "    print(f'\\n{recommendation}')\n",
    "    print(f'ç†ç”±: {reason}')\n",
    "    \n",
    "    # è©³ç´°æ¨å¥¨\n",
    "    print('\\nğŸ’¡ å®Ÿè£…æ¨å¥¨:')\n",
    "    if 'åº—èˆ—åˆ¥' in recommendation:\n",
    "        print('  1. å„åº—èˆ—ã§å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤')\n",
    "        print('  2. æ–°è¦åº—èˆ—ã¯çµ±åˆãƒ¢ãƒ‡ãƒ«ã§é–‹å§‹ã€ãƒ‡ãƒ¼ã‚¿è“„ç©å¾Œã«å€‹åˆ¥åŒ–')\n",
    "        print('  3. å®šæœŸçš„ã«ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’ï¼ˆæœˆæ¬¡æ¨å¥¨ï¼‰')\n",
    "    elif 'çµ±åˆ' in recommendation:\n",
    "        print('  1. å…¨åº—èˆ—ã§1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’å…±æœ‰ï¼ˆãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰')\n",
    "        print('  2. åº—èˆ—ãƒ€ãƒŸãƒ¼å¤‰æ•°ã§åº—èˆ—ç‰¹æ€§ã‚’å¸å')\n",
    "        print('  3. åº—èˆ—å›ºæœ‰ã®ç‰¹å¾´é‡ã‚’è¿½åŠ æ¤œè¨ï¼ˆç«‹åœ°ã€å•†åœãªã©ï¼‰')\n",
    "    else:  # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰\n",
    "        print('  1. ãƒ‡ãƒ¼ã‚¿é‡ãŒè±Šå¯Œãªåº—èˆ— â†’ å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«')\n",
    "        print('  2. ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®åº—èˆ— â†’ çµ±åˆãƒ¢ãƒ‡ãƒ«')\n",
    "        print('  3. é–¾å€¤: 1åº—èˆ—ã‚ãŸã‚Š5000è¡Œä»¥ä¸Šãªã‚‰å€‹åˆ¥åŒ–')\n",
    "    \n",
    "    # åº—èˆ—åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n",
    "    print('\\n' + '='*80)\n",
    "    print('ğŸ† åº—èˆ—åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ©ãƒ³ã‚­ãƒ³ã‚°')\n",
    "    print('='*80)\n",
    "    \n",
    "    store_only = store_comparison_df[store_comparison_df['åˆ†æã‚¿ã‚¤ãƒ—'] == 'åº—èˆ—åˆ¥'].copy()\n",
    "    if len(store_only) > 0:\n",
    "        store_only_sorted = store_only.sort_values('R2_Test', ascending=False)\n",
    "        print(store_only_sorted[['åº—èˆ—', 'ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«', 'R2_Test', 'R2_Gap',\n",
    "                                'éå­¦ç¿’', 'ãƒ‡ãƒ¼ã‚¿æ•°']].to_string(index=False))\n",
    "        \n",
    "        # ãƒ™ã‚¹ãƒˆåº—èˆ—ã¨ãƒ¯ãƒ¼ã‚¹ãƒˆåº—èˆ—\n",
    "        best_store = store_only_sorted.iloc[0]\n",
    "        worst_store = store_only_sorted.iloc[-1]\n",
    "        \n",
    "        print(f'\\nğŸ¥‡ æœ€é«˜ç²¾åº¦åº—èˆ—: {best_store[\"åº—èˆ—\"]} (RÂ²={best_store[\"R2_Test\"]:.4f})')\n",
    "        print(f'   ãƒ¢ãƒ‡ãƒ«: {best_store[\"ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«\"]}')\n",
    "        print(f'   ãƒ‡ãƒ¼ã‚¿æ•°: {best_store[\"ãƒ‡ãƒ¼ã‚¿æ•°\"]:,}è¡Œ')\n",
    "        \n",
    "        print(f'\\nâš ï¸ æ”¹å–„å¿…è¦åº—èˆ—: {worst_store[\"åº—èˆ—\"]} (RÂ²={worst_store[\"R2_Test\"]:.4f})')\n",
    "        print(f'   ãƒ¢ãƒ‡ãƒ«: {worst_store[\"ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«\"]}')\n",
    "        print(f'   ãƒ‡ãƒ¼ã‚¿æ•°: {worst_store[\"ãƒ‡ãƒ¼ã‚¿æ•°\"]:,}è¡Œ')\n",
    "        print(f'   æ”¹å–„ç­–: ãƒ‡ãƒ¼ã‚¿æœŸé–“å»¶é•·ã€ç‰¹å¾´é‡è¦‹ç›´ã—ã€çµ±åˆãƒ¢ãƒ‡ãƒ«åˆ©ç”¨æ¤œè¨')\n",
    "    \n",
    "    # CSVä¿å­˜\n",
    "    output_path = Path('output/store_comparison_results.csv')\n",
    "    store_comparison_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    print(f'\\nâœ… çµæœã‚’CSVä¿å­˜: {output_path}')\n",
    "\n",
    "else:\n",
    "    print('âš ï¸ æ¯”è¼ƒçµæœãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸ')\n",
    "\n",
    "print('\\nâœ… åº—èˆ—åˆ¥åˆ†æå®Œäº†ï¼')\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}