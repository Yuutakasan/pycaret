"""
ã™ã¹ã¦ã®æ®‹å­˜è‹±èªã‚’æ—¥æœ¬èªã«å®Œå…¨ç¿»è¨³
"""

import json
import re
from pathlib import Path

# åŒ…æ‹¬çš„ãªè‹±èªâ†’æ—¥æœ¬èªç¿»è¨³ãƒãƒƒãƒ—
COMPREHENSIVE_TRANSLATIONS = {
    # ã‚°ãƒ©ãƒ•ã‚¿ã‚¤ãƒˆãƒ«
    'Feature Importance for Sales Prediction': 'å£²ä¸Šäºˆæ¸¬ã®ç‰¹å¾´é‡é‡è¦åº¦',
    'Feature Importance by Category': 'ã‚«ãƒ†ã‚´ãƒªåˆ¥ç‰¹å¾´é‡é‡è¦åº¦',
    'SHAP Feature Importance': 'SHAPç‰¹å¾´é‡é‡è¦åº¦',
    'Customer Count YoY Growth (%)': 'å®¢æ•°å‰å¹´æ¯”æˆé•·ç‡ (%)',
    'Sales Distribution by Time Period': 'æ™‚é–“å¸¯åˆ¥å£²ä¸Šåˆ†å¸ƒ',
    'Weekday x Hour Heatmap': 'æ›œæ—¥Ã—æ™‚é–“å¸¯ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—',
    'Hour of Day': 'æ™‚åˆ»',
    'Coefficient of Variation': 'å¤‰å‹•ä¿‚æ•°',
    'Average Sales': 'å¹³å‡å£²ä¸Š',
    'Weekday data not available': 'æ›œæ—¥ãƒ‡ãƒ¼ã‚¿ãªã—',

    # KPIãƒ†ã‚­ã‚¹ãƒˆ
    "KPIs (Latest):": "ä¸»è¦æŒ‡æ¨™ (æœ€æ–°):",
    "Sales:": "å£²ä¸Š:",
    "YoY:": "å‰å¹´æ¯”:",
    "Avg Spend:": "å®¢å˜ä¾¡:",
    "Trend (7d):": "ãƒˆãƒ¬ãƒ³ãƒ‰(7æ—¥):",

    # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
    "TODAY'S ACTIONS:": "æœ¬æ—¥ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:",
    "Check TOP10 inventory": "TOP10åœ¨åº«ç¢ºèª",
    "Adjust orders by weather": "å¤©å€™ã«ã‚ˆã‚‹ç™ºæ³¨èª¿æ•´",
    "Analyze YoY negative items": "å‰å¹´æ¯”æ¸›å°‘å•†å“åˆ†æ",
    "Compare with other stores": "ä»–åº—èˆ—ã¨ã®æ¯”è¼ƒ",

    # ã‚¢ãƒ©ãƒ¼ãƒˆãƒ¬ãƒ™ãƒ«
    "'ğŸ”´ Critical'": "'ğŸ”´ ç·Šæ€¥'",
    "'ğŸŸ¡ Warning'": "'ğŸŸ¡ è­¦å‘Š'",
    "Criticalï¼ˆç·Šæ€¥ï¼‰": "ç·Šæ€¥",
    "Warningï¼ˆè­¦å‘Šï¼‰": "è­¦å‘Š",

    # æ¤œå‡ºæ–¹æ³•
    "'IQR'": "'å››åˆ†ä½ç¯„å›²'",
    "'Isolation Forest'": "'å­¤ç«‹æ£®æ—'",
    "'MA Deviation'": "'ç§»å‹•å¹³å‡åå·®'",
    "'YoY Deviation'": "'å‰å¹´æ¯”åå·®'",

    # è©•ä¾¡æŒ‡æ¨™
    "'MAE'": "'å¹³å‡çµ¶å¯¾èª¤å·®'",
    "'RMSE'": "'äºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·®'",
    "'MAPE'": "'å¹³å‡çµ¶å¯¾ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆèª¤å·®'",
    "'CAGR'": "'å¹´å¹³å‡æˆé•·ç‡'",

    # ãã®ä»–
    "'YlOrRd'": "'YlOrRd'",  # ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—åã¯ä¿æŒ
    "per Customer": "é¡§å®¢ã‚ãŸã‚Š",
    "Last 30d": "éå»30æ—¥é–“",
}

# ç‰¹æ®Šãªãƒ‘ã‚¿ãƒ¼ãƒ³å‡¦ç†ç”¨
PATTERN_REPLACEMENTS = [
    # results['MAE'] ã®ã‚ˆã†ãªè¾æ›¸ã‚­ãƒ¼
    (r"results\['MAE'\]", "results['å¹³å‡çµ¶å¯¾èª¤å·®']"),
    (r"results\['RMSE'\]", "results['äºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·®']"),
    (r"results\['MAPE'\]", "results['å¹³å‡çµ¶å¯¾ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆèª¤å·®']"),

    # anomaly_results['alert_level'] == 'Critical'
    (r"== 'ğŸ”´ Critical'", "== 'ğŸ”´ ç·Šæ€¥'"),
    (r"== 'ğŸŸ¡ Warning'", "== 'ğŸŸ¡ è­¦å‘Š'"),

    # CAGRã®å¤‰æ•°åï¼ˆæ–‡å­—åˆ—ã®å ´åˆã®ã¿ï¼‰
    (r"'CAGR':", "'å¹´å¹³å‡æˆé•·ç‡':"),
    (r"nlargest\(10, 'CAGR'\)", "nlargest(10, 'å¹´å¹³å‡æˆé•·ç‡')"),
    (r"\['CAGR'\]", "['å¹´å¹³å‡æˆé•·ç‡']"),

    # sort='MAE'
    (r"sort='MAE'", "sort='å¹³å‡çµ¶å¯¾èª¤å·®'"),

    # detection_method =
    (r"detection_method'] = 'IQR'", "detection_method'] = 'å››åˆ†ä½ç¯„å›²'"),
    (r"detection_method'] = 'Isolation Forest'", "detection_method'] = 'å­¤ç«‹æ£®æ—'"),
    (r"detection_method'] = 'MA Deviation'", "detection_method'] = 'ç§»å‹•å¹³å‡åå·®'"),
    (r"detection_method'] = 'YoY Deviation'", "detection_method'] = 'å‰å¹´æ¯”åå·®'"),
]

def translate_all_english(notebook_path):
    """ã™ã¹ã¦ã®è‹±èªã‚’æ—¥æœ¬èªã«ç¿»è¨³"""

    with open(notebook_path, 'r', encoding='utf-8') as f:
        nb = json.load(f)

    fixed_count = 0

    for cell in nb['cells']:
        if cell.get('cell_type') != 'code':
            continue

        source = cell.get('source', [])
        if not isinstance(source, list):
            continue

        new_source = []
        modified = False

        for line in source:
            new_line = line

            # 1. ç›´æ¥çš„ãªæ–‡å­—åˆ—ç½®æ›
            for eng, jpn in COMPREHENSIVE_TRANSLATIONS.items():
                if eng in new_line:
                    new_line = new_line.replace(eng, jpn)
                    modified = True

            # 2. ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã®ç½®æ›
            for pattern, replacement in PATTERN_REPLACEMENTS:
                if re.search(pattern, new_line):
                    new_line = re.sub(pattern, replacement, new_line)
                    modified = True

            new_source.append(new_line)
            if new_line != line:
                fixed_count += 1

        if new_source != source:
            cell['source'] = new_source

    if fixed_count > 0:
        with open(notebook_path, 'w', encoding='utf-8') as f:
            json.dump(nb, f, ensure_ascii=False, indent=1)
        return fixed_count

    return 0


if __name__ == '__main__':
    notebooks = [
        '/mnt/d/github/pycaret/work/åº—èˆ—åˆ¥åŒ…æ‹¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰_v5.0_Phase1.ipynb',
        '/mnt/d/github/pycaret/work/åº—èˆ—åˆ¥åŒ…æ‹¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰_v5.0_Phase2.ipynb',
        '/mnt/d/github/pycaret/work/åº—èˆ—åˆ¥åŒ…æ‹¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰_v5.0_Phase3.ipynb',
        '/mnt/d/github/pycaret/work/åº—èˆ—åˆ¥åŒ…æ‹¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰_v5.0_Phase4.ipynb'
    ]

    print("\n" + "="*80)
    print("ğŸŒ æ®‹å­˜è‹±èªã®å®Œå…¨ç¿»è¨³".center(80))
    print("="*80)

    total_fixed = 0

    for nb_path in notebooks:
        path = Path(nb_path)
        if not path.exists():
            continue

        print(f"\nğŸ“ {path.name}")
        count = translate_all_english(nb_path)
        if count > 0:
            print(f"  âœ… {count}ç®‡æ‰€ã‚’ç¿»è¨³")
            total_fixed += count
        else:
            print(f"  â„¹ï¸ ç¿»è¨³ç®‡æ‰€ãªã—")

    print("\n" + "="*80)
    print(f"âœ… åˆè¨ˆ {total_fixed}ç®‡æ‰€ã‚’ç¿»è¨³å®Œäº†".center(80))
    print("="*80)
