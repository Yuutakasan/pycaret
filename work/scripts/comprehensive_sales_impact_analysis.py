#!/usr/bin/env python3
"""
åŒ…æ‹¬çš„å£²ä¸Šã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆåˆ†æ
æ ¼ç´ç‡80%ä»¥ä¸Šã®å…¨ç‰¹å¾´é‡ã«ã¤ã„ã¦å£²ä¸Šå½±éŸ¿åº¦ã‚’ç®—å‡º
"""

import pandas as pd
import numpy as np
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

print("\n" + "="*80)
print("ğŸ“Š åŒ…æ‹¬çš„å£²ä¸Šã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆåˆ†æ")
print("="*80)

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
data_path = Path('output/06_final_enriched_20250701_20250930.csv')
if not data_path.exists():
    print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_path}")
    exit(1)

df = pd.read_csv(data_path, encoding='utf-8-sig')
print(f"\nâœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,}è¡Œ Ã— {len(df.columns)}åˆ—")

# å£²ä¸Šåˆ—ã®ç¢ºèª
sales_col = 'å£²ä¸Šé‡‘é¡' if 'å£²ä¸Šé‡‘é¡' in df.columns else 'å£²ä¸Šæ•°é‡'
print(f"ğŸ“Š å£²ä¸ŠæŒ‡æ¨™: {sales_col}")

# ========================================
# 1. ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æï¼ˆæ ¼ç´ç‡80%ä»¥ä¸Šï¼‰
# ========================================

print("\n" + "="*80)
print("ğŸ” ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æ")
print("="*80)

# å„åˆ—ã®æ ¼ç´ç‡è¨ˆç®—
completeness = {}
for col in df.columns:
    non_null_count = df[col].notna().sum()
    completeness[col] = non_null_count / len(df)

completeness_df = pd.DataFrame({
    'åˆ—å': list(completeness.keys()),
    'æ ¼ç´ç‡': list(completeness.values()),
    'ãƒ‡ãƒ¼ã‚¿å‹': [str(df[col].dtype) for col in completeness.keys()],
    'éNULLæ•°': [df[col].notna().sum() for col in completeness.keys()],
    'ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°': [df[col].nunique() for col in completeness.keys()]
}).sort_values('æ ¼ç´ç‡', ascending=False)

# æ ¼ç´ç‡80%ä»¥ä¸Šã®ã‚«ãƒ©ãƒ æŠ½å‡º
high_quality_cols = completeness_df[completeness_df['æ ¼ç´ç‡'] >= 0.8]['åˆ—å'].tolist()

print(f"\nâœ… æ ¼ç´ç‡80%ä»¥ä¸Šã®ã‚«ãƒ©ãƒ : {len(high_quality_cols)}å€‹")
print(f"   (å…¨{len(df.columns)}åˆ—ä¸­ {len(high_quality_cols)/len(df.columns):.1%})")

# ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡
exclude_cols = ['åº—èˆ—', 'å•†å“å', 'æ—¥ä»˜', sales_col, 'date', 'store_id', 'sku_id',
               'category_l', 'category_m', 'category_s',
               'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šå¤§åˆ†é¡', 'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šä¸­åˆ†é¡', 'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šå°åˆ†é¡']

analysis_cols = [col for col in high_quality_cols if col not in exclude_cols]

print(f"   åˆ†æå¯¾è±¡ã‚«ãƒ©ãƒ : {len(analysis_cols)}å€‹")

# ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
feature_categories = {
    'æ™‚ç³»åˆ—': [],
    'å¤©æ°—': [],
    'æ°—æ¸©': [],
    'ãƒ•ãƒ©ã‚°': [],
    'ã‚¤ãƒ™ãƒ³ãƒˆ': [],
    'çµ±è¨ˆé‡': [],
    'ãã®ä»–': []
}

for col in analysis_cols:
    col_lower = col.lower()
    if any(x in col_lower for x in ['_t-', '_ma', 'lag', 'shift', 'ãƒˆãƒ¬ãƒ³ãƒ‰', 'å¤‰åŒ–ç‡', 'ç´¯ç©']):
        feature_categories['æ™‚ç³»åˆ—'].append(col)
    elif any(x in col for x in ['å¤©æ°—', 'å¤©å€™', 'weather']):
        feature_categories['å¤©æ°—'].append(col)
    elif any(x in col for x in ['æ°—æ¸©', 'æ¸©åº¦', 'temp', 'æ°—åœ§', 'æ¹¿åº¦', 'humidity']):
        feature_categories['æ°—æ¸©'].append(col)
    elif 'ãƒ•ãƒ©ã‚°' in col or col.endswith('_flag') or any(x in col for x in ['æ—¥æ›œ', 'æœˆæ›œ', 'åœŸæ›œ', 'é€±æœ«', 'å¹³æ—¥', 'ç¥æ—¥', 'ä¼‘æ—¥']):
        feature_categories['ãƒ•ãƒ©ã‚°'].append(col)
    elif any(x in col for x in ['çµ¦æ–™æ—¥', 'ã‚¤ãƒ™ãƒ³ãƒˆ', 'event', 'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³', 'æœˆåˆ', 'æœˆæœ«']):
        feature_categories['ã‚¤ãƒ™ãƒ³ãƒˆ'].append(col)
    elif any(x in col for x in ['å¹³å‡', 'æœ€å¤§', 'æœ€å°', 'åˆè¨ˆ', 'mean', 'max', 'min', 'sum', 'std']):
        feature_categories['çµ±è¨ˆé‡'].append(col)
    else:
        feature_categories['ãã®ä»–'].append(col)

print("\nğŸ“‹ ç‰¹å¾´é‡ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ:")
for category, cols in feature_categories.items():
    if cols:
        print(f"  {category}: {len(cols)}å€‹")

# ========================================
# 2. å£²ä¸Šã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆåˆ†æ
# ========================================

print("\n" + "="*80)
print("ğŸ’° ã‚¹ãƒ†ãƒƒãƒ—2: å£²ä¸Šã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆåˆ†æ")
print("="*80)

impact_results = []

for col in analysis_cols:
    try:
        # ãƒ‡ãƒ¼ã‚¿å‹ç¢ºèª
        col_data = df[col].dropna()

        if len(col_data) < 100:
            continue

        # æ•°å€¤å‹ã®å ´åˆ
        if pd.api.types.is_numeric_dtype(col_data):
            # ãƒã‚¤ãƒŠãƒªãƒ•ãƒ©ã‚°ï¼ˆ0/1ï¼‰ã®å ´åˆ
            unique_vals = col_data.unique()
            if len(unique_vals) == 2 and set(unique_vals).issubset({0, 1, 0.0, 1.0}):
                # ãƒ•ãƒ©ã‚°ON/OFFã§ã®å£²ä¸Šæ¯”è¼ƒ
                flag_on = df[df[col] == 1][sales_col].mean()
                flag_off = df[df[col] == 0][sales_col].mean()

                if flag_off > 0:
                    impact = (flag_on - flag_off) / flag_off
                    impact_abs = flag_on - flag_off

                    impact_results.append({
                        'ç‰¹å¾´é‡': col,
                        'ã‚«ãƒ†ã‚´ãƒª': next((k for k, v in feature_categories.items() if col in v), 'ãã®ä»–'),
                        'åˆ†æã‚¿ã‚¤ãƒ—': 'ãƒã‚¤ãƒŠãƒªãƒ•ãƒ©ã‚°',
                        'å£²ä¸Š_ON': flag_on,
                        'å£²ä¸Š_OFF': flag_off,
                        'ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡': impact,
                        'ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆçµ¶å¯¾å€¤': impact_abs,
                        'ã‚µãƒ³ãƒ—ãƒ«æ•°_ON': (df[col] == 1).sum(),
                        'ã‚µãƒ³ãƒ—ãƒ«æ•°_OFF': (df[col] == 0).sum(),
                        'æ ¼ç´ç‡': completeness[col]
                    })

            # é€£ç¶šå€¤ã®å ´åˆï¼ˆç›¸é–¢åˆ†æï¼‰
            elif len(unique_vals) > 10:
                correlation = df[[col, sales_col]].corr().iloc[0, 1]

                if not np.isnan(correlation):
                    # ä¸Šä½25%ã¨ä¸‹ä½25%ã®å£²ä¸Šæ¯”è¼ƒ
                    q75 = col_data.quantile(0.75)
                    q25 = col_data.quantile(0.25)

                    sales_high = df[df[col] >= q75][sales_col].mean()
                    sales_low = df[df[col] <= q25][sales_col].mean()

                    if sales_low > 0:
                        impact = (sales_high - sales_low) / sales_low
                        impact_abs = sales_high - sales_low

                        impact_results.append({
                            'ç‰¹å¾´é‡': col,
                            'ã‚«ãƒ†ã‚´ãƒª': next((k for k, v in feature_categories.items() if col in v), 'ãã®ä»–'),
                            'åˆ†æã‚¿ã‚¤ãƒ—': 'é€£ç¶šå€¤ï¼ˆQ75 vs Q25ï¼‰',
                            'å£²ä¸Š_é«˜': sales_high,
                            'å£²ä¸Š_ä½': sales_low,
                            'ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡': impact,
                            'ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆçµ¶å¯¾å€¤': impact_abs,
                            'ç›¸é–¢ä¿‚æ•°': correlation,
                            'ã‚µãƒ³ãƒ—ãƒ«æ•°_é«˜': (df[col] >= q75).sum(),
                            'ã‚µãƒ³ãƒ—ãƒ«æ•°_ä½': (df[col] <= q25).sum(),
                            'æ ¼ç´ç‡': completeness[col]
                        })

            # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«æ•°å€¤ï¼ˆ3-10ç¨®é¡ï¼‰ã®å ´åˆ
            elif 3 <= len(unique_vals) <= 10:
                # å„ã‚«ãƒ†ã‚´ãƒªã®å£²ä¸Šå¹³å‡
                category_sales = df.groupby(col)[sales_col].agg(['mean', 'count'])

                if len(category_sales) >= 2:
                    max_sales = category_sales['mean'].max()
                    min_sales = category_sales['mean'].min()

                    if min_sales > 0:
                        impact = (max_sales - min_sales) / min_sales
                        impact_abs = max_sales - min_sales

                        impact_results.append({
                            'ç‰¹å¾´é‡': col,
                            'ã‚«ãƒ†ã‚´ãƒª': next((k for k, v in feature_categories.items() if col in v), 'ãã®ä»–'),
                            'åˆ†æã‚¿ã‚¤ãƒ—': 'ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«',
                            'å£²ä¸Š_æœ€å¤§': max_sales,
                            'å£²ä¸Š_æœ€å°': min_sales,
                            'ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡': impact,
                            'ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆçµ¶å¯¾å€¤': impact_abs,
                            'ã‚«ãƒ†ã‚´ãƒªæ•°': len(unique_vals),
                            'æ ¼ç´ç‡': completeness[col]
                        })

        # æ–‡å­—åˆ—å‹ã®å ´åˆ
        elif pd.api.types.is_string_dtype(col_data) or pd.api.types.is_object_dtype(col_data):
            unique_vals = col_data.unique()

            if 2 <= len(unique_vals) <= 20:
                category_sales = df.groupby(col)[sales_col].agg(['mean', 'count'])
                category_sales = category_sales[category_sales['count'] >= 10]  # æœ€ä½10ã‚µãƒ³ãƒ—ãƒ«

                if len(category_sales) >= 2:
                    max_sales = category_sales['mean'].max()
                    min_sales = category_sales['mean'].min()

                    if min_sales > 0:
                        impact = (max_sales - min_sales) / min_sales
                        impact_abs = max_sales - min_sales

                        impact_results.append({
                            'ç‰¹å¾´é‡': col,
                            'ã‚«ãƒ†ã‚´ãƒª': next((k for k, v in feature_categories.items() if col in v), 'ãã®ä»–'),
                            'åˆ†æã‚¿ã‚¤ãƒ—': 'ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ†ã‚´ãƒª',
                            'å£²ä¸Š_æœ€å¤§': max_sales,
                            'å£²ä¸Š_æœ€å°': min_sales,
                            'ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡': impact,
                            'ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆçµ¶å¯¾å€¤': impact_abs,
                            'ã‚«ãƒ†ã‚´ãƒªæ•°': len(category_sales),
                            'æ ¼ç´ç‡': completeness[col]
                        })

    except Exception as e:
        print(f"âš ï¸ {col}: ã‚¨ãƒ©ãƒ¼ ({str(e)[:50]})")
        continue

print(f"\nâœ… åˆ†æå®Œäº†: {len(impact_results)}å€‹ã®ç‰¹å¾´é‡ã‚’åˆ†æ")

# ========================================
# 3. çµæœé›†è¨ˆã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°
# ========================================

if len(impact_results) > 0:
    impact_df = pd.DataFrame(impact_results)

    # ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡ã®çµ¶å¯¾å€¤ã§ã‚½ãƒ¼ãƒˆ
    impact_df['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡_çµ¶å¯¾å€¤'] = impact_df['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡'].abs()
    impact_df = impact_df.sort_values('ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡_çµ¶å¯¾å€¤', ascending=False)

    print("\n" + "="*80)
    print("ğŸ† å£²ä¸Šã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆãƒ©ãƒ³ã‚­ãƒ³ã‚° Top 30")
    print("="*80)

    # è¡¨ç¤ºç”¨ã«æ•´å½¢
    display_df = impact_df.head(30).copy()
    display_df['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡_è¡¨ç¤º'] = display_df['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡'].apply(lambda x: f"{x:+.2%}")
    display_df['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆçµ¶å¯¾å€¤_è¡¨ç¤º'] = display_df['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆçµ¶å¯¾å€¤'].apply(lambda x: f"{x:+,.0f}å††")

    print(display_df[['ç‰¹å¾´é‡', 'ã‚«ãƒ†ã‚´ãƒª', 'åˆ†æã‚¿ã‚¤ãƒ—', 'ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡_è¡¨ç¤º',
                      'ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆçµ¶å¯¾å€¤_è¡¨ç¤º', 'æ ¼ç´ç‡']].to_string(index=False))

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªãƒ¼
    print("\n" + "="*80)
    print("ğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚µãƒãƒªãƒ¼")
    print("="*80)

    category_summary = impact_df.groupby('ã‚«ãƒ†ã‚´ãƒª').agg({
        'ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡_çµ¶å¯¾å€¤': ['mean', 'max', 'count']
    }).round(4)
    category_summary.columns = ['å¹³å‡ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡', 'æœ€å¤§ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡', 'ç‰¹å¾´é‡æ•°']
    category_summary = category_summary.sort_values('æœ€å¤§ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡', ascending=False)

    print(category_summary)

    # CSVä¿å­˜
    output_path = Path('output/comprehensive_sales_impact_analysis.csv')
    impact_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"\nâœ… è©³ç´°çµæœã‚’ä¿å­˜: {output_path}")

    # Top 50ã ã‘åˆ¥é€”ä¿å­˜
    top50_path = Path('output/sales_impact_top50.csv')
    impact_df.head(50).to_csv(top50_path, index=False, encoding='utf-8-sig')
    print(f"âœ… Top 50ã‚’ä¿å­˜: {top50_path}")

    # ========================================
    # 4. æ´å¯Ÿã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ 
    # ========================================

    print("\n" + "="*80)
    print("ğŸ’¡ ä¸»è¦ãªæ´å¯Ÿ")
    print("="*80)

    # æ­£ã®ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆTop 5
    positive_impact = impact_df[impact_df['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡'] > 0].head(5)
    print("\nâœ… å£²ä¸Šå¢—åŠ è¦å›  Top 5:")
    for idx, row in positive_impact.iterrows():
        print(f"  {row['ç‰¹å¾´é‡']}: {row['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡']:+.2%} ({row['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆçµ¶å¯¾å€¤']:+,.0f}å††)")

    # è² ã®ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆTop 5
    negative_impact = impact_df[impact_df['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡'] < 0].sort_values('ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡').head(5)
    print("\nâš ï¸ å£²ä¸Šæ¸›å°‘è¦å›  Top 5:")
    for idx, row in negative_impact.iterrows():
        print(f"  {row['ç‰¹å¾´é‡']}: {row['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡']:+.2%} ({row['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆçµ¶å¯¾å€¤']:+,.0f}å††)")

    print("\n" + "="*80)
    print("ğŸ¯ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³")
    print("="*80)

    print("""
ã€å£²ä¸Šæœ€å¤§åŒ–æ–½ç­–ã€‘
1. æ­£ã®ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆè¦å› ã‚’æœ€å¤§åŒ–
   - è©²å½“æ—¥ãƒ»æ¡ä»¶ã§ã®åœ¨åº«ç¢ºä¿
   - ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³å¼·åŒ–
   - ä¾¡æ ¼æœ€é©åŒ–

2. è² ã®ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆè¦å› ã‚’ç·©å’Œ
   - å£²ä¸Šæ¸›å°‘æ—¥ã®ç‰¹åˆ¥æ–½ç­–
   - ä»£æ›¿å•†å“ã®ææ¡ˆ
   - å‰²å¼•ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³

3. äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã¸ã®æ´»ç”¨
   - ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡ä¸Šä½ã®ç‰¹å¾´é‡ã‚’é‡ç‚¹çš„ã«ä½¿ç”¨
   - ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã§ç›¸äº’ä½œç”¨é …ã‚’ä½œæˆ
   - ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«é‡è¦ãªç‰¹å¾´é‡ã‚’é¸å®š
    """)

else:
    print("\nâš ï¸ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆåˆ†æçµæœãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

print("\n" + "="*80)
print("âœ… åŒ…æ‹¬çš„å£²ä¸Šã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆåˆ†æå®Œäº†")
print("="*80)
