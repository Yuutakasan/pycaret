#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Phase 1-4ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ä¸€æ‹¬ä¿®æ­£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã¨åº—èˆ—é¸æŠã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã‚’å…¨Phaseãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã«è¿½åŠ ã—ã¾ã™ã€‚
"""

import json
import shutil
from pathlib import Path
from datetime import datetime

# ä¿®æ­£å¯¾è±¡ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯
NOTEBOOKS = [
    'åº—èˆ—åˆ¥åŒ…æ‹¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰_v5.0_Phase1.ipynb',
    'åº—èˆ—åˆ¥åŒ…æ‹¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰_v5.0_Phase2.ipynb',
    'åº—èˆ—åˆ¥åŒ…æ‹¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰_v5.0_Phase3.ipynb',
    'åº—èˆ—åˆ¥åŒ…æ‹¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰_v5.0_Phase4.ipynb'
]

# æ–°ã—ã„ãƒ˜ãƒƒãƒ€ãƒ¼ã‚»ãƒ«ï¼ˆæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆå¯¾å¿œï¼‰
NEW_HEADER_CELL = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# è­¦å‘ŠæŠ‘åˆ¶\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "\n",
        "# å¯è¦–åŒ–\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆ\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "    WIDGETS_AVAILABLE = True\n",
        "    print(\"âœ… ipywidgetsåˆ©ç”¨å¯èƒ½\")\n",
        "except ImportError:\n",
        "    WIDGETS_AVAILABLE = False\n",
        "    print(\"âš ï¸ ipywidgetsæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - ä¸€éƒ¨æ©Ÿèƒ½åˆ¶é™\")\n",
        "\n",
        "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š\n",
        "try:\n",
        "    import japanize_matplotlib\n",
        "    japanize_matplotlib.japanize()\n",
        "    print(\"âœ… æ—¥æœ¬èªè¡¨ç¤º: japanize_matplotlib\")\n",
        "except ImportError:\n",
        "    # ä»£æ›¿ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š\n",
        "    import matplotlib.font_manager as fm\n",
        "    # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’æ¤œç´¢\n",
        "    japanese_fonts = [f.name for f in fm.fontManager.ttflist if 'Gothic' in f.name or 'Noto Sans CJK' in f.name or 'IPA' in f.name]\n",
        "    if japanese_fonts:\n",
        "        plt.rcParams['font.family'] = japanese_fonts[0]\n",
        "        print(f\"âœ… æ—¥æœ¬èªè¡¨ç¤º: {japanese_fonts[0]}\")\n",
        "    else:\n",
        "        plt.rcParams['font.family'] = ['Noto Sans CJK JP', 'IPAGothic', 'sans-serif']\n",
        "        print(\"âš ï¸ ä»£æ›¿ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆãƒ•ã‚©ãƒ³ãƒˆæ¤œå‡ºå¤±æ•—ï¼‰\")\n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# matplotlibå…±é€šè¨­å®š\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "plt.rcParams['figure.figsize'] = (18, 12)\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['savefig.dpi'] = 150\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "# seabornè¨­å®š\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "# pandasè¨­å®š\n",
        "pd.set_option('display.unicode.east_asian_width', True)\n",
        "pd.set_option('display.max_columns', 50)\n",
        "pd.set_option('display.max_rows', 200)\n",
        "pd.set_option('display.width', 120)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸª åº—èˆ—åˆ¥åŒ…æ‹¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ v5.0\".center(80))\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nâœ… ç’°å¢ƒè¨­å®šå®Œäº†\")\n",
        "print(f\"   å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\")\n",
        "print(f\"   pandas: {pd.__version__}\")\n",
        "print(f\"   matplotlib: {plt.matplotlib.__version__}\")"
    ]
}

# åº—èˆ—é¸æŠã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã‚»ãƒ«
STORE_SELECTION_CELL = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# ğŸ¯ åº—èˆ—é¸æŠã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆ\n",
        "\n",
        "# åº—èˆ—ä¸€è¦§\n",
        "stores = sorted(df_enriched['åº—èˆ—'].unique())\n",
        "DEFAULT_STORE = stores[0]\n",
        "\n",
        "print(f\"\\nğŸª åˆ©ç”¨å¯èƒ½ãªåº—èˆ— ({len(stores)}åº—èˆ—):\")\n",
        "for i, store in enumerate(stores, 1):\n",
        "    print(f\"   {i}. {store}\")\n",
        "\n",
        "# åº—èˆ—é¸æŠã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆ\n",
        "if WIDGETS_AVAILABLE:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ğŸ¯ ä»¥ä¸‹ã®ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã‹ã‚‰åˆ†æå¯¾è±¡åº—èˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    store_dropdown = widgets.Dropdown(\n",
        "        options=stores,\n",
        "        value=DEFAULT_STORE,\n",
        "        description='åˆ†æå¯¾è±¡åº—èˆ—:',\n",
        "        disabled=False,\n",
        "        style={'description_width': '120px'},\n",
        "        layout=widgets.Layout(width='500px')\n",
        "    )\n",
        "    \n",
        "    info_label = widgets.HTML(\n",
        "        value=\"<b>ğŸ’¡ ãƒ’ãƒ³ãƒˆ:</b> åº—èˆ—ã‚’å¤‰æ›´ã™ã‚‹ã¨ã€ä»¥é™ã®ã™ã¹ã¦ã®åˆ†æãŒé¸æŠã—ãŸåº—èˆ—ã§å†è¨ˆç®—ã•ã‚Œã¾ã™ã€‚\"\n",
        "    )\n",
        "    \n",
        "    display(widgets.VBox([store_dropdown, info_label]))\n",
        "    \n",
        "    # é¸æŠã•ã‚ŒãŸåº—èˆ—\n",
        "    MY_STORE = store_dropdown.value\n",
        "else:\n",
        "    # ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆãŒä½¿ãˆãªã„å ´åˆ\n",
        "    MY_STORE = DEFAULT_STORE\n",
        "    print(f\"\\nğŸ¯ åˆ†æå¯¾è±¡åº—èˆ—: {MY_STORE} (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)\")\n",
        "\n",
        "# åº—èˆ—ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
        "my_df = df_enriched[df_enriched['åº—èˆ—'] == MY_STORE].copy()\n",
        "\n",
        "print(f\"\\nâœ… é¸æŠã•ã‚ŒãŸåº—èˆ—: {MY_STORE}\")\n",
        "print(f\"   å¯¾è±¡ãƒ‡ãƒ¼ã‚¿: {len(my_df):,}è¡Œ\")"
    ]
}

# ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚»ãƒ«
DATA_VALIDATION_CELL = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ï¼ˆå­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼‰\n",
        "\n",
        "def validate_data_column(df, col_name, analysis_name=\"åˆ†æ\"):\n",
        "    \"\"\"ãƒ‡ãƒ¼ã‚¿ã‚«ãƒ©ãƒ ã®å­˜åœ¨ã¨æœ‰åŠ¹æ€§ã‚’ãƒã‚§ãƒƒã‚¯\"\"\"\n",
        "    if col_name not in df.columns:\n",
        "        print(f\"âš ï¸ {analysis_name}: '{col_name}' ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“\")\n",
        "        return False\n",
        "    \n",
        "    non_null_count = df[col_name].notna().sum()\n",
        "    coverage = non_null_count / len(df) * 100\n",
        "    \n",
        "    if coverage < 50:\n",
        "        print(f\"âš ï¸ {analysis_name}: '{col_name}' ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ãŒä½ã„ ({coverage:.1f}%)\")\n",
        "        return False\n",
        "    \n",
        "    return True\n",
        "\n",
        "print(\"\\nğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ä¸­...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# å¿…é ˆã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯\n",
        "required_cols = ['æ—¥ä»˜', 'å£²ä¸Šé‡‘é¡', 'åº—èˆ—']\n",
        "for col in required_cols:\n",
        "    if col in df_enriched.columns:\n",
        "        print(f\"âœ… å¿…é ˆã‚«ãƒ©ãƒ  '{col}' - å­˜åœ¨\")\n",
        "    else:\n",
        "        print(f\"âŒ å¿…é ˆã‚«ãƒ©ãƒ  '{col}' - ä¸è¶³\")\n",
        "\n",
        "# ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯\n",
        "optional_cols = {\n",
        "    'æ°—è±¡ãƒ‡ãƒ¼ã‚¿': ['æœ€é«˜æ°—æ¸©', 'é™æ°´é‡'],\n",
        "    'å‰å¹´ãƒ‡ãƒ¼ã‚¿': ['æ˜¨å¹´åŒæ—¥_å£²ä¸Š', 'æ˜¨å¹´åŒæ—¥_å®¢æ•°'],\n",
        "    'æ™‚é–“å¸¯ãƒ‡ãƒ¼ã‚¿': ['æ™‚åˆ»', 'æ™‚é–“']\n",
        "}\n",
        "\n",
        "for category, cols in optional_cols.items():\n",
        "    has_any = any(col in df_enriched.columns for col in cols)\n",
        "    if has_any:\n",
        "        available_cols = [col for col in cols if col in df_enriched.columns]\n",
        "        print(f\"âœ… {category}: {', '.join(available_cols)}\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ {category}: åˆ©ç”¨ä¸å¯ï¼ˆä»£æ›¿ãƒ­ã‚¸ãƒƒã‚¯ä½¿ç”¨ï¼‰\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"âœ… ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å®Œäº†\\n\")"
    ]
}


def backup_notebook(notebook_path):
    """ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = notebook_path.with_suffix(f'.backup_{timestamp}.ipynb')
    shutil.copy2(notebook_path, backup_path)
    print(f"   ğŸ“‹ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_path.name}")
    return backup_path


def fix_notebook(notebook_path):
    """ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä¿®æ­£"""
    print(f"\n{'='*80}")
    print(f"ğŸ”§ ä¿®æ­£ä¸­: {notebook_path.name}")
    print(f"{'='*80}")

    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    backup_path = backup_notebook(notebook_path)

    try:
        # ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯èª­ã¿è¾¼ã¿
        with open(notebook_path, 'r', encoding='utf-8') as f:
            nb = json.load(f)

        cells = nb['cells']
        print(f"   ğŸ“Š ã‚»ãƒ«æ•°: {len(cells)}")

        # 1ç•ªç›®ã®ã‚»ãƒ«ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ï¼‰ã¯ç¶­æŒ
        # 2ç•ªç›®ã®ã‚»ãƒ«ï¼ˆimportã‚»ãƒ«ï¼‰ã‚’ç½®ãæ›ãˆ
        if len(cells) > 1 and cells[1].get('cell_type') == 'code':
            print(f"   ğŸ”„ ãƒ˜ãƒƒãƒ€ãƒ¼ã‚»ãƒ«ã‚’ç½®æ›ï¼ˆã‚»ãƒ«1ï¼‰")
            cells[1] = NEW_HEADER_CELL

        # 3ç•ªç›®ã®ã‚»ãƒ«ï¼ˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼‰ã®å¾Œã«åº—èˆ—é¸æŠã‚»ãƒ«ã‚’æŒ¿å…¥
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚»ãƒ«ã‚’æ¢ã™
        data_load_index = None
        for i, cell in enumerate(cells):
            if cell.get('cell_type') == 'code':
                source = ''.join(cell.get('source', []))
                if 'df_enriched' in source and 'read_csv' in source:
                    data_load_index = i
                    break

        if data_load_index is not None:
            # æ—¢å­˜ã®åº—èˆ—é¸æŠã‚»ãƒ«ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            has_store_selection = False
            if data_load_index + 1 < len(cells):
                next_cell_source = ''.join(cells[data_load_index + 1].get('source', []))
                if 'åº—èˆ—é¸æŠ' in next_cell_source or 'store_dropdown' in next_cell_source:
                    has_store_selection = True

            if not has_store_selection:
                print(f"   â• åº—èˆ—é¸æŠã‚»ãƒ«ã‚’æŒ¿å…¥ï¼ˆã‚»ãƒ«{data_load_index + 1}ã®å¾Œï¼‰")
                cells.insert(data_load_index + 1, STORE_SELECTION_CELL.copy())
                cells.insert(data_load_index + 2, DATA_VALIDATION_CELL.copy())
            else:
                print(f"   âœ… åº—èˆ—é¸æŠã‚»ãƒ«ã¯æ—¢ã«å­˜åœ¨ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")

        # ã™ã¹ã¦ã®ã‚³ãƒ¼ãƒ‰ã‚»ãƒ«ã§ MY_STORE = DEFAULT_STORE ã‚’ç¢ºèª
        my_store_count = 0
        for i, cell in enumerate(cells):
            if cell.get('cell_type') == 'code':
                source = cell.get('source', [])
                if isinstance(source, list):
                    source_str = ''.join(source)
                    if 'MY_STORE = DEFAULT_STORE' in source_str:
                        # ã“ã®è¡Œã‚’å‰Šé™¤ï¼ˆåº—èˆ—é¸æŠã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã«ä»»ã›ã‚‹ï¼‰
                        new_source = [line for line in source if 'MY_STORE = DEFAULT_STORE' not in line]
                        if len(new_source) < len(source):
                            cell['source'] = new_source
                            my_store_count += 1

        if my_store_count > 0:
            print(f"   ğŸ—‘ï¸  MY_STORE=DEFAULT_STORE ã‚’{my_store_count}ç®‡æ‰€å‰Šé™¤")

        # ä¿®æ­£ã—ãŸãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä¿å­˜
        with open(notebook_path, 'w', encoding='utf-8') as f:
            json.dump(nb, f, ensure_ascii=False, indent=1)

        print(f"   âœ… ä¿®æ­£å®Œäº†: {notebook_path.name}")
        print(f"   ğŸ“Š æœ€çµ‚ã‚»ãƒ«æ•°: {len(cells)}")

        return True

    except Exception as e:
        print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒ
        shutil.copy2(backup_path, notebook_path)
        print(f"   â†©ï¸  ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒ")
        return False


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("\n" + "="*80)
    print("ğŸ”§ Phase 1-4 ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ä¸€æ‹¬ä¿®æ­£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("="*80)
    timestamp = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')
    print(f"å®Ÿè¡Œæ—¥æ™‚: {timestamp}")

    work_dir = Path('/mnt/d/github/pycaret/work')

    print(f"\nğŸ“‚ ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {work_dir}")

    success_count = 0
    fail_count = 0

    for notebook_name in NOTEBOOKS:
        notebook_path = work_dir / notebook_name

        if not notebook_path.exists():
            print(f"\nâš ï¸ ã‚¹ã‚­ãƒƒãƒ—: {notebook_name} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            fail_count += 1
            continue

        if fix_notebook(notebook_path):
            success_count += 1
        else:
            fail_count += 1

    # ã‚µãƒãƒªãƒ¼
    print("\n" + "="*80)
    print("ğŸ“‹ ä¿®æ­£ã‚µãƒãƒªãƒ¼")
    print("="*80)
    print(f"âœ… æˆåŠŸ: {success_count}å€‹")
    print(f"âŒ å¤±æ•—: {fail_count}å€‹")

    if success_count > 0:
        print("\nğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("   1. Jupyter Notebookã§å„ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’é–‹ã")
        print("   2. 'Restart & Run All' ã§å…¨ã‚»ãƒ«å®Ÿè¡Œ")
        print("   3. æ—¥æœ¬èªè¡¨ç¤ºã¨åº—èˆ—é¸æŠãŒæ­£å¸¸å‹•ä½œã™ã‚‹ã‹ç¢ºèª")
        print("   4. ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã° .backup_*.ipynb ã‹ã‚‰å¾©å…ƒ")

    print("\n" + "="*80)


if __name__ == '__main__':
    main()
