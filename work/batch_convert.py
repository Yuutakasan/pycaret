#!/usr/bin/env python3
"""
Excel to CSV batch conversion utility for the work pipeline.

This refactored version keeps the original behaviour while improving
structure, logging, and error handling. Key features:
  * Format detection with optional structure analysis in debug mode
  * Automatic wide â†’ long conversion for POS workbooks
  * Graceful fallbacks to simple and multi-sheet conversions
  * Optional multiprocessing
  * Post-processing hooks reused by the debug pipeline
"""

from __future__ import annotations

import argparse
import logging
import re
import subprocess
import sys
from concurrent.futures import ProcessPoolExecutor, as_completed
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, List, Optional, Tuple

import pandas as pd

from work_utils import ConversionResult, configure_logging, summarize_results


ID_KEYWORDS = ("åº—èˆ—", "ãƒ•ã‚§ã‚¤ã‚¹", "å•†å“", "åˆ†é¡")
DATE_TOKEN_PATTERN = re.compile(r"(\d{4}/\d{1,2}/\d{1,2})")
DISPLAY_BREAK = "=" * 60


@dataclass(frozen=True)
class ConversionTask:
    """Serializable payload for processing a single Excel workbook."""

    input_file: Path
    output_dir: Path
    use_wide_to_long: bool
    analyze: bool
    debug: bool


class ExcelConverter:
    """Handle conversion of one Excel workbook into CSV."""

    def __init__(
        self,
        input_file: Path,
        output_dir: Path,
        *,
        use_wide_to_long: bool,
        analyze: bool,
        debug: bool,
        logger: Optional[logging.Logger] = None,
    ) -> None:
        self.input_file = input_file
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.output_file = output_dir / f"{input_file.stem}.csv"
        self.use_wide_to_long = use_wide_to_long
        self.analyze = analyze
        self.debug = debug
        self.logger = logger or configure_logging(
            debug, name=f"work.batch_convert.{input_file.stem}"
        )

    # ------------------------------------------------------------------ #
    # Public API
    # ------------------------------------------------------------------ #
    def convert(self) -> ConversionResult:
        """Convert the workbook to CSV and return the outcome."""
        if self.input_file.name.startswith("~$"):
            return ConversionResult.skipped_file(
                self.input_file, "Excelä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰"
            )

        file_format = self._detect_file_format()

        if self.use_wide_to_long and file_format == "pos":
            result = self._convert_with_wide_to_long()
            if result.ok:
                return result
            self.logger.info("ã‚·ãƒ³ãƒ—ãƒ«å¤‰æ›ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: %s", self.input_file.name)
            return self._convert_simple_fallback()

        return self._convert_smart()

    # ------------------------------------------------------------------ #
    # Conversion helpers
    # ------------------------------------------------------------------ #
    def _convert_smart(self) -> ConversionResult:
        """Attempt conversion with header inference and wide â†’ long reshape."""
        try:
            self.logger.info("å‡¦ç†é–‹å§‹: %s", self.input_file.name)

            if self.analyze:
                self._analyze_structure()

            df = pd.read_excel(self.input_file, header=None)
            header_idx = self._find_header_row(df)

            if header_idx == -1:
                self.logger.warning(
                    "ãƒ˜ãƒƒãƒ€ãƒ¼ãŒæ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã¾ã™"
                )
                return self._convert_simple_fallback()

            date_idx = self._find_date_row(df, header_idx)
            header_row = df.iloc[header_idx]
            id_columns = self._infer_id_columns(header_row)

            if not id_columns:
                self.logger.warning(
                    "IDåˆ—ãŒæ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã¾ã™"
                )
                return self._convert_simple_fallback()

            result_df = self._convert_wide_to_long(df, header_idx, date_idx, id_columns)

            if result_df.empty:
                self.logger.warning("ãƒ­ãƒ³ã‚°å½¢å¼ã¸ã®å¤‰æ›çµæœãŒç©ºã§ã—ãŸã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™")
                return self._convert_simple()

            result_df.to_csv(self.output_file, index=False, encoding="utf-8-sig")
            self.logger.info(
                "âœ“ å®Œäº†: %s (%sè¡Œ)",
                self.output_file.name,
                f"{len(result_df):,}",
            )
            return ConversionResult.success(
                self.input_file, self.output_file, len(result_df)
            )

        except Exception as exc:
            self._log_exception(f"ã‚¹ãƒãƒ¼ãƒˆå¤‰æ›ã‚¨ãƒ©ãƒ¼ [{self.input_file.name}]", exc)

        return self._convert_simple_fallback()

    def _convert_simple(self) -> ConversionResult:
        """Fallback that preserves the first sheet with inferred headers."""
        try:
            df = pd.read_excel(self.input_file)
            df.to_csv(self.output_file, index=False, encoding="utf-8-sig")
            self.logger.info(
                "âœ“ å®Œäº†ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ï¼‰: %s (%sè¡Œ)",
                self.output_file.name,
                f"{len(df):,}",
            )
            return ConversionResult.success(
                self.input_file, self.output_file, len(df)
            )
        except Exception as exc:
            self._log_exception(
                f"ã‚·ãƒ³ãƒ—ãƒ«å¤‰æ›ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ [{self.input_file.name}]", exc
            )
            return self._convert_simple_fallback()

    def _convert_simple_fallback(self) -> ConversionResult:
        """
        Final fallback that concatenates every non-empty sheet with the sheet name.
        """
        try:
            xlsx = pd.ExcelFile(self.input_file)
            frames: List[pd.DataFrame] = []

            for sheet_name in xlsx.sheet_names:
                frame = xlsx.parse(sheet_name=sheet_name)
                if frame.empty:
                    continue
                frame.insert(0, "ã‚·ãƒ¼ãƒˆå", sheet_name)
                frames.append(frame)

            if not frames:
                return ConversionResult.failure(
                    self.input_file, "æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
                )

            combined = pd.concat(frames, ignore_index=True)
            combined.to_csv(self.output_file, index=False, encoding="utf-8-sig")
            self.logger.info(
                "âœ“ å®Œäº†ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰: %s (%sè¡Œ)",
                self.output_file.name,
                f"{len(combined):,}",
            )
            return ConversionResult.success(
                self.input_file, self.output_file, len(combined), sheets=len(frames)
            )

        except Exception as exc:
            self._log_exception(
                f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¤‰æ›ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ [{self.input_file.name}]",
                exc,
            )
            return ConversionResult.failure(
                self.input_file, f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¤‰æ›ã‚¨ãƒ©ãƒ¼: {exc}"
            )

    def _convert_with_wide_to_long(self) -> ConversionResult:
        """Invoke wide_to_long.py for POS data."""
        script = Path(__file__).resolve().parent / "wide_to_long.py"
        if not script.exists():
            self.logger.error("wide_to_long.pyãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return ConversionResult.failure(
                self.input_file, "wide_to_long.pyãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            )

        command = [
            sys.executable,
            str(script),
            str(self.input_file),
            str(self.output_file),
            "--skip-weather",
            "--no-gpu",
        ]

        try:
            result = subprocess.run(
                command,
                capture_output=True,
                text=True,
                cwd=script.parent,
                check=False,
            )

            if result.returncode != 0:
                stderr = result.stderr.strip() or "unknown error"
                self.logger.error("wide_to_long.pyå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: %s", stderr)
                return ConversionResult.failure(
                    self.input_file,
                    f"wide_to_long.pyå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ (exit {result.returncode})",
                )

            df = pd.read_csv(self.output_file)
            self.logger.info(
                "âœ“ å®Œäº†ï¼ˆwide_to_longï¼‰: %s (%sè¡Œ)",
                self.output_file.name,
                f"{len(df):,}",
            )
            return ConversionResult.success(
                self.input_file, self.output_file, len(df)
            )

        except Exception as exc:
            self._log_exception("wide_to_long.pyã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼", exc)
            return ConversionResult.failure(self.input_file, str(exc))

    # ------------------------------------------------------------------ #
    # Detection utilities
    # ------------------------------------------------------------------ #
    def _detect_file_format(self) -> str:
        """Attempt to classify the workbook format."""
        try:
            filename = self.input_file.name
            if "å£²ä¸Šæƒ…å ±" in filename:
                return "sales"
            if "POSæƒ…å ±" in filename:
                return "pos"

            df = pd.read_excel(self.input_file, header=None, nrows=30)
            for idx in range(min(30, len(df))):
                row = df.iloc[idx]
                row_preview = " ".join(str(cell) for cell in row.tolist()[:10])
                if "åº—èˆ—" in row_preview and "ãƒ•ã‚§ã‚¤ã‚¹" in row_preview:
                    return "pos"
                if "åº—èˆ—" in row_preview and "å£²ä¸Š" in row_preview:
                    return "sales"
        except Exception as exc:
            self._log_exception("ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¤œå‡ºã«å¤±æ•—ã—ã¾ã—ãŸ", exc)
        return "unknown"

    def _analyze_structure(self) -> None:
        """Emit detailed insights about the workbook layout (debug helper)."""
        try:
            df = pd.read_excel(self.input_file, header=None, nrows=50)

            self.logger.info("ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ åˆ†æ: %s", self.input_file.name)
            self.logger.info("  ç·è¡Œæ•°: %dè¡Œï¼ˆå…ˆé ­50è¡Œã®ã¿è¡¨ç¤ºï¼‰", len(df))
            self.logger.info("  ç·åˆ—æ•°: %dåˆ—", len(df.columns))
            self.logger.info("")

            self.logger.info("ã€å…ˆé ­20è¡Œã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå·¦15åˆ—ï¼‰ã€‘")
            for idx in range(min(20, len(df))):
                row_preview = df.iloc[idx, :15].tolist()
                truncated = [
                    str(value)[:20] if pd.notna(value) else "NaN" for value in row_preview
                ]
                self.logger.info("  è¡Œ%2d: %s", idx, truncated)
            self.logger.info("")

            self.logger.info("ã€è¡¨é ­ï¼ˆæ—¥ä»˜ï¼‰ã®æ¤œå‡ºã€‘")
            for idx in range(min(30, len(df))):
                row = df.iloc[idx]
                matches: List[Tuple[int, str]] = []
                for col_idx, value in enumerate(row):
                    if pd.notna(value):
                        match = DATE_TOKEN_PATTERN.search(str(value))
                        if match:
                            matches.append((col_idx, match.group(1)))
                if matches:
                    self.logger.info("  è¡Œ%d: %då€‹ã®æ—¥ä»˜ã‚’æ¤œå‡º", idx, len(matches))
                    for col_idx, token in matches[:5]:
                        self.logger.info("    åˆ—%d: %s", col_idx, token)
                    if len(matches) > 5:
                        self.logger.info("    ... ä»–%då€‹", len(matches) - 5)
                    break
            else:
                self.logger.warning("  æ—¥ä»˜ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            self.logger.info("")

            self.logger.info("ã€è¡¨å´ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼è¡Œï¼‰ã®æ¤œå‡ºã€‘")
            for idx in range(min(30, len(df))):
                row = df.iloc[idx, :10].tolist()
                row_str = " ".join([str(value) for value in row if pd.notna(value)])
                if any(keyword in row_str for keyword in ("åº—èˆ—", "å•†å“", "ãƒ•ã‚§ã‚¤ã‚¹", "åˆ†é¡")):
                    display = [
                        str(value)[:15] if pd.notna(value) else "NaN" for value in row[:10]
                    ]
                    self.logger.info("  è¡Œ%d: ãƒ˜ãƒƒãƒ€ãƒ¼å€™è£œã‚’æ¤œå‡º %s", idx, display)
            self.logger.info("")

            self.logger.info("ã€è¡¨å´ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«ï¼ˆè¡Œ8-18ã®å·¦5åˆ—ï¼‰ã€‘")
            for idx in range(8, min(18, len(df))):
                sample = df.iloc[idx, :5].tolist()
                truncated = [
                    str(value)[:20] if pd.notna(value) else "NaN" for value in sample
                ]
                self.logger.info("  è¡Œ%d: %s", idx, truncated)
            self.logger.info("")

            self.logger.info("ã€å·¦ç«¯åˆ—ï¼ˆç¬¬0åˆ—ï¼‰ã®ã‚µãƒ³ãƒ—ãƒ«ï¼ˆè¡Œ10-20ï¼‰ã€‘")
            for idx in range(10, min(20, len(df))):
                value = df.iloc[idx, 0]
                if pd.notna(value):
                    self.logger.info("  è¡Œ%d: %s", idx, str(value)[:50])
            self.logger.info("")

        except Exception as exc:
            self._log_exception("æ§‹é€ åˆ†æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ", exc)

    @staticmethod
    def _find_header_row(df: pd.DataFrame) -> int:
        """Locate the row that contains the header columns."""
        for idx in range(min(30, len(df))):
            row = df.iloc[idx, :10].tolist()
            row_str = " ".join([str(value) for value in row if pd.notna(value)])
            if "åº—èˆ—" in row_str and any(
                marker in row_str for marker in ("å•†å“", "ãƒ•ã‚§ã‚¤ã‚¹", "å£²ä¸Š")
            ):
                return idx
        return -1

    @staticmethod
    def _find_date_row(df: pd.DataFrame, header_idx: int) -> int:
        """Identify the row that contains the date headers above the metrics."""
        for offset in range(1, 4):
            candidate = header_idx - offset
            if candidate < 0:
                continue
            row = df.iloc[candidate]
            hits = 0
            for value in row:
                if pd.notna(value) and DATE_TOKEN_PATTERN.search(str(value)):
                    hits += 1
            if hits:
                return candidate
        return -1

    @staticmethod
    def _infer_id_columns(header_row: pd.Series) -> List[str]:
        """Collect ID column names before the first metric column."""
        id_columns: List[str] = []
        for value in header_row:
            if pd.isna(value):
                continue
            header_str = str(value)
            if any(keyword in header_str for keyword in ID_KEYWORDS):
                id_columns.append(header_str)
                continue
            if id_columns:
                break
        return id_columns

    def _convert_wide_to_long(
        self,
        df: pd.DataFrame,
        header_idx: int,
        date_idx: int,
        id_columns: List[str],
    ) -> pd.DataFrame:
        """Transform the wide table into a long table with metrics per date."""
        header_row = df.iloc[header_idx]
        date_row = df.iloc[date_idx] if date_idx >= 0 else None
        data = df.iloc[header_idx + 1 :].copy()

        num_id_cols = len(id_columns)

        column_map: dict[int, Tuple[str, str]] = {}
        current_date: Optional[str] = None

        for col_idx in range(num_id_cols, len(header_row)):
            if date_row is not None and col_idx < len(date_row):
                cell_value = date_row.iloc[col_idx]
                if pd.notna(cell_value):
                    match = DATE_TOKEN_PATTERN.search(str(cell_value))
                    if match:
                        current_date = match.group(1)
                    else:
                        try:
                            current_date = pd.to_datetime(cell_value).strftime(
                                "%Y/%m/%d"
                            )
                        except Exception:
                            pass

            metric = header_row.iloc[col_idx]
            if pd.notna(metric) and current_date:
                column_map[col_idx] = (current_date, str(metric))

        for col_idx in range(num_id_cols, len(data.columns)):
            if col_idx in column_map:
                date_token, metric_name = column_map[col_idx]
                data.rename(columns={col_idx: (date_token, metric_name)}, inplace=True)
            else:
                data.rename(columns={col_idx: f"_drop_{col_idx}"}, inplace=True)

        keep_columns = [col for col in data.columns if not str(col).startswith("_drop_")]
        data = data[keep_columns]

        for col in id_columns:
            if col in data.columns:
                data[col] = data[col].replace("", pd.NA).ffill()

        if id_columns and id_columns[-1] in data.columns:
            data = data[data[id_columns[-1]].notna()]

        if id_columns:
            first_col = id_columns[0]
            if first_col in data.columns:
                data = data[
                    ~data[first_col].astype(str).str.contains(
                        "ç·åˆè¨ˆ|^åˆè¨ˆ$", na=False, regex=True
                    )
                ]

        for col in id_columns[1:-1]:
            if col in data.columns:
                data = data[~data[col].astype(str).str.fullmatch("åˆè¨ˆ", na=False)]

        id_vars = [col for col in id_columns if col in data.columns]
        value_columns = [col for col in data.columns if isinstance(col, tuple)]

        if not value_columns:
            return data

        melted = data.melt(
            id_vars=id_vars,
            value_vars=value_columns,
            var_name="date_metric",
            value_name="å€¤",
        )

        melted[["æ—¥ä»˜", "æŒ‡æ¨™"]] = pd.DataFrame(
            melted["date_metric"].tolist(), index=melted.index
        )
        melted = melted.drop(columns=["date_metric"])
        melted = melted.dropna(subset=["å€¤"])

        result = (
            melted.pivot_table(
                index=id_vars + ["æ—¥ä»˜"],
                columns="æŒ‡æ¨™",
                values="å€¤",
                aggfunc="first",
            )
            .reset_index()
            .rename_axis(None, axis=1)
        )

        return result

    # ------------------------------------------------------------------ #
    # Logging helper
    # ------------------------------------------------------------------ #
    def _log_exception(self, message: str, exc: Exception) -> None:
        if self.debug:
            self.logger.exception(message)
        else:
            self.logger.error("%s: %s", message, exc)


# ---------------------------------------------------------------------- #
# Batch execution helpers
# ---------------------------------------------------------------------- #


def gather_excel_files(input_dir: Path) -> List[Path]:
    files: List[Path] = []
    for pattern in ("*.xlsx", "*.xls"):
        files.extend(sorted(input_dir.glob(pattern)))
    return sorted(files)


def _process_task(task: ConversionTask) -> ConversionResult:
    logger = configure_logging(
        task.debug, name=f"work.batch_convert.{Path(task.input_file).stem}"
    )
    converter = ExcelConverter(
        task.input_file,
        task.output_dir,
        use_wide_to_long=task.use_wide_to_long,
        analyze=task.analyze,
        debug=task.debug,
        logger=logger,
    )
    return converter.convert()


def parse_args(argv: Optional[Iterable[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="work/inputå†…ã®å…¨Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ‹¬å¤‰æ›ã—ã€CSVã¨ã—ã¦work/outputã«ä¿å­˜ã—ã¾ã™ã€‚",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""ä½¿ç”¨ä¾‹:
  python batch_convert.py
  python batch_convert.py --debug
  python batch_convert.py --use-wide-to-long
  python batch_convert.py --workers 4
""",
    )

    parser.add_argument(
        "--debug", action="store_true", help="ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ï¼ˆè©³ç´°ãƒ­ã‚°ï¼‰"
    )
    parser.add_argument(
        "--skip-features",
        action="store_true",
        help="ç‰¹å¾´é‡ä»˜ä¸ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆCSVå¤‰æ›ã®ã¿å®Ÿè¡Œï¼‰",
    )
    parser.add_argument(
        "--use-wide-to-long",
        action="store_true",
        help="POSæƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã«wide_to_long.pyã‚’é©ç”¨ï¼ˆå®Ÿé¨“çš„ï¼‰",
    )
    parser.add_argument(
        "--workers",
        type=int,
        default=1,
        help="ä¸¦åˆ—å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1ï¼‰",
    )
    parser.add_argument(
        "--input-dir",
        type=str,
        default="input",
        help="å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: inputï¼‰",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="output",
        help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: outputï¼‰",
    )
    parser.add_argument(
        "--single-file",
        type=str,
        help="å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å‡¦ç†ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰",
    )

    return parser.parse_args(args=list(argv) if argv is not None else None)


def run_debug_steps(output_dir: Path, logger: logging.Logger) -> None:
    """Execute the auxiliary scripts that clean and enrich the 06-series files."""
    script_dir = Path(__file__).resolve().parent

    steps = [
        ("06_ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±åˆ", script_dir / "merge_converted_06.py"),
        ("06_ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—", script_dir / "clean_06_data.py"),
    ]

    for description, script in steps:
        if not script.exists():
            logger.warning("%s: ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (%s)", description, script)
            continue
        logger.info("%sã‚’å®Ÿè¡Œä¸­...", description)
        try:
            subprocess.run(
                [sys.executable, str(script)],
                check=True,
                cwd=script.parent,
            )
            logger.info("âœ“ %sãŒå®Œäº†ã—ã¾ã—ãŸ", description)
        except subprocess.CalledProcessError as exc:
            logger.error("%sã«å¤±æ•—ã—ã¾ã—ãŸ: %s", description, exc)

    enrich_script = script_dir / "enrich_features_v2.py"
    cleaned_file = output_dir / "06_cleaned_20250701_20250930.csv"
    enriched_file = output_dir / "06_final_enriched_20250701_20250930.csv"
    stores_file = script_dir / "stores.csv"
    past_year_file = (
        output_dir / "01_ã€å£²ä¸Šæƒ…å ±ã€‘åº—åˆ¥å®Ÿç¸¾_20250903143116ï¼ˆ20240901-20250831ï¼‰.csv"
    )

    if enrich_script.exists() and cleaned_file.exists():
        logger.info("ç‰¹å¾´é‡ä»˜ä¸ã‚’å®Ÿè¡Œä¸­...")
        command = [
            sys.executable,
            str(enrich_script),
            str(cleaned_file),
            str(enriched_file),
            "--store-locations",
            str(stores_file),
            "--past-year-data",
            str(past_year_file),
        ]
        try:
            subprocess.run(command, check=True, cwd=enrich_script.parent)
            logger.info("âœ“ ç‰¹å¾´é‡ä»˜ä¸ãŒå®Œäº†ã—ã¾ã—ãŸ")
            logger.info("âœ… æœ€çµ‚å‡ºåŠ›: %s", enriched_file)
        except subprocess.CalledProcessError as exc:
            logger.error("ç‰¹å¾´é‡ä»˜ä¸ã«å¤±æ•—ã—ã¾ã—ãŸ: %s", exc)
    else:
        logger.warning("ç‰¹å¾´é‡ä»˜ä¸ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¾ãŸã¯ã‚¯ãƒªãƒ¼ãƒ³æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")


def main(argv: Optional[Iterable[str]] = None) -> None:
    args = parse_args(argv)
    logger = configure_logging(args.debug, name="work.batch_convert")

    try:
        input_dir = Path(args.input_dir).resolve()
        output_dir = Path(args.output_dir).resolve()

        logger.info(DISPLAY_BREAK)
        logger.info("Excel â†’ CSV ãƒãƒƒãƒå¤‰æ›")
        logger.info(DISPLAY_BREAK)
        logger.info("å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: %s", input_dir)
        logger.info("å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: %s", output_dir)

        if not input_dir.exists():
            raise FileNotFoundError(f"å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_dir}")

        output_dir.mkdir(parents=True, exist_ok=True)

        analyze = args.debug
        tasks: List[ConversionTask] = []

        if args.single_file:
            target = input_dir / args.single_file
            if not target.exists():
                logger.error("ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: %s", target)
                sys.exit(1)
            tasks.append(
                ConversionTask(
                    target,
                    output_dir,
                    args.use_wide_to_long,
                    analyze,
                    args.debug,
                )
            )
        else:
            excel_files = gather_excel_files(input_dir)
            if not excel_files:
                logger.warning("Excelãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: %s", input_dir)
                return
            logger.info("å‡¦ç†å¯¾è±¡: %dãƒ•ã‚¡ã‚¤ãƒ«", len(excel_files))
            logger.info("-" * 60)
            for file_path in excel_files:
                tasks.append(
                    ConversionTask(
                        file_path,
                        output_dir,
                        args.use_wide_to_long,
                        analyze,
                        args.debug,
                    )
                )

        results: List[ConversionResult] = []

        if args.workers > 1 and len(tasks) > 1:
            logger.info("ä¸¦åˆ—å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: %dãƒ¯ãƒ¼ã‚«ãƒ¼", args.workers)
            with ProcessPoolExecutor(max_workers=args.workers) as executor:
                future_map = {
                    executor.submit(_process_task, task): task for task in tasks
                }
                for future in as_completed(future_map):
                    result = future.result()
                    results.append(result)
        else:
            logger.info("ã‚·ãƒ³ã‚°ãƒ«ãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ¼ãƒ‰")
            for task in tasks:
                result = _process_task(task)
                results.append(result)

        summarize_results(results, logger)

        failures = [r for r in results if not r.ok and not r.skipped]
        if failures:
            logger.warning("%dãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ", len(failures))
            sys.exit(1)

        logger.info("âœ… å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®å¤‰æ›ãŒå®Œäº†ã—ã¾ã—ãŸ")

        # ç‰¹å¾´é‡ä»˜ä¸ã‚’å®Ÿè¡Œï¼ˆ--skip-featuresãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰
        if not args.skip_features:
            logger.info("")
            logger.info(DISPLAY_BREAK)
            logger.info("ã‚¹ãƒ†ãƒƒãƒ—2: ç‰¹å¾´é‡ä»˜ä¸ã‚’é–‹å§‹")
            logger.info(DISPLAY_BREAK)
            run_debug_steps(output_dir, logger)
        else:
            logger.info("ç‰¹å¾´é‡ä»˜ä¸ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸï¼ˆ--skip-featuresæŒ‡å®šï¼‰")

    except Exception as exc:
        if args.debug:
            logger.exception("âŒ ã‚¨ãƒ©ãƒ¼: %s", exc)
        else:
            logger.error("âŒ ã‚¨ãƒ©ãƒ¼: %s", exc)
        sys.exit(1)


if __name__ == "__main__":
    main()
