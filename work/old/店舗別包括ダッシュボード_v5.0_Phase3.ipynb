{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸª åº—èˆ—åˆ¥åŒ…æ‹¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ v5.0 - Phase 3å®Ÿè£…ç‰ˆ\n",
    "\n",
    "## ğŸ“‹ Phase 3ã§å®Ÿè£…ã™ã‚‹5ã¤ã®é«˜åº¦ãªåˆ†ææ©Ÿèƒ½\n",
    "\n",
    "### ğŸ¯ **AI/æ©Ÿæ¢°å­¦ç¿’ã§å£²ä¸Šã‚’æœ€å¤§åŒ–ã™ã‚‹**\n",
    "\n",
    "1. **I1. PyCaretè‡ªå‹•ç‰¹å¾´é‡é¸æŠ** - SHAPå€¤ã§é‡è¦ç‰¹å¾´é‡ã‚’è‡ªå‹•æŠ½å‡º\n",
    "2. **F2. ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œçŸ¥ãƒ»æˆé•·ç‡åˆ†æ** - Mann-Kendallæ¤œå®šã§çµ±è¨ˆçš„ã«ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æ¤œè¨¼\n",
    "3. **D2. æ¬ å“æ¤œçŸ¥ãƒ»æ©Ÿä¼šæå¤±å®šé‡åŒ–** - éå»ã®æ¬ å“ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æå¤±é¡ã‚’æ¨å®š\n",
    "4. **B2. ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æŠ½å‡º** - ãƒˆãƒƒãƒ—åº—èˆ—ã®æˆåŠŸè¦å› ã‚’è‡ªå‹•åˆ†æ\n",
    "5. **C3. ãƒãƒ¼ã‚±ãƒƒãƒˆãƒã‚¹ã‚±ãƒƒãƒˆåˆ†æ** - Aprioriã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§å•†å“é–“ã®é–¢é€£æ€§ã‚’ç™ºè¦‹\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ Phase 3ã®è¨­è¨ˆæ€æƒ³\n",
    "\n",
    "### Phase 1ãƒ»2ã¨ã®é•ã„\n",
    "- **Phase 1**: ã€Œç¾çŠ¶ã‚’æŠŠæ¡ã™ã‚‹ã€ï¼ˆè¦‹ãˆã‚‹åŒ–ï¼‰\n",
    "- **Phase 2**: ã€Œå•é¡Œã‚’äºˆé˜²ã—ã€æœ€é©è¡Œå‹•ã‚’å°ãã€ï¼ˆæœ€é©åŒ–ï¼‰\n",
    "- **Phase 3**: ã€ŒAIã§å£²ä¸Šã‚’æœ€å¤§åŒ–ã™ã‚‹ã€ï¼ˆè‡ªå‹•åŒ–ãƒ»é«˜åº¦åŒ–ï¼‰\n",
    "\n",
    "### 3ã¤ã®æŸ±\n",
    "1. **è§£é‡ˆå¯èƒ½ãªAI**: ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã§ã¯ãªãã€ãªãœãã†äºˆæ¸¬ã—ãŸã‹ã‚’èª¬æ˜\n",
    "2. **ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹**: äººé–“ã§ã¯æ°—ã¥ã‘ãªã„å•†å“é–“ã®é–¢é€£æ€§ã‚’ç™ºè¦‹\n",
    "3. **ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å­¦ç¿’**: ãƒˆãƒƒãƒ—åº—èˆ—ã®æˆåŠŸè¦å› ã‚’ä»–åº—ã«æ¨ªå±•é–‹\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š\n",
    "import font_setup\n",
    "JP_FP = font_setup.setup_fonts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªå…ˆèª­ã¿\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotlyï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    import plotly.io as pio\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "\n",
    "# ipywidgets\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, HTML, clear_output\n",
    "    WIDGETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WIDGETS_AVAILABLE = False\n",
    "\n",
    "# matplotlibå…±é€šè¨­å®š\n",
    "plt.rcParams['figure.figsize'] = (18, 12)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 150\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# seaborn\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# pandas\n",
    "pd.set_option('display.unicode.east_asian_width', True)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('ğŸª åº—èˆ—åˆ¥åŒ…æ‹¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ v5.0'.center(80))\n",
    "print('='*80)\n",
    "print(f'\\nâœ… ç’°å¢ƒè¨­å®šå®Œäº†')\n",
    "print(f'   å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime(\"%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S\")}')\n",
    "print(f'   pandas: {pd.__version__}')\n",
    "print(f'   matplotlib: {plt.matplotlib.__version__}')\n",
    "print(f'   Plotly: {\"åˆ©ç”¨å¯èƒ½\" if PLOTLY_AVAILABLE else \"æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\"}')\n",
    "print(f'   ipywidgets: {\"åˆ©ç”¨å¯èƒ½\" if WIDGETS_AVAILABLE else \"æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\"}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªå…ˆèª­ã¿\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotlyï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    import plotly.io as pio\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "\n",
    "# ipywidgets\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, HTML, clear_output\n",
    "    WIDGETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WIDGETS_AVAILABLE = False\n",
    "\n",
    "# matplotlibå…±é€šè¨­å®š\n",
    "plt.rcParams['figure.figsize'] = (18, 12)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 150\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# seaborn\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# pandas\n",
    "pd.set_option('display.unicode.east_asian_width', True)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('ğŸª åº—èˆ—åˆ¥åŒ…æ‹¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ v5.0'.center(80))\n",
    "print('='*80)\n",
    "print(f'\\nâœ… ç’°å¢ƒè¨­å®šå®Œäº†')\n",
    "print(f'   å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime(\"%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S\")}')\n",
    "print(f'   pandas: {pd.__version__}')\n",
    "print(f'   matplotlib: {plt.matplotlib.__version__}')\n",
    "print(f'   Plotly: {\"åˆ©ç”¨å¯èƒ½\" if PLOTLY_AVAILABLE else \"æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\"}')\n",
    "print(f'   ipywidgets: {\"åˆ©ç”¨å¯èƒ½\" if WIDGETS_AVAILABLE else \"æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\"}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªå…ˆèª­ã¿\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotlyï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    import plotly.io as pio\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "\n",
    "# ipywidgets\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, HTML, clear_output\n",
    "    WIDGETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WIDGETS_AVAILABLE = False\n",
    "\n",
    "# matplotlibå…±é€šè¨­å®š\n",
    "plt.rcParams['figure.figsize'] = (18, 12)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 150\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# seaborn\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# pandas\n",
    "pd.set_option('display.unicode.east_asian_width', True)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('ğŸª åº—èˆ—åˆ¥åŒ…æ‹¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ v5.0'.center(80))\n",
    "print('='*80)\n",
    "print(f'\\nâœ… ç’°å¢ƒè¨­å®šå®Œäº†')\n",
    "print(f'   å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime(\"%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S\")}')\n",
    "print(f'   pandas: {pd.__version__}')\n",
    "print(f'   matplotlib: {plt.matplotlib.__version__}')\n",
    "print(f'   Plotly: {\"åˆ©ç”¨å¯èƒ½\" if PLOTLY_AVAILABLE else \"æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\"}')\n",
    "print(f'   ipywidgets: {\"åˆ©ç”¨å¯èƒ½\" if WIDGETS_AVAILABLE else \"æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\"}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ntry:\n    import ipywidgets as widgets\n    from IPython.display import display\n    WIDGETS_AVAILABLE = True\nexcept:\n    WIDGETS_AVAILABLE = False\nplt.rcParams['figure.figsize'] = (18, 12)\nplt.rcParams['axes.unicode_minus'] = False\nsns.set_style('whitegrid')\npd.set_option('display.max_columns', 50)\nprint('\\n' + '='*80)\nprint('ğŸª åº—èˆ—åˆ¥åŒ…æ‹¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ v5.0'.center(80))\nprint('='*80)\nprint(f'\\nâœ… ç’°å¢ƒè¨­å®šå®Œäº† {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
    "print(\"\\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "\n",
    "df_enriched = pd.read_csv('output/06_final_enriched_20250701_20250930.csv')\n",
    "df_enriched['æ—¥ä»˜'] = pd.to_datetime(df_enriched['æ—¥ä»˜'])\n",
    "\n",
    "print(f\"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "print(f\"   è¡Œæ•°: {len(df_enriched):,}\")\n",
    "print(f\"   åˆ—æ•°: {len(df_enriched.columns)}\")\n",
    "print(f\"   æœŸé–“: {df_enriched['æ—¥ä»˜'].min()} ~ {df_enriched['æ—¥ä»˜'].max()}\")\n",
    "print(f\"   åº—èˆ—æ•°: {df_enriched['åº—èˆ—'].nunique()}\")\n",
    "print(f\"   å•†å“æ•°: {df_enriched['å•†å“å'].nunique():,}\")\n",
    "\n",
    "stores = df_enriched['åº—èˆ—'].unique()\n",
    "print(f\"\\nğŸª åº—èˆ—ä¸€è¦§:\")\n",
    "for i, store in enumerate(stores, 1):\n",
    "    print(f\"   {i}. {store}\")\n",
    "\n",
    "DEFAULT_STORE = stores[0]\n",
    "try:\n",
    "    MY_STORE\n",
    "except NameError:\n",
    "    MY_STORE = DEFAULT_STORE\n",
    "print(f\"\\nğŸ¯ åˆ†æå¯¾è±¡åº—èˆ—: {MY_STORE}\")\n",
    "\n",
    "my_df = df_enriched[df_enriched['åº—èˆ—'] == MY_STORE].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ åº—èˆ—é¸æŠã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆ\n",
    "\n",
    "# åº—èˆ—ä¸€è¦§\n",
    "stores = sorted(df_enriched['åº—èˆ—'].unique())\n",
    "DEFAULT_STORE = stores[0]\n",
    "\n",
    "print(f\"\\nğŸª åˆ©ç”¨å¯èƒ½ãªåº—èˆ— ({len(stores)}åº—èˆ—):\")\n",
    "for i, store in enumerate(stores, 1):\n",
    "    print(f\"   {i}. {store}\")\n",
    "\n",
    "# åº—èˆ—é¸æŠã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆ\n",
    "if WIDGETS_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ¯ ä»¥ä¸‹ã®ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã‹ã‚‰åˆ†æå¯¾è±¡åº—èˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    store_dropdown = widgets.Dropdown(\n",
    "        options=stores,\n",
    "        value=DEFAULT_STORE,\n",
    "        description='åˆ†æå¯¾è±¡åº—èˆ—:',\n",
    "        disabled=False,\n",
    "        style={'description_width': '120px'},\n",
    "        layout=widgets.Layout(width='500px')\n",
    "    )\n",
    "    \n",
    "    info_label = widgets.HTML(\n",
    "        value=\"<b>ğŸ’¡ ãƒ’ãƒ³ãƒˆ:</b> åº—èˆ—ã‚’å¤‰æ›´ã™ã‚‹ã¨ã€ä»¥é™ã®ã™ã¹ã¦ã®åˆ†æãŒé¸æŠã—ãŸåº—èˆ—ã§å†è¨ˆç®—ã•ã‚Œã¾ã™ã€‚\"\n",
    "    )\n",
    "    \n",
    "    display(widgets.VBox([store_dropdown, info_label]))\n",
    "    \n",
    "    # é¸æŠã•ã‚ŒãŸåº—èˆ—\n",
    "    MY_STORE = store_dropdown.value\n",
    "else:\n",
    "    pass\n",
    "    # ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆãŒä½¿ãˆãªã„å ´åˆ\n",
    "    print(f\"\\nğŸ¯ åˆ†æå¯¾è±¡åº—èˆ—: {MY_STORE} (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)\")\n",
    "\n",
    "# åº—èˆ—ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "my_df = df_enriched[df_enriched['åº—èˆ—'] == MY_STORE].copy()\n",
    "\n",
    "print(f\"\\nâœ… é¸æŠã•ã‚ŒãŸåº—èˆ—: {MY_STORE}\")\n",
    "print(f\"   å¯¾è±¡ãƒ‡ãƒ¼ã‚¿: {len(my_df):,}è¡Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ï¼ˆå­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼‰\n",
    "\n",
    "def validate_data_column(df, col_name, analysis_name=\"åˆ†æ\"):\n",
    "    \"\"\"ãƒ‡ãƒ¼ã‚¿ã‚«ãƒ©ãƒ ã®å­˜åœ¨ã¨æœ‰åŠ¹æ€§ã‚’ãƒã‚§ãƒƒã‚¯\"\"\"\n",
    "    if col_name not in df.columns:\n",
    "        print(f\"âš ï¸ {analysis_name}: '{col_name}' ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“\")\n",
    "        return False\n",
    "    \n",
    "    non_null_count = df[col_name].notna().sum()\n",
    "    coverage = non_null_count / len(df) * 100\n",
    "    \n",
    "    if coverage < 50:\n",
    "        print(f\"âš ï¸ {analysis_name}: '{col_name}' ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ãŒä½ã„ ({coverage:.1f}%)\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "print(\"\\nğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ä¸­...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# å¿…é ˆã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯\n",
    "required_cols = ['æ—¥ä»˜', 'å£²ä¸Šé‡‘é¡', 'åº—èˆ—']\n",
    "for col in required_cols:\n",
    "    if col in df_enriched.columns:\n",
    "        print(f\"âœ… å¿…é ˆã‚«ãƒ©ãƒ  '{col}' - å­˜åœ¨\")\n",
    "    else:\n",
    "        print(f\"âŒ å¿…é ˆã‚«ãƒ©ãƒ  '{col}' - ä¸è¶³\")\n",
    "        print(f\"âŒ å¿…é ˆã‚«ãƒ©ãƒ  '{col}' - ä¸è¶³\")\n",
    "\n",
    "# ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯\n",
    "optional_cols = {\n",
    "    'æ°—è±¡ãƒ‡ãƒ¼ã‚¿': ['æœ€é«˜æ°—æ¸©', 'é™æ°´é‡'],\n",
    "    'å‰å¹´ãƒ‡ãƒ¼ã‚¿': ['æ˜¨å¹´åŒæ—¥_å£²ä¸Š', 'æ˜¨å¹´åŒæ—¥_å®¢æ•°'],\n",
    "    'æ™‚é–“å¸¯ãƒ‡ãƒ¼ã‚¿': ['æ™‚åˆ»', 'æ™‚é–“']\n",
    "}\n",
    "\n",
    "for category, cols in optional_cols.items():\n",
    "    has_any = any(col in df_enriched.columns for col in cols)\n",
    "    if has_any:\n",
    "        available_cols = [col for col in cols if col in df_enriched.columns]\n",
    "        print(f\"âœ… {category}: {', '.join(available_cols)}\")\n",
    "    else:\n",
    "        print(f\"âŒ å¿…é ˆã‚«ãƒ©ãƒ  '{col}' - ä¸è¶³\")\n",
    "        print(f\"âš ï¸ {category}: åˆ©ç”¨ä¸å¯ï¼ˆä»£æ›¿ãƒ­ã‚¸ãƒƒã‚¯ä½¿ç”¨ï¼‰\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"âœ… ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å®Œäº†\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ¤– ã€æ©Ÿèƒ½1ã€‘PyCaretè‡ªå‹•ç‰¹å¾´é‡é¸æŠãƒ»é‡è¦åº¦åˆ†æ\n",
    "\n",
    "## SHAPå€¤ã§ã€Œãªãœãã®äºˆæ¸¬ã«ãªã£ãŸã‹ã€ã‚’èª¬æ˜\n",
    "\n",
    "### SHAP (SHapley Additive exPlanations) ã¨ã¯\n",
    "- ã‚²ãƒ¼ãƒ ç†è«–ã«åŸºã¥ãç‰¹å¾´é‡ã®è²¢çŒ®åº¦åˆ†æ\n",
    "- å„ç‰¹å¾´é‡ãŒäºˆæ¸¬ã«ã©ã‚Œã ã‘å¯„ä¸ã—ãŸã‹ã‚’å®šé‡åŒ–\n",
    "- ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–ã®å½±éŸ¿ã‚’å¯è¦–åŒ–\n",
    "\n",
    "### åˆ†æå†…å®¹\n",
    "1. **ã‚°ãƒ­ãƒ¼ãƒãƒ«ç‰¹å¾´é‡é‡è¦åº¦** - å…¨ä½“ã§æœ€ã‚‚é‡è¦ãªç‰¹å¾´é‡TOP 20\n",
    "2. **SHAP Summary Plot** - å„ç‰¹å¾´é‡ã®å½±éŸ¿ã®åˆ†å¸ƒ\n",
    "3. **SHAP Dependence Plot** - ç‰¹å®šç‰¹å¾´é‡ã¨äºˆæ¸¬ã®é–¢ä¿‚\n",
    "4. **ç‰¹å¾´é‡ã®è‡ªå‹•å‰Šæ¸›** - é‡è¦åº¦ãŒä½ã„ç‰¹å¾´é‡ã‚’é™¤å¤–\n",
    "5. **äºˆæ¸¬ç²¾åº¦ã®æ¯”è¼ƒ** - å‰Šæ¸›å‰å¾Œã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n# ğŸ“Š ã‚°ãƒ©ãƒ•ã®è¦‹æ–¹ã‚¬ã‚¤ãƒ‰\n#\n# ã€ç‰¹å¾´é‡é‡è¦åº¦ã‚°ãƒ©ãƒ•ã€‘\n#   ãƒ»æ£’ãŒé•·ã„é …ç›® â†’ å£²ä¸Šäºˆæ¸¬ã«å¤§ããå½±éŸ¿ã™ã‚‹è¦ç´ \n#   ãƒ»ä¸Šä½3ã¤ã®è¦ç´ ã«æ³¨ç›®ã—ã¦æ–½ç­–ã‚’è€ƒãˆã‚‹\n#\n#   ä¾‹ï¼‰ã€Œæœ€é«˜æ°—æ¸©ã€ãŒä¸Šä½ â†’ æ°—æ¸©ã«ã‚ˆã‚‹å•†å“å…¥æ›¿ãŒåŠ¹æœçš„\n#       ã€Œæ›œæ—¥ã€ãŒä¸Šä½ â†’ æ›œæ—¥åˆ¥ã®å“æƒãˆå¤‰æ›´ãŒé‡è¦\n#       ã€Œæ˜¨å¹´åŒæ—¥_å£²ä¸Šã€ãŒä¸Šä½ â†’ å‰å¹´ãƒ‡ãƒ¼ã‚¿ã‚’å‚è€ƒã«ã—ãŸç™ºæ³¨ãŒæœ‰åŠ¹\n#\n#   âœ… åˆ¤æ–­åŸºæº–: é‡è¦åº¦0.1ä»¥ä¸Šã®è¦ç´ ã«é›†ä¸­ã—ã¦å¯¾ç­–ã‚’æ‰“ã¤\n\n\n",
    "# ğŸ¤– PyCaretè‡ªå‹•ç‰¹å¾´é‡é¸æŠã‚¨ãƒ³ã‚¸ãƒ³\n",
    "\n",
    "if PYCARET_AVAILABLE:\n",
    "    print(\"\\nğŸ¤– PyCaretè‡ªå‹•ç‰¹å¾´é‡é¸æŠãƒ»é‡è¦åº¦åˆ†æ\\n\")\n",
    "    print(\"=\" * 120)\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "    print(\"\\nğŸ“Š ç‰¹å¾´é‡æº–å‚™ä¸­...\")\n",
    "    \n",
    "    # ç‰¹å¾´é‡å€™è£œï¼ˆPhase 1ã‚ˆã‚Šã‚‚æ‹¡å¼µï¼‰\n",
    "    feature_candidates = [\n",
    "        # åŸºæœ¬æ™‚é–“\n",
    "        'æ›œæ—¥', 'æœˆ', 'æ—¥', 'é€±ç•ªå·',\n",
    "        # ãƒ•ãƒ©ã‚°\n",
    "        'ç¥æ—¥ãƒ•ãƒ©ã‚°', 'é€±æœ«ãƒ•ãƒ©ã‚°', 'å¹³æ—¥ãƒ•ãƒ©ã‚°',\n",
    "        # ã‚¤ãƒ™ãƒ³ãƒˆ\n",
    "        'çµ¦æ–™æ—¥', 'é€£ä¼‘ãƒ•ãƒ©ã‚°', 'é€£ä¼‘æ—¥æ•°', 'é€£ä¼‘åˆæ—¥', 'é€£ä¼‘æœ€çµ‚æ—¥',\n",
    "        'GW', 'ç›†ä¼‘ã¿', 'å¹´æœ«å¹´å§‹',\n",
    "        # å­¦æ ¡\n",
    "        'å¤ä¼‘ã¿', 'å†¬ä¼‘ã¿',\n",
    "        # å­£ç¯€å¤‰å‹•\n",
    "        'å­£ç¯€å¤‰å‹•æŒ‡æ•°_æœˆ', 'å­£ç¯€å¤‰å‹•æŒ‡æ•°_é€±', 'å­£ç¯€_ãƒ”ãƒ¼ã‚¯æœŸ',\n",
    "        # å‰å¹´æ¯”è¼ƒ\n",
    "        'æ˜¨å¹´åŒæ—¥_å£²ä¸Š', 'æ˜¨å¹´åŒæ—¥_å®¢æ•°', 'æ˜¨å¹´åŒæ—¥_å®¢å˜ä¾¡',\n",
    "        'æ˜¨å¹´åŒæ—¥æ¯”_å£²ä¸Š_å¤‰åŒ–ç‡', 'æ˜¨å¹´åŒæ—¥æ¯”_å®¢æ•°_å¤‰åŒ–ç‡',\n",
    "    ]\n",
    "    \n",
    "    # æ°—è±¡ç‰¹å¾´é‡\n",
    "    weather_candidates = [\n",
    "        'æœ€é«˜æ°—æ¸©', 'æœ€ä½æ°—æ¸©', 'é™æ°´é‡', 'é™é›¨ãƒ•ãƒ©ã‚°',\n",
    "        'æœ€é«˜æ°—æ¸©_MA7', 'æœ€é«˜æ°—æ¸©_MA14', 'æœ€é«˜æ°—æ¸©_MA30',\n",
    "        'æ°—æ¸©ãƒˆãƒ¬ãƒ³ãƒ‰_7d', 'æ°—æ¸©ãƒˆãƒ¬ãƒ³ãƒ‰_14d',\n",
    "        'æ°—æ¸©å¤‰åŒ–_å‰æ—¥æ¯”', 'é™æ°´é‡_ç´¯ç©7d'\n",
    "    ]\n",
    "    \n",
    "    # åˆ©ç”¨å¯èƒ½ãªåˆ—ã®ã¿é¸æŠ\n",
    "    available_features = [col for col in feature_candidates if col in my_df.columns]\n",
    "    available_weather = [col for col in weather_candidates if col in my_df.columns and my_df[col].notna().sum() > 0]\n",
    "    \n",
    "    all_features = available_features + available_weather\n",
    "    \n",
    "    print(f\"   åŸºæœ¬ç‰¹å¾´é‡: {len(available_features)}å€‹\")\n",
    "    print(f\"   æ°—è±¡ç‰¹å¾´é‡: {len(available_weather)}å€‹\")\n",
    "    print(f\"   åˆè¨ˆ: {len(all_features)}å€‹\")\n",
    "    \n",
    "    # å•†å“åˆ¥æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿\n",
    "    product_daily = my_df.groupby(['å•†å“å', 'æ—¥ä»˜']).agg({\n",
    "        'å£²ä¸Šé‡‘é¡': 'sum',\n",
    "        **{col: 'first' for col in all_features}\n",
    "    }).reset_index()\n",
    "    \n",
    "    # TOP 50å•†å“ã®ã¿ï¼ˆè¨ˆç®—æ™‚é–“çŸ­ç¸®ï¼‰\n",
    "    top_products = my_df.groupby('å•†å“å')['å£²ä¸Šé‡‘é¡'].sum().nlargest(50).index\n",
    "    product_daily = product_daily[product_daily['å•†å“å'].isin(top_products)]\n",
    "    \n",
    "    # æ¬ æå€¤å‰Šé™¤\n",
    "    product_daily = product_daily.dropna(subset=['å£²ä¸Šé‡‘é¡'] + all_features)\n",
    "    \n",
    "    print(f\"\\n   ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿: {len(product_daily):,}è¡Œ\")\n",
    "    \n",
    "    if len(product_daily) >= 100:\n",
    "        print(\"\\nâ³ PyCaretã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...ï¼ˆæ•°åˆ†ã‹ã‹ã‚Šã¾ã™ï¼‰\")\n",
    "        \n",
    "        # PyCaretã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "        reg = setup(\n",
    "            data=product_daily,\n",
    "            target='å£²ä¸Šé‡‘é¡',\n",
    "            categorical_features=['å•†å“å'] if 'å•†å“å' in all_features else None,\n",
    "            numeric_features=[col for col in all_features if col != 'å•†å“å'],\n",
    "            fold_strategy='timeseries',\n",
    "            fold=3,\n",
    "            normalize=True,\n",
    "            feature_selection=True,\n",
    "            feature_selection_threshold=0.8,\n",
    "            remove_multicollinearity=True,\n",
    "            multicollinearity_threshold=0.9,\n",
    "            session_id=42,\n",
    "            verbose=False,\n",
    "            html=False\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†\")\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’\n",
    "        print(\"\\nâ³ LightGBMãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...\")\n",
    "        model = create_model('lightgbm', verbose=False)\n",
    "        \n",
    "        print(\"âœ… ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†\")\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡\n",
    "        results = pull()\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ï¼ˆ3åˆ†å‰²æ™‚ç³»åˆ—CVï¼‰\")\n",
    "        print(\"=\" * 120)\n",
    "        print(f\"   MAE:  Â¥{results['å¹³å‡çµ¶å¯¾èª¤å·®'].mean():,.0f}\")\n",
    "        print(f\"   RMSE: Â¥{results['äºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·®'].mean():,.0f}\")\n",
    "        print(f\"   R2:   {results['R2'].mean():.4f}\")\n",
    "        print(f\"   MAPE: {results['å¹³å‡çµ¶å¯¾ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆèª¤å·®'].mean():.2f}%\")\n",
    "        \n",
    "        # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆLightGBMçµ„ã¿è¾¼ã¿ï¼‰\n",
    "        print(f\"\\n\\nğŸ” ç‰¹å¾´é‡é‡è¦åº¦åˆ†æï¼ˆLightGBMï¼‰\")\n",
    "        print(\"=\" * 120)\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feature_names = get_config('X_train').columns\n",
    "            importance_df = pd.DataFrame({\n",
    "                'ç‰¹å¾´é‡': feature_names,\n",
    "                'é‡è¦åº¦': model.feature_importances_\n",
    "            }).sort_values('é‡è¦åº¦', ascending=False)\n",
    "            \n",
    "            print(f\"\\nç‰¹å¾´é‡é‡è¦åº¦ TOP 20:\")\n",
    "            print(\"-\" * 120)\n",
    "            for idx, row in importance_df.head(20).iterrows():\n",
    "                feature = row['ç‰¹å¾´é‡']\n",
    "                importance = row['é‡è¦åº¦']\n",
    "                bar = \"â–ˆ\" * int(importance * 50)\n",
    "                print(f\"   {feature:<40} {bar:<50} {importance:.4f}\")\n",
    "            \n",
    "            # å¯è¦–åŒ–\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "            \n",
    "            # 1. TOP 20ç‰¹å¾´é‡é‡è¦åº¦\n",
    "            ax1 = axes[0]\n",
    "            top20 = importance_df.head(20)\n",
    "            ax1.barh(range(len(top20)), top20['é‡è¦åº¦'], color='#4ECDC4', edgecolor='black')\n",
    "            ax1.set_yticks(range(len(top20)))\n",
    "            ax1.set_yticklabels(top20['ç‰¹å¾´é‡'], fontsize=10)\n",
    "            ax1.set_xlabel('é‡è¦åº¦ã‚¹ã‚³ã‚¢', fontsize=12, fontproperties=JP_FP)\n",
    "            ax1.set_title('Feature Importance TOP 20 (LightGBM)', fontsize=14, fontproperties=JP_FP)\n",
    "            ax1.invert_yaxis()\n",
    "            ax1.grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            # 2. ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®é‡è¦åº¦é›†è¨ˆ\n",
    "            ax2 = axes[1]\n",
    "            \n",
    "            def categorize_feature(feature):\n",
    "                if any(x in feature for x in ['æ›œæ—¥', 'æœˆ', 'æ—¥', 'é€±']):\n",
    "                    return 'æ™‚é–“'\n",
    "                elif any(x in feature for x in ['ç¥æ—¥', 'é€±æœ«', 'å¹³æ—¥']):\n",
    "                    return 'ãƒ•ãƒ©ã‚°'\n",
    "                elif any(x in feature for x in ['çµ¦æ–™', 'é€£ä¼‘', 'GW', 'ç›†', 'å¹´æœ«', 'å¤ä¼‘', 'å†¬ä¼‘']):\n",
    "                    return 'ã‚¤ãƒ™ãƒ³ãƒˆ'\n",
    "                elif any(x in feature for x in ['æ°—æ¸©', 'é™æ°´', 'é™é›¨']):\n",
    "                    return 'æ°—è±¡'\n",
    "                elif any(x in feature for x in ['æ˜¨å¹´', 'å‰å¹´']):\n",
    "                    return 'å‰å¹´æ¯”è¼ƒ'\n",
    "                elif any(x in feature for x in ['å­£ç¯€']):\n",
    "                    return 'å­£ç¯€æ€§'\n",
    "                else:\n",
    "                    pass\n",
    "                    return 'ãã®ä»–'\n",
    "            \n",
    "            importance_df['ã‚«ãƒ†ã‚´ãƒª'] = importance_df['ç‰¹å¾´é‡'].apply(categorize_feature)\n",
    "            category_importance = importance_df.groupby('ã‚«ãƒ†ã‚´ãƒª')['é‡è¦åº¦'].sum().sort_values(ascending=False)\n",
    "            \n",
    "            colors_cat = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8', '#F7DC6F', '#BB8FCE']\n",
    "            category_importance.plot(kind='pie', ax=ax2, autopct='%1.1f%%', \n",
    "                                     colors=colors_cat[:len(category_importance)],\n",
    "                                     startangle=90)\n",
    "            ax2.set_ylabel('', fontproperties=JP_FP)\n",
    "            ax2.set_title('ã‚«ãƒ†ã‚´ãƒªåˆ¥ç‰¹å¾´é‡é‡è¦åº¦', fontsize=14, fontproperties=JP_FP)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # SHAPå€¤åˆ†æï¼ˆæ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚ç°¡æ˜“ç‰ˆï¼‰\n",
    "            print(f\"\\n\\nğŸ”¬ SHAPå€¤åˆ†æï¼ˆè§£é‡ˆå¯èƒ½æ€§ï¼‰\")\n",
    "            print(\"=\" * 120)\n",
    "            print(\"   â³ SHAPå€¤è¨ˆç®—ä¸­...ï¼ˆæ•°åˆ†ã‹ã‹ã‚Šã¾ã™ï¼‰\")\n",
    "            \n",
    "            try:\n",
    "                # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆè¨ˆç®—æ™‚é–“çŸ­ç¸®ï¼‰\n",
    "                X_sample = get_config('X_train').sample(min(500, len(get_config('X_train'))), random_state=42)\n",
    "                \n",
    "                # TreeExplainerï¼ˆLightGBMç”¨ï¼‰\n",
    "                explainer = shap.TreeExplainer(model)\n",
    "                shap_values = explainer.shap_values(X_sample)\n",
    "                \n",
    "                print(\"   âœ… SHAPå€¤è¨ˆç®—å®Œäº†\")\n",
    "                \n",
    "                # SHAP Summary Plot\n",
    "                fig, ax = plt.subplots(figsize=(14, 10))\n",
    "                shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False, max_display=20)\n",
    "                plt.title('SHAPç‰¹å¾´é‡é‡è¦åº¦', fontsize=16, fontproperties=JP_FP)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # è§£é‡ˆ\n",
    "                print(f\"\\nğŸ’¡ SHAPå€¤ã«ã‚ˆã‚‹è§£é‡ˆ\")\n",
    "                print(\"-\" * 120)\n",
    "                print(f\"   SHAPå€¤ã¯å„ç‰¹å¾´é‡ãŒäºˆæ¸¬ã«ã©ã‚Œã ã‘è²¢çŒ®ã—ãŸã‹ã‚’ç¤ºã—ã¾ã™\")\n",
    "                print(f\"   ãƒ—ãƒ©ã‚¹å€¤ = å£²ä¸Šã‚’æŠ¼ã—ä¸Šã’ã‚‹è¦å› \")\n",
    "                print(f\"   ãƒã‚¤ãƒŠã‚¹å€¤ = å£²ä¸Šã‚’æŠ¼ã—ä¸‹ã’ã‚‹è¦å› \")\n",
    "                print(f\"   çµ¶å¯¾å€¤ãŒå¤§ãã„ = å½±éŸ¿åŠ›ãŒå¤§ãã„\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸ SHAPå€¤è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
    "                print(f\"   é€šå¸¸ã®ç‰¹å¾´é‡é‡è¦åº¦ã®ã¿ä½¿ç”¨ã—ã¾ã™\")\n",
    "        \n",
    "        # ç‰¹å¾´é‡å‰Šæ¸›ã®åŠ¹æœæ¤œè¨¼\n",
    "        print(f\"\\n\\nğŸ”§ ç‰¹å¾´é‡å‰Šæ¸›ã®åŠ¹æœæ¤œè¨¼\")\n",
    "        print(\"=\" * 120)\n",
    "        \n",
    "        # é‡è¦åº¦ã®ä½ã„ç‰¹å¾´é‡ã‚’å‰Šé™¤\n",
    "        threshold = importance_df['é‡è¦åº¦'].quantile(0.2)  # ä¸‹ä½20%ã‚’å‰Šé™¤\n",
    "        low_importance_features = importance_df[importance_df['é‡è¦åº¦'] < threshold]['ç‰¹å¾´é‡'].tolist()\n",
    "        \n",
    "        print(f\"   å‰Šæ¸›å€™è£œç‰¹å¾´é‡: {len(low_importance_features)}å€‹ï¼ˆä¸‹ä½20%ï¼‰\")\n",
    "        print(f\"   å‰Šæ¸›å¾Œç‰¹å¾´é‡æ•°: {len(feature_names) - len(low_importance_features)}å€‹\")\n",
    "        \n",
    "        if len(low_importance_features) > 0:\n",
    "            print(f\"\\n   å‰Šæ¸›ã™ã‚‹ç‰¹å¾´é‡:\")\n",
    "            for feat in low_importance_features[:10]:\n",
    "                print(f\"      - {feat}\")\n",
    "            if len(low_importance_features) > 10:\n",
    "                print(f\"      ... ä»–{len(low_importance_features)-10}å€‹\")\n",
    "            \n",
    "            print(f\"\\n   ğŸ’¡ æ¨å¥¨:\")\n",
    "            print(f\"      ã“ã‚Œã‚‰ã®ç‰¹å¾´é‡ã‚’å‰Šé™¤ã™ã‚‹ã“ã¨ã§:\")\n",
    "            print(f\"      - å­¦ç¿’æ™‚é–“ãŒç´„{(len(low_importance_features)/len(feature_names))*100:.0f}%çŸ­ç¸®\")\n",
    "            print(f\"      - ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆæ€§ãŒå‘ä¸Š\")\n",
    "            print(f\"      - éå­¦ç¿’ã®ãƒªã‚¹ã‚¯ãŒä½æ¸›\")\n",
    "            print(f\"      - äºˆæ¸¬ç²¾åº¦ã¸ã®å½±éŸ¿ã¯æœ€å°ï¼ˆé‡è¦åº¦ãŒä½ã„ãŸã‚ï¼‰\")\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "        print(f\"   âš ï¸ ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {len(product_daily)}è¡Œï¼ˆæœ€ä½100è¡Œå¿…è¦ï¼‰\")\n",
    "\n",
    "else:\n",
    "    pass\n",
    "    print(\"\\nâš ï¸ PyCaretãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ã“ã®æ©Ÿèƒ½ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™\")\n",
    "    print(\"   pip install pycaret shap ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“ˆ ã€æ©Ÿèƒ½2ã€‘ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œçŸ¥ãƒ»æˆé•·ç‡åˆ†æ\n",
    "\n",
    "## Mann-Kendallæ¤œå®šã§çµ±è¨ˆçš„ã«ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æ¤œè¨¼\n",
    "\n",
    "### Mann-Kendallæ¤œå®šã¨ã¯\n",
    "- ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®šï¼ˆæ­£è¦åˆ†å¸ƒã‚’ä»®å®šã—ãªã„ï¼‰\n",
    "- æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆå˜èª¿å¢—åŠ /æ¸›å°‘ï¼‰ã‚’æ¤œå‡º\n",
    "- på€¤ < 0.05 ã§çµ±è¨ˆçš„ã«æœ‰æ„ãªãƒˆãƒ¬ãƒ³ãƒ‰ã¨åˆ¤å®š\n",
    "\n",
    "### åˆ†æå†…å®¹\n",
    "1. **å•†å“åˆ¥ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå®š** - å„å•†å“ã®å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰ã‚’çµ±è¨ˆçš„ã«æ¤œè¨¼\n",
    "2. **æˆé•·ç‡ã®è¨ˆç®—** - CAGRï¼ˆå¹´å¹³å‡æˆé•·ç‡ï¼‰ã®ç®—å‡º\n",
    "3. **ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†é¡** - æˆé•·å•†å“/è¡°é€€å•†å“/å®‰å®šå•†å“\n",
    "4. **å­£ç¯€èª¿æ•´ãƒˆãƒ¬ãƒ³ãƒ‰** - å­£ç¯€æ€§ã‚’é™¤å»ã—ãŸçœŸã®ãƒˆãƒ¬ãƒ³ãƒ‰\n",
    "5. **å°†æ¥äºˆæ¸¬** - ç·šå½¢ãƒˆãƒ¬ãƒ³ãƒ‰ã«ã‚ˆã‚‹å¤–æŒ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ˆ ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œçŸ¥ãƒ»æˆé•·ç‡åˆ†æã‚¨ãƒ³ã‚¸ãƒ³\n",
    "\n",
    "print(\"\\nğŸ“ˆ ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œçŸ¥ãƒ»æˆé•·ç‡åˆ†æ\\n\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "def mann_kendall_test(data):\n",
    "    \"\"\"\n",
    "    Mann-Kendallæ¤œå®š\n",
    "    Returns: (trend, p_value, tau)\n",
    "        trend: 'increasing', 'decreasing', 'no trend'\n",
    "        p_value: æœ‰æ„ç¢ºç‡\n",
    "        tau: Kendallã®ã‚¿ã‚¦ï¼ˆ-1 ~ 1ï¼‰\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    if n < 3:\n",
    "        return 'insufficient data', 1.0, 0.0\n",
    "    \n",
    "    # Kendallã®ã‚¿ã‚¦è¨ˆç®—\n",
    "    s = 0\n",
    "    for i in range(n-1):\n",
    "        for j in range(i+1, n):\n",
    "            s += np.sign(data[j] - data[i])\n",
    "    \n",
    "    # åˆ†æ•£\n",
    "    var_s = n * (n - 1) * (2 * n + 5) / 18\n",
    "    \n",
    "    # Zçµ±è¨ˆé‡\n",
    "    if s > 0:\n",
    "        z = (s - 1) / np.sqrt(var_s)\n",
    "    elif s < 0:\n",
    "        z = (s + 1) / np.sqrt(var_s)\n",
    "    else:\n",
    "        pass\n",
    "        z = 0\n",
    "    \n",
    "    # på€¤ï¼ˆä¸¡å´æ¤œå®šï¼‰\n",
    "    p_value = 2 * (1 - stats.norm.cdf(abs(z)))\n",
    "    \n",
    "    # Kendallã®ã‚¿ã‚¦\n",
    "    tau = s / (n * (n - 1) / 2)\n",
    "    \n",
    "    # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®š\n",
    "    if p_value < 0.05:\n",
    "        if tau > 0:\n",
    "            trend = 'increasing'\n",
    "        else:\n",
    "            pass\n",
    "            trend = 'decreasing'\n",
    "    else:\n",
    "        pass\n",
    "        trend = 'no trend'\n",
    "    \n",
    "    return trend, p_value, tau\n",
    "\n",
    "def calculate_cagr(start_value, end_value, periods):\n",
    "    \"\"\"CAGRï¼ˆå¹´å¹³å‡æˆé•·ç‡ï¼‰ã®è¨ˆç®—\"\"\"\n",
    "    if start_value <= 0 or end_value <= 0 or periods <= 0:\n",
    "        return np.nan\n",
    "    return (np.power(end_value / start_value, 1 / periods) - 1) * 100\n",
    "\n",
    "# å•†å“åˆ¥ã®æ—¥æ¬¡å£²ä¸Š\n",
    "product_trends = []\n",
    "\n",
    "print(\"\\nâ³ å•†å“åˆ¥ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æä¸­...\")\n",
    "\n",
    "for product in my_df['å•†å“å'].unique():\n",
    "    product_data = my_df[my_df['å•†å“å'] == product].sort_values('æ—¥ä»˜')\n",
    "    \n",
    "    if len(product_data) < 10:  # æœ€ä½10æ—¥ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿\n",
    "        continue\n",
    "    \n",
    "    # æ—¥æ¬¡é›†è¨ˆ\n",
    "    daily_sales = product_data.groupby('æ—¥ä»˜')['å£²ä¸Šé‡‘é¡'].sum().reset_index()\n",
    "    \n",
    "    if len(daily_sales) < 10:\n",
    "        continue\n",
    "    \n",
    "    sales_values = daily_sales['å£²ä¸Šé‡‘é¡'].values\n",
    "    \n",
    "    # Mann-Kendallæ¤œå®š\n",
    "    trend, p_value, tau = mann_kendall_test(sales_values)\n",
    "    \n",
    "    # æˆé•·ç‡è¨ˆç®—\n",
    "    start_value = sales_values[:7].mean()  # æœ€åˆã®1é€±é–“å¹³å‡\n",
    "    end_value = sales_values[-7:].mean()  # æœ€å¾Œã®1é€±é–“å¹³å‡\n",
    "    periods = len(sales_values) / 365  # å¹´æ•°\n",
    "    \n",
    "    cagr = calculate_cagr(start_value, end_value, periods)\n",
    "    \n",
    "    # ç·šå½¢å›å¸°ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ï¼‰\n",
    "    x = np.arange(len(sales_values))\n",
    "    slope, intercept = np.polyfit(x, sales_values, 1)\n",
    "    \n",
    "    # ç·å£²ä¸Š\n",
    "    total_sales = sales_values.sum()\n",
    "    \n",
    "    product_trends.append({\n",
    "        'å•†å“å': product,\n",
    "        'ãƒˆãƒ¬ãƒ³ãƒ‰': trend,\n",
    "        'på€¤': p_value,\n",
    "        'Kendallã®ã‚¿ã‚¦': tau,\n",
    "        'å¹´å¹³å‡æˆé•·ç‡': cagr,\n",
    "        'å‚¾ã': slope,\n",
    "        'åˆ‡ç‰‡': intercept,\n",
    "        'ç·å£²ä¸Š': total_sales,\n",
    "        'ãƒ‡ãƒ¼ã‚¿æ—¥æ•°': len(sales_values),\n",
    "        'å¹³å‡æ—¥è²©': sales_values.mean(),\n",
    "        'æ¨™æº–åå·®': sales_values.std()\n",
    "    })\n",
    "\n",
    "trends_df = pd.DataFrame(product_trends)\n",
    "\n",
    "print(f\"âœ… ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æå®Œäº†: {len(trends_df)}å•†å“\")\n",
    "\n",
    "# ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†é¡\n",
    "increasing = trends_df[trends_df['ãƒˆãƒ¬ãƒ³ãƒ‰'] == 'increasing']\n",
    "decreasing = trends_df[trends_df['ãƒˆãƒ¬ãƒ³ãƒ‰'] == 'decreasing']\n",
    "no_trend = trends_df[trends_df['ãƒˆãƒ¬ãƒ³ãƒ‰'] == 'no trend']\n",
    "\n",
    "print(f\"\\nğŸ“Š ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†é¡ã‚µãƒãƒªãƒ¼\")\n",
    "print(\"=\" * 120)\n",
    "print(f\"   ğŸ“ˆ æˆé•·ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆçµ±è¨ˆçš„ã«æœ‰æ„ï¼‰: {len(increasing)}å•†å“ ({len(increasing)/len(trends_df)*100:.1f}%)\")\n",
    "print(f\"   ğŸ“‰ è¡°é€€ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆçµ±è¨ˆçš„ã«æœ‰æ„ï¼‰: {len(decreasing)}å•†å“ ({len(decreasing)/len(trends_df)*100:.1f}%)\")\n",
    "print(f\"   â¡ï¸ ãƒˆãƒ¬ãƒ³ãƒ‰ãªã—ï¼ˆå®‰å®šï¼‰:         {len(no_trend)}å•†å“ ({len(no_trend)/len(trends_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ˆ æˆé•·å•†å“TOP 10\n",
    "\n",
    "print(f\"\\n\\nğŸ“ˆ æˆé•·å•†å“ TOP 10ï¼ˆçµ±è¨ˆçš„ã«æœ‰æ„ãªãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "if len(increasing) > 0:\n",
    "    top_growth = increasing.nlargest(10, 'å¹´å¹³å‡æˆé•·ç‡')\n",
    "    \n",
    "    print(f\"\\n{'å•†å“å':<40} {'å¹´å¹³å‡æˆé•·ç‡':>10} {'ã‚¿ã‚¦':>8} {'på€¤':>10} {'å¹³å‡æ—¥è²©':>12} {'ç·å£²ä¸Š':>15}\")\n",
    "    print(\"=\" * 120)\n",
    "    \n",
    "    for _, row in top_growth.iterrows():\n",
    "        product = row['å•†å“å'][:38]\n",
    "        cagr = f\"{row['å¹´å¹³å‡æˆé•·ç‡']:+.1f}%\"\n",
    "        tau = f\"{row['Kendallã®ã‚¿ã‚¦']:.3f}\"\n",
    "        p_val = f\"{row['på€¤']:.4f}\"\n",
    "        avg_sales = f\"Â¥{row['å¹³å‡æ—¥è²©']:,.0f}\"\n",
    "        total = f\"Â¥{row['ç·å£²ä¸Š']:,.0f}\"\n",
    "        \n",
    "        print(f\"{product:<40} {cagr:>10} {tau:>8} {p_val:>10} {avg_sales:>12} {total:>15}\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ æˆé•·å•†å“ã®æˆ¦ç•¥\")\n",
    "    print(\"-\" * 120)\n",
    "    for _, row in top_growth.head(5).iterrows():\n",
    "        print(f\"\\n   ğŸ“¦ {row['å•†å“å'][:50]}\")\n",
    "        print(f\"      æˆé•·ç‡: CAGR {row['å¹´å¹³å‡æˆé•·ç‡']:+.1f}% (çµ±è¨ˆçš„æœ‰æ„æ€§: p={row['på€¤']:.4f})\")\n",
    "        print(f\"      æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:\")\n",
    "        \n",
    "        if row['å¹´å¹³å‡æˆé•·ç‡'] > 50:\n",
    "            print(f\"         ğŸ”¥ è¶…é«˜æˆé•·å•†å“: ãƒ•ã‚§ã‚¤ã‚¹æ•°ã‚’2å€ã«ã€åœ¨åº«ã‚’3å€ã«å¢—é‡\")\n",
    "            print(f\"         ğŸ”¥ ã‚¨ãƒ³ãƒ‰é™³åˆ—ãƒ»ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³å¼·åŒ–\")\n",
    "            print(f\"         ğŸ”¥ æ¬ å“ãƒªã‚¹ã‚¯æœ€å¤§æ³¨æ„\")\n",
    "        elif row['å¹´å¹³å‡æˆé•·ç‡'] > 20:\n",
    "            print(f\"         â­ é«˜æˆé•·å•†å“: ãƒ•ã‚§ã‚¤ã‚¹æ•°ã‚’1.5å€ã«ã€åœ¨åº«ã‚’2å€ã«å¢—é‡\")\n",
    "            print(f\"         â­ é–¢é€£å•†å“ã¨ä½µå£²\")\n",
    "        else:\n",
    "            pass\n",
    "            print(f\"         âœ… æˆé•·å•†å“: ãƒ•ã‚§ã‚¤ã‚¹æ•°ã‚’+20%ã€åœ¨åº«ã‚’+50%å¢—é‡\")\n",
    "            print(f\"         âœ… æˆé•·è¦å› ã‚’åˆ†æãƒ»ä»–å•†å“ã«å¿œç”¨\")\n",
    "\n",
    "else:\n",
    "    pass\n",
    "    print(\"   ï¼ˆè©²å½“å•†å“ãªã—ï¼‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“‰ è¡°é€€å•†å“TOP 10\n",
    "\n",
    "print(f\"\\n\\nğŸ“‰ è¡°é€€å•†å“ TOP 10ï¼ˆçµ±è¨ˆçš„ã«æœ‰æ„ãªãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "if len(decreasing) > 0:\n",
    "    bottom_decline = decreasing.nsmallest(10, 'å¹´å¹³å‡æˆé•·ç‡')\n",
    "    \n",
    "    print(f\"\\n{'å•†å“å':<40} {'å¹´å¹³å‡æˆé•·ç‡':>10} {'ã‚¿ã‚¦':>8} {'på€¤':>10} {'å¹³å‡æ—¥è²©':>12} {'ç·å£²ä¸Š':>15}\")\n",
    "    print(\"=\" * 120)\n",
    "    \n",
    "    for _, row in bottom_decline.iterrows():\n",
    "        product = row['å•†å“å'][:38]\n",
    "        cagr = f\"{row['å¹´å¹³å‡æˆé•·ç‡']:+.1f}%\"\n",
    "        tau = f\"{row['Kendallã®ã‚¿ã‚¦']:.3f}\"\n",
    "        p_val = f\"{row['på€¤']:.4f}\"\n",
    "        avg_sales = f\"Â¥{row['å¹³å‡æ—¥è²©']:,.0f}\"\n",
    "        total = f\"Â¥{row['ç·å£²ä¸Š']:,.0f}\"\n",
    "        \n",
    "        print(f\"{product:<40} {cagr:>10} {tau:>8} {p_val:>10} {avg_sales:>12} {total:>15}\")\n",
    "    \n",
    "    print(f\"\\nâš ï¸ è¡°é€€å•†å“ã®å¯¾ç­–\")\n",
    "    print(\"-\" * 120)\n",
    "    for _, row in bottom_decline.head(5).iterrows():\n",
    "        print(f\"\\n   ğŸ“¦ {row['å•†å“å'][:50]}\")\n",
    "        print(f\"      è¡°é€€ç‡: CAGR {row['å¹´å¹³å‡æˆé•·ç‡']:+.1f}% (çµ±è¨ˆçš„æœ‰æ„æ€§: p={row['på€¤']:.4f})\")\n",
    "        print(f\"      æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:\")\n",
    "        \n",
    "        if row['å¹´å¹³å‡æˆé•·ç‡'] < -50:\n",
    "            print(f\"         ğŸ”´ æ€¥é€Ÿè¡°é€€: SKUå‰Šæ¸›ã‚’æ¤œè¨ã€ä»£æ›¿å•†å“ã¸ã®åˆ‡æ›¿\")\n",
    "            print(f\"         ğŸ”´ åœ¨åº«ã‚’æœ€å°é™ã«ã€ç™ºæ³¨é »åº¦ã‚’ä¸‹ã’ã‚‹\")\n",
    "            print(f\"         ğŸ”´ å»ƒæ£„ãƒªã‚¹ã‚¯æœ€å¤§æ³¨æ„\")\n",
    "        elif row['å¹´å¹³å‡æˆé•·ç‡'] < -20:\n",
    "            print(f\"         ğŸŸ¡ è¡°é€€å‚¾å‘: ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã§ãƒ†ã‚³å…¥ã‚Œ\")\n",
    "            print(f\"         ğŸŸ¡ ä¾¡æ ¼è¦‹ç›´ã—ã€ã‚»ãƒƒãƒˆè²©å£²ã‚’è©¦è¡Œ\")\n",
    "        else:\n",
    "            pass\n",
    "            print(f\"         ğŸ“Š ç·©ã‚„ã‹ãªè¡°é€€: é™³åˆ—ä½ç½®ã®æ”¹å–„\")\n",
    "            print(f\"         ğŸ“Š POPã§è¨´æ±‚å¼·åŒ–\")\n",
    "\n",
    "else:\n",
    "    pass\n",
    "    print(\"   ï¼ˆè©²å½“å•†å“ãªã—ï¼‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã®å¯è¦–åŒ–\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(22, 12))\n",
    "\n",
    "# 1. ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†é¡ã®åˆ†å¸ƒ\n",
    "ax1 = axes[0, 0]\n",
    "trend_counts = trends_df['ãƒˆãƒ¬ãƒ³ãƒ‰'].value_counts()\n",
    "colors_trend = {'increasing': '#2ECC71', 'decreasing': '#E74C3C', 'no trend': '#95A5A6'}\n",
    "colors = [colors_trend.get(x, 'gray') for x in trend_counts.index]\n",
    "trend_counts.plot(kind='pie', ax=ax1, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_ylabel('', fontproperties=JP_FP)\n",
    "ax1.set_title('ãƒˆãƒ¬ãƒ³ãƒ‰ Classification Distribution', fontsize=14, fontproperties=JP_FP)\n",
    "\n",
    "# 2. CAGRåˆ†å¸ƒ\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(trends_df['å¹´å¹³å‡æˆé•·ç‡'].dropna(), bins=50, color='#3498DB', edgecolor='black', alpha=0.7)\n",
    "ax2.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Growth')\n",
    "ax2.axvline(x=trends_df['å¹´å¹³å‡æˆé•·ç‡'].median(), color='green', linestyle='--', linewidth=2, label='Median')\n",
    "ax2.set_xlabel('CAGR (%)', fontsize=12, fontproperties=JP_FP)\n",
    "ax2.set_ylabel('Frequency', fontsize=12, fontproperties=JP_FP)\n",
    "ax2.set_title('CAGR Distribution', fontsize=14, fontproperties=JP_FP)\n",
    "ax2.legend(prop=JP_FP)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. å¹³å‡æ—¥è²© vs CAGRï¼ˆæ•£å¸ƒå›³ï¼‰\n",
    "ax3 = axes[0, 2]\n",
    "scatter_data = trends_df.nlargest(100, 'ç·å£²ä¸Š')\n",
    "colors_scatter = scatter_data['ãƒˆãƒ¬ãƒ³ãƒ‰'].map(colors_trend)\n",
    "ax3.scatter(scatter_data['å¹³å‡æ—¥è²©'], scatter_data['å¹´å¹³å‡æˆé•·ç‡'], \n",
    "           c=colors_scatter, s=100, alpha=0.6, edgecolors='black')\n",
    "ax3.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax3.set_xlabel('Avg Daily Sales (JPY)', fontsize=12, fontproperties=JP_FP)\n",
    "ax3.set_ylabel('CAGR (%)', fontsize=12, fontproperties=JP_FP)\n",
    "ax3.set_title('æ—¥æ¬¡ Sales vs Growth Rate (TOP 100)', fontsize=14, fontproperties=JP_FP)\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# 4. æˆé•·å•†å“TOP 10ã®CAGR\n",
    "ax4 = axes[1, 0]\n",
    "if len(increasing) > 0:\n",
    "    top10_growth = increasing.nlargest(10, 'å¹´å¹³å‡æˆé•·ç‡')\n",
    "    product_names_short = [name[:20] + '...' if len(name) > 20 else name for name in top10_growth['å•†å“å']]\n",
    "    ax4.barh(range(len(top10_growth)), top10_growth['å¹´å¹³å‡æˆé•·ç‡'], color='#2ECC71', edgecolor='black')\n",
    "    ax4.set_yticks(range(len(top10_growth)))\n",
    "    ax4.set_yticklabels(product_names_short, fontsize=9)\n",
    "    ax4.set_xlabel('CAGR (%)', fontsize=12, fontproperties=JP_FP)\n",
    "    ax4.set_title('TOP 10 Growing Products', fontsize=14, fontproperties=JP_FP)\n",
    "    ax4.invert_yaxis()\n",
    "    ax4.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 5. è¡°é€€å•†å“TOP 10ã®CAGR\n",
    "ax5 = axes[1, 1]\n",
    "if len(decreasing) > 0:\n",
    "    bottom10_decline = decreasing.nsmallest(10, 'å¹´å¹³å‡æˆé•·ç‡')\n",
    "    product_names_short = [name[:20] + '...' if len(name) > 20 else name for name in bottom10_decline['å•†å“å']]\n",
    "    ax5.barh(range(len(bottom10_decline)), bottom10_decline['å¹´å¹³å‡æˆé•·ç‡'], color='#E74C3C', edgecolor='black')\n",
    "    ax5.set_yticks(range(len(bottom10_decline)))\n",
    "    ax5.set_yticklabels(product_names_short, fontsize=9)\n",
    "    ax5.set_xlabel('CAGR (%)', fontsize=12, fontproperties=JP_FP)\n",
    "    ax5.set_title('TOP 10 Declining Products', fontsize=14, fontproperties=JP_FP)\n",
    "    ax5.invert_yaxis()\n",
    "    ax5.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 6. Kendallã®ã‚¿ã‚¦åˆ†å¸ƒ\n",
    "ax6 = axes[1, 2]\n",
    "ax6.hist(trends_df['Kendallã®ã‚¿ã‚¦'], bins=50, color='#9B59B6', edgecolor='black', alpha=0.7)\n",
    "ax6.axvline(x=0, color='red', linestyle='--', linewidth=2, label='No Correlation')\n",
    "ax6.set_xlabel('Kendall Tau', fontsize=12, fontproperties=JP_FP)\n",
    "ax6.set_ylabel('Frequency', fontsize=12, fontproperties=JP_FP)\n",
    "ax6.set_title('Kendall Tau Distribution', fontsize=14, fontproperties=JP_FP)\n",
    "ax6.legend(prop=JP_FP)\n",
    "ax6.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œçŸ¥ãƒ»æˆé•·ç‡åˆ†æå®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸš« ã€æ©Ÿèƒ½3ã€‘æ¬ å“æ¤œçŸ¥ãƒ»æ©Ÿä¼šæå¤±å®šé‡åŒ–\n",
    "\n",
    "## éå»ã®æ¬ å“ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æå¤±é¡ã‚’æ¨å®š\n",
    "\n",
    "### æ¬ å“æ¤œçŸ¥ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ \n",
    "1. **å£²ä¸Šã‚¼ãƒ­æ—¥ã®æ¤œå‡º** - å£²ä¸ŠãŒ0å††ã®æ—¥ã‚’æ¬ å“å€™è£œã¨ã™ã‚‹\n",
    "2. **ç•°å¸¸ãªå£²ä¸Šæ¸›å°‘** - å¹³å‡æ—¥è²©ã®50%ä»¥ä¸‹ã®æ—¥ã‚’æ¬ å“ç–‘ã„ã¨ã™ã‚‹\n",
    "3. **é€£ç¶šæ¬ å“ã®æ¤œå‡º** - 2æ—¥ä»¥ä¸Šé€£ç¶šã§å£²ä¸Šã‚¼ãƒ­\n",
    "4. **å­£ç¯€æ€§ã‚’è€ƒæ…®** - åŒæ›œæ—¥ãƒ»åŒæœˆã®å¹³å‡ã¨æ¯”è¼ƒ\n",
    "\n",
    "### æ©Ÿä¼šæå¤±ã®å®šé‡åŒ–\n",
    "```\n",
    "æ©Ÿä¼šæå¤± = äºˆæƒ³å£²ä¸Š - å®Ÿéš›å£²ä¸Š\n",
    "äºˆæƒ³å£²ä¸Š = (ç›´è¿‘7æ—¥å¹³å‡ + å‰å¹´åŒæ—¥) Ã· 2\n",
    "```\n",
    "\n",
    "### æ¬ å“ã‚³ã‚¹ãƒˆ\n",
    "- **ç›´æ¥æå¤±**: è²©å£²ã§ããªã‹ã£ãŸå£²ä¸Š\n",
    "- **é–“æ¥æå¤±**: é¡§å®¢ã®åº—èˆ—é›¢ã‚Œï¼ˆæ¨å®š20%ï¼‰\n",
    "- **åˆè¨ˆæå¤±**: ç›´æ¥æå¤± Ã— 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš« æ¬ å“æ¤œçŸ¥ãƒ»æ©Ÿä¼šæå¤±å®šé‡åŒ–ã‚¨ãƒ³ã‚¸ãƒ³\n",
    "\n",
    "print(\"\\nğŸš« æ¬ å“æ¤œçŸ¥ãƒ»æ©Ÿä¼šæå¤±å®šé‡åŒ–\\n\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# å•†å“åˆ¥ã®æ—¥æ¬¡å£²ä¸Šãƒ‡ãƒ¼ã‚¿\n",
    "stockout_analysis = []\n",
    "\n",
    "print(\"\\nâ³ å•†å“åˆ¥æ¬ å“åˆ†æä¸­...\")\n",
    "\n",
    "for product in my_df['å•†å“å'].unique():\n",
    "    product_data = my_df[my_df['å•†å“å'] == product].copy()\n",
    "    \n",
    "    # æ—¥æ¬¡é›†è¨ˆ\n",
    "    daily = product_data.groupby('æ—¥ä»˜').agg({\n",
    "        'å£²ä¸Šé‡‘é¡': 'sum',\n",
    "        'å£²ä¸Šæ•°é‡': 'sum',\n",
    "        'æ˜¨å¹´åŒæ—¥_å£²ä¸Š': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    if len(daily) < 7:\n",
    "        continue\n",
    "    \n",
    "    # å¹³å‡æ—¥è²©\n",
    "    avg_sales = daily['å£²ä¸Šé‡‘é¡'].mean()\n",
    "    \n",
    "    if avg_sales <= 0:\n",
    "        continue\n",
    "    \n",
    "    # æ¬ å“å€™è£œã®æ¤œå‡º\n",
    "    # 1. å£²ä¸Šã‚¼ãƒ­æ—¥\n",
    "    zero_sales_days = daily[daily['å£²ä¸Šé‡‘é¡'] == 0]\n",
    "    \n",
    "    # 2. ç•°å¸¸ãªå£²ä¸Šæ¸›å°‘ï¼ˆå¹³å‡ã®50%ä»¥ä¸‹ï¼‰\n",
    "    low_sales_days = daily[daily['å£²ä¸Šé‡‘é¡'] < avg_sales * 0.5]\n",
    "    \n",
    "    # 3. ç§»å‹•å¹³å‡ã‹ã‚‰ã®å¤§å¹…ä¹–é›¢\n",
    "    daily['MA7'] = daily['å£²ä¸Šé‡‘é¡'].rolling(window=7, min_periods=1).mean()\n",
    "    severe_drops = daily[daily['å£²ä¸Šé‡‘é¡'] < daily['MA7'] * 0.3]\n",
    "    \n",
    "    # æ¬ å“ç–‘ã„æ—¥æ•°\n",
    "    suspected_stockout_days = pd.concat([zero_sales_days, severe_drops]).drop_duplicates()\n",
    "    \n",
    "    if len(suspected_stockout_days) == 0:\n",
    "        continue\n",
    "    \n",
    "    # æ©Ÿä¼šæå¤±ã®è¨ˆç®—\n",
    "    opportunity_loss = 0\n",
    "    \n",
    "    for _, day in suspected_stockout_days.iterrows():\n",
    "        date = day['æ—¥ä»˜']\n",
    "        actual_sales = day['å£²ä¸Šé‡‘é¡']\n",
    "        \n",
    "        # äºˆæƒ³å£²ä¸Šã®æ¨å®š\n",
    "        # æ–¹æ³•1: ç›´è¿‘7æ—¥å¹³å‡\n",
    "        recent_avg = daily[daily['æ—¥ä»˜'] < date].tail(7)['å£²ä¸Šé‡‘é¡'].mean()\n",
    "        \n",
    "        # æ–¹æ³•2: å‰å¹´åŒæ—¥\n",
    "        last_year = day['æ˜¨å¹´åŒæ—¥_å£²ä¸Š']\n",
    "        \n",
    "        # äºˆæƒ³å£²ä¸Š = (ç›´è¿‘å¹³å‡ + å‰å¹´) / 2\n",
    "        if pd.notna(last_year) and last_year > 0:\n",
    "            expected_sales = (recent_avg + last_year) / 2\n",
    "        else:\n",
    "            pass\n",
    "            expected_sales = recent_avg\n",
    "        \n",
    "        # æ©Ÿä¼šæå¤±\n",
    "        loss = max(0, expected_sales - actual_sales)\n",
    "        opportunity_loss += loss\n",
    "    \n",
    "    # é–“æ¥æå¤±ã‚’åŠ å‘³ï¼ˆé¡§å®¢é›¢ã‚ŒåŠ¹æœ +20%ï¼‰\n",
    "    total_loss = opportunity_loss * 1.2\n",
    "    \n",
    "    stockout_analysis.append({\n",
    "        'å•†å“å': product,\n",
    "        'æ¬ å“ç–‘ã„æ—¥æ•°': len(suspected_stockout_days),\n",
    "        'ã‚¼ãƒ­å£²ä¸Šæ—¥æ•°': len(zero_sales_days),\n",
    "        'ç·è²©å£²æ—¥æ•°': len(daily),\n",
    "        'æ¬ å“ç‡': len(suspected_stockout_days) / len(daily) * 100,\n",
    "        'å¹³å‡æ—¥è²©': avg_sales,\n",
    "        'ç›´æ¥æå¤±': opportunity_loss,\n",
    "        'ç·æå¤±': total_loss,\n",
    "        'æœˆé–“æå¤±æ¨å®š': total_loss / len(daily) * 30\n",
    "    })\n",
    "\n",
    "stockout_df = pd.DataFrame(stockout_analysis)\n",
    "stockout_df = stockout_df[stockout_df['æ¬ å“ç–‘ã„æ—¥æ•°'] > 0].sort_values('ç·æå¤±', ascending=False)\n",
    "\n",
    "print(f\"âœ… æ¬ å“åˆ†æå®Œäº†: {len(stockout_df)}å•†å“ã§æ¬ å“ç–‘ã„ã‚’æ¤œå‡º\")\n",
    "\n",
    "# ã‚µãƒãƒªãƒ¼\n",
    "total_opportunity_loss = stockout_df['ç·æå¤±'].sum()\n",
    "total_stockout_days = stockout_df['æ¬ å“ç–‘ã„æ—¥æ•°'].sum()\n",
    "avg_stockout_rate = stockout_df['æ¬ å“ç‡'].mean()\n",
    "\n",
    "print(f\"\\nğŸ“Š æ¬ å“åˆ†æã‚µãƒãƒªãƒ¼\")\n",
    "print(\"=\" * 120)\n",
    "print(f\"   æ¬ å“ç–‘ã„å•†å“æ•°:   {len(stockout_df)}å•†å“\")\n",
    "print(f\"   ç·æ¬ å“ç–‘ã„æ—¥æ•°:   {total_stockout_days:.0f}æ—¥\")\n",
    "print(f\"   å¹³å‡æ¬ å“ç‡:       {avg_stockout_rate:.1f}%\")\n",
    "print(f\"   ç·æ©Ÿä¼šæå¤±:       Â¥{total_opportunity_loss:,.0f}\")\n",
    "print(f\"   æœˆé–“æå¤±æ¨å®š:     Â¥{stockout_df['æœˆé–“æå¤±æ¨å®š'].sum():,.0f}\")\n",
    "print(f\"   å¹´é–“æå¤±æ¨å®š:     Â¥{stockout_df['æœˆé–“æå¤±æ¨å®š'].sum() * 12:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš« æ¬ å“ãƒªã‚¹ã‚¯TOP 20\n",
    "\n",
    "print(f\"\\n\\nğŸš« æ¬ å“ãƒªã‚¹ã‚¯TOP 20å•†å“\")\n",
    "print(\"=\" * 130)\n",
    "\n",
    "if len(stockout_df) > 0:\n",
    "    top20_stockout = stockout_df.head(20)\n",
    "    \n",
    "    print(f\"\\n{'å•†å“å':<40} {'æ¬ å“æ—¥æ•°':>10} {'æ¬ å“ç‡':>10} {'å¹³å‡æ—¥è²©':>12} {'ç·æå¤±':>15} {'æœˆé–“æå¤±':>15}\")\n",
    "    print(\"=\" * 130)\n",
    "    \n",
    "    for _, row in top20_stockout.iterrows():\n",
    "        product = row['å•†å“å'][:38]\n",
    "        days = f\"{row['æ¬ å“ç–‘ã„æ—¥æ•°']:.0f}æ—¥\"\n",
    "        rate = f\"{row['æ¬ å“ç‡']:.1f}%\"\n",
    "        avg_sales = f\"Â¥{row['å¹³å‡æ—¥è²©']:,.0f}\"\n",
    "        total_loss = f\"Â¥{row['ç·æå¤±']:,.0f}\"\n",
    "        monthly_loss = f\"Â¥{row['æœˆé–“æå¤±æ¨å®š']:,.0f}\"\n",
    "        \n",
    "        print(f\"{product:<40} {days:>10} {rate:>10} {avg_sales:>12} {total_loss:>15} {monthly_loss:>15}\")\n",
    "    \n",
    "    # è©³ç´°å¯¾ç­–\n",
    "    print(f\"\\n\\nğŸ’¡ æ¬ å“å¯¾ç­–ã®å„ªå…ˆé †ä½\")\n",
    "    print(\"=\" * 120)\n",
    "    \n",
    "    for rank, (_, row) in enumerate(top20_stockout.head(5).iterrows(), 1):\n",
    "        print(f\"\\nã€ç¬¬{rank}ä½ã€‘ {row['å•†å“å'][:50]}\")\n",
    "        print(\"-\" * 120)\n",
    "        print(f\"   æ¬ å“æ—¥æ•°: {row['æ¬ å“ç–‘ã„æ—¥æ•°']:.0f}æ—¥ / {row['ç·è²©å£²æ—¥æ•°']:.0f}æ—¥ (æ¬ å“ç‡ {row['æ¬ å“ç‡']:.1f}%)\")\n",
    "        print(f\"   å¹³å‡æ—¥è²©: Â¥{row['å¹³å‡æ—¥è²©']:,.0f}\")\n",
    "        print(f\"   æ©Ÿä¼šæå¤±: Â¥{row['ç·æå¤±']:,.0f} (æœˆé–“æ¨å®š Â¥{row['æœˆé–“æå¤±æ¨å®š']:,.0f})\")\n",
    "        \n",
    "        print(f\"\\n   ğŸ¯ æ¨å¥¨å¯¾ç­–:\")\n",
    "        \n",
    "        if row['æ¬ å“ç‡'] > 20:\n",
    "            print(f\"      ğŸ”´ æ·±åˆ»ãªæ¬ å“: ç·Šæ€¥å¯¾å¿œãŒå¿…è¦\")\n",
    "            print(f\"         1. ç™ºæ³¨é »åº¦ã‚’æ¯æ—¥ã«å¤‰æ›´\")\n",
    "            print(f\"         2. å®‰å…¨åœ¨åº«ã‚’ç¾åœ¨ã®3å€ã«è¨­å®š\")\n",
    "            print(f\"         3. è¤‡æ•°ä»•å…¥å…ˆã®ç¢ºä¿ã‚’æ¤œè¨\")\n",
    "            print(f\"         4. ä»£æ›¿å•†å“ã®æº–å‚™\")\n",
    "        elif row['æ¬ å“ç‡'] > 10:\n",
    "            print(f\"      ğŸŸ¡ é«˜é »åº¦æ¬ å“: æ”¹å–„ãŒå¿…è¦\")\n",
    "            print(f\"         1. ç™ºæ³¨é »åº¦ã‚’é€±3-4å›ã«å¤‰æ›´\")\n",
    "            print(f\"         2. å®‰å…¨åœ¨åº«ã‚’ç¾åœ¨ã®2å€ã«è¨­å®š\")\n",
    "            print(f\"         3. ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆã®è¨­å®š\")\n",
    "        else:\n",
    "            pass\n",
    "            print(f\"      ğŸ“Š æ•£ç™ºçš„æ¬ å“: ç›£è¦–å¼·åŒ–\")\n",
    "            print(f\"         1. ç™ºæ³¨é »åº¦ã‚’é€±2å›ã«å¤‰æ›´\")\n",
    "            print(f\"         2. å®‰å…¨åœ¨åº«ã‚’+50%å¢—é‡\")\n",
    "            print(f\"         3. æ¬ å“ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æï¼ˆæ›œæ—¥ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆï¼‰\")\n",
    "        \n",
    "        # ROIè¨ˆç®—\n",
    "        additional_inventory_cost = row['å¹³å‡æ—¥è²©'] * 2 * 0.3  # åœ¨åº«2å€ã€ã‚³ã‚¹ãƒˆç‡30%\n",
    "        monthly_benefit = row['æœˆé–“æå¤±æ¨å®š']\n",
    "        roi = (monthly_benefit - additional_inventory_cost) / additional_inventory_cost * 100\n",
    "        \n",
    "        print(f\"\\n   ğŸ’° ROIè©¦ç®—:\")\n",
    "        print(f\"      åœ¨åº«å¢—é‡ã‚³ã‚¹ãƒˆ: Â¥{additional_inventory_cost:,.0f}/æœˆ\")\n",
    "        print(f\"      æ¬ å“å‰Šæ¸›åŠ¹æœ:   Â¥{monthly_benefit:,.0f}/æœˆ\")\n",
    "        print(f\"      ROI:            {roi:+.0f}%\")\n",
    "        print(f\"      æŠ•è³‡å›åæœŸé–“:   å³æ™‚å›åï¼ˆåˆæœˆã‹ã‚‰é»’å­—ï¼‰\" if roi > 0 else f\"      âš ï¸ è¦æ¤œè¨\")\n",
    "\n",
    "else:\n",
    "    pass\n",
    "    print(\"   âœ… æ¬ å“ãƒªã‚¹ã‚¯ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚excellentï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ† ã€æ©Ÿèƒ½4ã€‘ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æŠ½å‡ºãƒ»æ¨ªå±•é–‹æ¨å¥¨\n",
    "\n",
    "## ãƒˆãƒƒãƒ—åº—èˆ—ã®æˆåŠŸè¦å› ã‚’è‡ªå‹•åˆ†æ\n",
    "\n",
    "### åˆ†ææ‰‹æ³•\n",
    "1. **åº—èˆ—ãƒ©ãƒ³ã‚­ãƒ³ã‚°** - å£²ä¸Šã€å®¢æ•°ã€å®¢å˜ä¾¡ã€æˆé•·ç‡ã§è©•ä¾¡\n",
    "2. **TOP vs è‡ªåº—èˆ—ã®å·®åˆ†åˆ†æ** - ä½•ãŒé•ã†ã®ã‹ã‚’å®šé‡åŒ–\n",
    "3. **æˆåŠŸå•†å“ã®æŠ½å‡º** - ãƒˆãƒƒãƒ—åº—ã§å£²ã‚Œã¦ã„ã‚‹ãŒè‡ªåº—ã§ä½èª¿ãªå•†å“\n",
    "4. **ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã®ç‰¹å®š** - æ™‚é–“å¸¯ã€é™³åˆ—ã€è²©ä¿ƒã®é•ã„\n",
    "5. **æ¨ªå±•é–‹ã®å„ªå…ˆé †ä½** - åŠ¹æœÃ—å®Ÿç¾æ€§ã§å„ªå…ˆé †ä½ä»˜ã‘\n",
    "\n",
    "### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æŒ‡æ¨™\n",
    "- å£²ä¸ŠåŠ¹ç‡ï¼ˆå£²ä¸Š/é¢ç©ï¼‰\n",
    "- å®¢å˜ä¾¡\n",
    "- æ¥åº—é »åº¦\n",
    "- å•†å“å›è»¢ç‡\n",
    "- å»ƒæ£„ç‡ï¼ˆæ¨å®šï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ† ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æŠ½å‡ºã‚¨ãƒ³ã‚¸ãƒ³\n",
    "\n",
    "print(\"\\nğŸ† ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æŠ½å‡ºãƒ»æ¨ªå±•é–‹æ¨å¥¨\\n\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# å…¨åº—èˆ—ã®å®Ÿç¸¾é›†è¨ˆ\n",
    "store_performance = []\n",
    "\n",
    "for store in stores:\n",
    "    store_data = df_enriched[df_enriched['åº—èˆ—'] == store]\n",
    "    \n",
    "    # æ—¥æ¬¡é›†è¨ˆ\n",
    "    daily_store = store_data.groupby('æ—¥ä»˜').agg({\n",
    "        'å£²ä¸Šé‡‘é¡': 'sum',\n",
    "        'å£²ä¸Šæ•°é‡': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    if len(daily_store) == 0:\n",
    "        continue\n",
    "    \n",
    "    # åŸºæœ¬æŒ‡æ¨™\n",
    "    total_sales = daily_store['å£²ä¸Šé‡‘é¡'].sum()\n",
    "    avg_daily_sales = daily_store['å£²ä¸Šé‡‘é¡'].mean()\n",
    "    total_quantity = daily_store['å£²ä¸Šæ•°é‡'].sum()\n",
    "    avg_customer_spend = total_sales / total_quantity if total_quantity > 0 else 0\n",
    "    \n",
    "    # æˆé•·ç‡ï¼ˆå‰åŠ vs å¾ŒåŠï¼‰\n",
    "    first_half = daily_store.head(len(daily_store)//2)['å£²ä¸Šé‡‘é¡'].mean()\n",
    "    second_half = daily_store.tail(len(daily_store)//2)['å£²ä¸Šé‡‘é¡'].mean()\n",
    "    growth_rate = (second_half - first_half) / first_half * 100 if first_half > 0 else 0\n",
    "    \n",
    "    # å®‰å®šæ€§ï¼ˆå¤‰å‹•ä¿‚æ•°ï¼‰\n",
    "    cv = daily_store['å£²ä¸Šé‡‘é¡'].std() / daily_store['å£²ä¸Šé‡‘é¡'].mean() if daily_store['å£²ä¸Šé‡‘é¡'].mean() > 0 else 0\n",
    "    \n",
    "    store_performance.append({\n",
    "        'åº—èˆ—': store,\n",
    "        'ç·å£²ä¸Š': total_sales,\n",
    "        'å¹³å‡æ—¥å•†': avg_daily_sales,\n",
    "        'ç·è²©å£²æ•°é‡': total_quantity,\n",
    "        'å®¢å˜ä¾¡': avg_customer_spend,\n",
    "        'æˆé•·ç‡': growth_rate,\n",
    "        'å¤‰å‹•ä¿‚æ•°': cv,\n",
    "        'ãƒ‡ãƒ¼ã‚¿æ—¥æ•°': len(daily_store)\n",
    "    })\n",
    "\n",
    "performance_df = pd.DataFrame(store_performance)\n",
    "\n",
    "# ç·åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—ï¼ˆå£²ä¸Š60% + æˆé•·ç‡20% + å®‰å®šæ€§20%ï¼‰\n",
    "performance_df['å£²ä¸Šã‚¹ã‚³ã‚¢'] = (performance_df['ç·å£²ä¸Š'] / performance_df['ç·å£²ä¸Š'].max()) * 60\n",
    "performance_df['æˆé•·ã‚¹ã‚³ã‚¢'] = (performance_df['æˆé•·ç‡'] / performance_df['æˆé•·ç‡'].max()) * 20 if performance_df['æˆé•·ç‡'].max() > 0 else 0\n",
    "performance_df['å®‰å®šæ€§ã‚¹ã‚³ã‚¢'] = (1 - performance_df['å¤‰å‹•ä¿‚æ•°'] / performance_df['å¤‰å‹•ä¿‚æ•°'].max()) * 20 if performance_df['å¤‰å‹•ä¿‚æ•°'].max() > 0 else 0\n",
    "performance_df['ç·åˆã‚¹ã‚³ã‚¢'] = performance_df['å£²ä¸Šã‚¹ã‚³ã‚¢'] + performance_df['æˆé•·ã‚¹ã‚³ã‚¢'] + performance_df['å®‰å®šæ€§ã‚¹ã‚³ã‚¢']\n",
    "\n",
    "performance_df = performance_df.sort_values('ç·åˆã‚¹ã‚³ã‚¢', ascending=False)\n",
    "performance_df['ãƒ©ãƒ³ã‚¯'] = range(1, len(performance_df) + 1)\n",
    "\n",
    "print(f\"\\nğŸ“Š åº—èˆ—ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°\")\n",
    "print(\"=\" * 130)\n",
    "print(f\"{'ãƒ©ãƒ³ã‚¯':<6} {'åº—èˆ—':<30} {'ç·å£²ä¸Š':>15} {'å¹³å‡æ—¥å•†':>12} {'å®¢å˜ä¾¡':>10} {'æˆé•·ç‡':>10} {'ç·åˆã‚¹ã‚³ã‚¢':>12}\")\n",
    "print(\"=\" * 130)\n",
    "\n",
    "for _, row in performance_df.iterrows():\n",
    "    rank = row['ãƒ©ãƒ³ã‚¯']\n",
    "    store = row['åº—èˆ—'][:28]\n",
    "    total = f\"Â¥{row['ç·å£²ä¸Š']:,.0f}\"\n",
    "    daily = f\"Â¥{row['å¹³å‡æ—¥å•†']:,.0f}\"\n",
    "    spend = f\"Â¥{row['å®¢å˜ä¾¡']:.0f}\"\n",
    "    growth = f\"{row['æˆé•·ç‡']:+.1f}%\"\n",
    "    score = f\"{row['ç·åˆã‚¹ã‚³ã‚¢']:.1f}\"\n",
    "    \n",
    "    marker = \"â˜…\" if store == MY_STORE[:28] else \"\"\n",
    "    \n",
    "    print(f\"{rank:<6} {store:<30} {total:>15} {daily:>12} {spend:>10} {growth:>10} {score:>12} {marker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ† ãƒˆãƒƒãƒ—åº—èˆ—ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹åˆ†æ\n",
    "\n",
    "top_store = performance_df.iloc[0]['åº—èˆ—']\n",
    "my_rank = performance_df[performance_df['åº—èˆ—'] == MY_STORE]['ãƒ©ãƒ³ã‚¯'].values[0]\n",
    "\n",
    "print(f\"\\n\\nğŸ† ãƒˆãƒƒãƒ—åº—èˆ—: {top_store}\")\n",
    "print(f\"   è‡ªåº—èˆ—: {MY_STORE} (ãƒ©ãƒ³ã‚¯: {my_rank}ä½/{len(performance_df)}åº—èˆ—)\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "if top_store != MY_STORE:\n",
    "    # ãƒˆãƒƒãƒ—åº—èˆ—ã®ãƒ‡ãƒ¼ã‚¿\n",
    "    top_store_data = df_enriched[df_enriched['åº—èˆ—'] == top_store]\n",
    "    \n",
    "    # å•†å“åˆ¥å£²ä¸Šï¼ˆãƒˆãƒƒãƒ—åº— vs è‡ªåº—ï¼‰\n",
    "    top_product_sales = top_store_data.groupby('å•†å“å')['å£²ä¸Šé‡‘é¡'].sum().sort_values(ascending=False)\n",
    "    my_product_sales = my_df.groupby('å•†å“å')['å£²ä¸Šé‡‘é¡'].sum()\n",
    "    \n",
    "    # å·®åˆ†åˆ†æ\n",
    "    comparison = pd.DataFrame({\n",
    "        'ãƒˆãƒƒãƒ—åº—å£²ä¸Š': top_product_sales,\n",
    "        'è‡ªåº—å£²ä¸Š': my_product_sales\n",
    "    }).fillna(0)\n",
    "    \n",
    "    comparison['å·®åˆ†'] = comparison['ãƒˆãƒƒãƒ—åº—å£²ä¸Š'] - comparison['è‡ªåº—å£²ä¸Š']\n",
    "    comparison['æ¯”ç‡'] = comparison['è‡ªåº—å£²ä¸Š'] / comparison['ãƒˆãƒƒãƒ—åº—å£²ä¸Š']\n",
    "    comparison = comparison[comparison['ãƒˆãƒƒãƒ—åº—å£²ä¸Š'] > 0].sort_values('å·®åˆ†', ascending=False)\n",
    "    \n",
    "    # ãƒˆãƒƒãƒ—åº—ã§å£²ã‚Œã¦ã„ã‚‹ãŒè‡ªåº—ã§å¼±ã„å•†å“\n",
    "    underperforming = comparison[comparison['æ¯”ç‡'] < 0.7].head(15)\n",
    "    \n",
    "    print(f\"\\nğŸ¯ æ¨ªå±•é–‹æ¨å¥¨å•†å“ TOP 15\")\n",
    "    print(\"   ï¼ˆãƒˆãƒƒãƒ—åº—ã§å¥½èª¿ã ãŒè‡ªåº—ã§å¼±ã„å•†å“ï¼‰\")\n",
    "    print(\"-\" * 120)\n",
    "    print(f\"\\n{'å•†å“å':<40} {'ãƒˆãƒƒãƒ—åº—å£²ä¸Š':>15} {'è‡ªåº—å£²ä¸Š':>15} {'å·®åˆ†':>15} {'æ¯”ç‡':>8}\")\n",
    "    print(\"=\" * 120)\n",
    "    \n",
    "    for product, row in underperforming.iterrows():\n",
    "        prod_name = product[:38]\n",
    "        top_sales = f\"Â¥{row['ãƒˆãƒƒãƒ—åº—å£²ä¸Š']:,.0f}\"\n",
    "        my_sales = f\"Â¥{row['è‡ªåº—å£²ä¸Š']:,.0f}\"\n",
    "        diff = f\"Â¥{row['å·®åˆ†']:,.0f}\"\n",
    "        ratio = f\"{row['æ¯”ç‡']*100:.0f}%\"\n",
    "        \n",
    "        print(f\"{prod_name:<40} {top_sales:>15} {my_sales:>15} {diff:>15} {ratio:>8}\")\n",
    "    \n",
    "    # å…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³\n",
    "    print(f\"\\n\\nğŸ’¡ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æ¨ªå±•é–‹ãƒ—ãƒ©ãƒ³\")\n",
    "    print(\"=\" * 120)\n",
    "    \n",
    "    total_gap = underperforming['å·®åˆ†'].sum()\n",
    "    \n",
    "    print(f\"\\n   ğŸ“Š ã‚®ãƒ£ãƒƒãƒ—åˆ†æ\")\n",
    "    print(f\"      ãƒˆãƒƒãƒ—åº—ã¨ã®å£²ä¸Šå·®: Â¥{total_gap:,.0f}\")\n",
    "    print(f\"      æœˆé–“æ©Ÿä¼šæå¤±:       Â¥{total_gap * 30 / comparison.sum()['ãƒˆãƒƒãƒ—åº—å£²ä¸Š']:,.0f}\")\n",
    "    print(f\"      å¹´é–“æ©Ÿä¼šæå¤±:       Â¥{total_gap * 365 / comparison.sum()['ãƒˆãƒƒãƒ—åº—å£²ä¸Š']:,.0f}\")\n",
    "    \n",
    "    print(f\"\\n   ğŸ¯ å„ªå…ˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆåŠ¹æœÃ—å®Ÿç¾æ€§ã§é¸å®šï¼‰\")\n",
    "    \n",
    "    for rank, (product, row) in enumerate(underperforming.head(5).iterrows(), 1):\n",
    "        print(f\"\\n   ã€ç¬¬{rank}ä½ã€‘ {product[:50]}\")\n",
    "        print(f\"      ç¾çŠ¶: è‡ªåº— Â¥{row['è‡ªåº—å£²ä¸Š']:,.0f} / ãƒˆãƒƒãƒ—åº— Â¥{row['ãƒˆãƒƒãƒ—åº—å£²ä¸Š']:,.0f} ({row['æ¯”ç‡']*100:.0f}%)\")\n",
    "        print(f\"      æ©Ÿä¼š: Â¥{row['å·®åˆ†']:,.0f}/æœŸé–“\")\n",
    "        \n",
    "        print(f\"\\n      å®Ÿæ–½ã‚¹ãƒ†ãƒƒãƒ—:\")\n",
    "        print(f\"         1. ãƒˆãƒƒãƒ—åº—ã®é™³åˆ—ãƒ»è²©ä¿ƒã‚’è¦–å¯Ÿ\")\n",
    "        print(f\"         2. ãƒ•ã‚§ã‚¤ã‚¹æ•°ã‚’ç¾åœ¨ã®1.5å€ã«å¢—é‡\")\n",
    "        print(f\"         3. ã‚¨ãƒ³ãƒ‰é™³åˆ—ã¾ãŸã¯ç›®ç«‹ã¤ä½ç½®ã«é…ç½®\")\n",
    "        print(f\"         4. POPãƒ»è©¦é£Ÿãªã©ã®è²©ä¿ƒå®Ÿæ–½\")\n",
    "        print(f\"         5. 1é€±é–“å¾Œã«åŠ¹æœã‚’æ¤œè¨¼\")\n",
    "        \n",
    "        # ROIæ¨å®š\n",
    "        additional_cost = row['ãƒˆãƒƒãƒ—åº—å£²ä¸Š'] * 0.1  # è²©ä¿ƒã‚³ã‚¹ãƒˆ10%\n",
    "        expected_benefit = row['å·®åˆ†'] * 0.5  # 50%æ”¹å–„ã‚’æƒ³å®š\n",
    "        roi = (expected_benefit - additional_cost) / additional_cost * 100\n",
    "        \n",
    "        print(f\"\\n      ROIæ¨å®š:\")\n",
    "        print(f\"         æŠ•è³‡é¡:   Â¥{additional_cost:,.0f} (è²©ä¿ƒãƒ»é™³åˆ—å¤‰æ›´)\")\n",
    "        print(f\"         æœŸå¾…åŠ¹æœ: Â¥{expected_benefit:,.0f} (50%æ”¹å–„æƒ³å®š)\")\n",
    "        print(f\"         ROI:      {roi:+.0f}%\")\n",
    "\n",
    "else:\n",
    "    pass\n",
    "    print(f\"\\n   ğŸ‰ ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼ã‚ãªãŸã®åº—èˆ—ãŒãƒˆãƒƒãƒ—ã§ã™ï¼\")\n",
    "    print(f\"   ã“ã®æˆåŠŸã‚’ä»–åº—ã«æ¨ªå±•é–‹ã—ã¾ã—ã‚‡ã†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ›’ ã€æ©Ÿèƒ½5ã€‘ãƒãƒ¼ã‚±ãƒƒãƒˆãƒã‚¹ã‚±ãƒƒãƒˆåˆ†æãƒ»ã‚¯ãƒ­ã‚¹ã‚»ãƒ«ææ¡ˆ\n",
    "\n",
    "## Aprioriã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§å•†å“é–“ã®é–¢é€£æ€§ã‚’ç™ºè¦‹\n",
    "\n",
    "### ã‚¢ã‚½ã‚·ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«\n",
    "```\n",
    "A â†’ B\n",
    "å•†å“Aã‚’è²·ã£ãŸäººã¯å•†å“Bã‚‚è²·ã†å‚¾å‘ãŒã‚ã‚‹\n",
    "```\n",
    "\n",
    "### è©•ä¾¡æŒ‡æ¨™\n",
    "1. **Supportï¼ˆæ”¯æŒåº¦ï¼‰**: A âˆ© B ã®å‡ºç¾é »åº¦\n",
    "2. **Confidenceï¼ˆç¢ºä¿¡åº¦ï¼‰**: P(B|A) = P(A âˆ© B) / P(A)\n",
    "3. **Liftï¼ˆãƒªãƒ•ãƒˆå€¤ï¼‰**: Confidence / P(B)\n",
    "   - Lift > 1: æ­£ã®ç›¸é–¢ï¼ˆä¸€ç·’ã«è²·ã‚ã‚Œã‚‹ï¼‰\n",
    "   - Lift = 1: ç‹¬ç«‹\n",
    "   - Lift < 1: è² ã®ç›¸é–¢ï¼ˆä¸€ç·’ã«è²·ã‚ã‚Œãªã„ï¼‰\n",
    "\n",
    "### æ´»ç”¨æ–¹æ³•\n",
    "- ä½µå£²é™³åˆ—ï¼ˆé–¢é€£å•†å“ã‚’éš£ã«é…ç½®ï¼‰\n",
    "- ã‚»ãƒƒãƒˆè²©å£²\n",
    "- ã‚¯ãƒ­ã‚¹ã‚»ãƒ«ææ¡ˆ\n",
    "- POPã§ã®è¨´æ±‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ›’ ãƒãƒ¼ã‚±ãƒƒãƒˆãƒã‚¹ã‚±ãƒƒãƒˆåˆ†æã‚¨ãƒ³ã‚¸ãƒ³\n",
    "\n",
    "if MLXTEND_AVAILABLE:\n",
    "    print(\"\\nğŸ›’ ãƒãƒ¼ã‚±ãƒƒãƒˆãƒã‚¹ã‚±ãƒƒãƒˆåˆ†æãƒ»ã‚¯ãƒ­ã‚¹ã‚»ãƒ«ææ¡ˆ\\n\")\n",
    "    print(\"=\" * 120)\n",
    "    \n",
    "    print(\"\\nâ³ ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­...\")\n",
    "    \n",
    "    # æ—¥ä»˜Ã—å•†å“ã®ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³è¡Œåˆ—ä½œæˆ\n",
    "    # å„æ—¥ã«è³¼å…¥ã•ã‚ŒãŸå•†å“ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
    "    transactions = my_df.groupby('æ—¥ä»˜')['å•†å“å'].apply(list).tolist()\n",
    "    \n",
    "    print(f\"   ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³æ•°: {len(transactions)}ä»¶\")\n",
    "    print(f\"   ãƒ¦ãƒ‹ãƒ¼ã‚¯å•†å“æ•°: {my_df['å•†å“å'].nunique()}å•†å“\")\n",
    "    \n",
    "    # One-Hot Encoding\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    basket_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    \n",
    "    print(f\"\\nâ³ Aprioriã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè¡Œä¸­...\")\n",
    "    \n",
    "    # Aprioriã§é »å‡ºã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆã‚’æŠ½å‡º\n",
    "    frequent_itemsets = apriori(basket_df, min_support=0.01, use_colnames=True)\n",
    "    \n",
    "    print(f\"   é »å‡ºã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆ: {len(frequent_itemsets)}å€‹\")\n",
    "    \n",
    "    if len(frequent_itemsets) > 0:\n",
    "        # ã‚¢ã‚½ã‚·ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«ã®ç”Ÿæˆ\n",
    "        print(f\"\\nâ³ ã‚¢ã‚½ã‚·ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«ç”Ÿæˆä¸­...\")\n",
    "        \n",
    "        rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "        \n",
    "        print(f\"   ã‚¢ã‚½ã‚·ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«: {len(rules)}å€‹\")\n",
    "        \n",
    "        if len(rules) > 0:\n",
    "            # Liftå€¤ã§ä¸¦ã³æ›¿ãˆ\n",
    "            rules = rules.sort_values('lift', ascending=False)\n",
    "            \n",
    "            # ConfidenceãŒé«˜ã„ï¼ˆç¢ºå®Ÿæ€§ãŒé«˜ã„ï¼‰ãƒ«ãƒ¼ãƒ«ã‚’æŠ½å‡º\n",
    "            strong_rules = rules[rules['confidence'] > 0.3]\n",
    "            \n",
    "            print(f\"\\nğŸ“Š å¼·ã„ã‚¢ã‚½ã‚·ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ« TOP 20\")\n",
    "            print(\"   ï¼ˆConfidence > 30%, Liftå€¤é †ï¼‰\")\n",
    "            print(\"=\" * 130)\n",
    "            print(f\"\\n{'å•†å“A':<35} {'â†’':<3} {'å•†å“B':<35} {'Support':>10} {'Confidence':>12} {'Lift':>8}\")\n",
    "            print(\"=\" * 130)\n",
    "            \n",
    "            for _, rule in strong_rules.head(20).iterrows():\n",
    "                antecedent = ', '.join(list(rule['antecedents']))[:33]\n",
    "                consequent = ', '.join(list(rule['consequents']))[:33]\n",
    "                support = f\"{rule['support']*100:.1f}%\"\n",
    "                confidence = f\"{rule['confidence']*100:.1f}%\"\n",
    "                lift = f\"{rule['lift']:.2f}\"\n",
    "                \n",
    "                print(f\"{antecedent:<35} {'â†’':<3} {consequent:<35} {support:>10} {confidence:>12} {lift:>8}\")\n",
    "            \n",
    "            # ã‚¯ãƒ­ã‚¹ã‚»ãƒ«ææ¡ˆ\n",
    "            print(f\"\\n\\nğŸ’¡ ã‚¯ãƒ­ã‚¹ã‚»ãƒ«ææ¡ˆï¼ˆå®Ÿè·µçš„ãªæ´»ç”¨æ–¹æ³•ï¼‰\")\n",
    "            print(\"=\" * 120)\n",
    "            \n",
    "            for rank, (_, rule) in enumerate(strong_rules.head(10).iterrows(), 1):\n",
    "                antecedent_list = list(rule['antecedents'])\n",
    "                consequent_list = list(rule['consequents'])\n",
    "                \n",
    "                print(f\"\\nã€ææ¡ˆ{rank}ã€‘\")\n",
    "                print(f\"   IF: {', '.join(antecedent_list[:2])}\")\n",
    "                print(f\"   THEN: {', '.join(consequent_list[:2])}\")\n",
    "                print(f\"   ç¢ºä¿¡åº¦: {rule['confidence']*100:.1f}% (Lift: {rule['lift']:.2f}å€)\")\n",
    "                \n",
    "                print(f\"\\n   å®Ÿæ–½æ–¹æ³•:\")\n",
    "                print(f\"      1. é™³åˆ—: {antecedent_list[0]}ã®éš£ã«{consequent_list[0]}ã‚’é…ç½®\")\n",
    "                print(f\"      2. POP: ã€Œ{antecedent_list[0]}ã¨ä¸€ç·’ã«ã„ã‹ãŒã§ã™ã‹ï¼Ÿã€\")\n",
    "                print(f\"      3. ã‚»ãƒƒãƒˆè²©å£²: 2å•†å“ã§â—‹â—‹å††ã®ã‚»ãƒƒãƒˆä¼ç”»\")\n",
    "                print(f\"      4. ãƒ¬ã‚¸ææ¡ˆ: ã€Œ{consequent_list[0]}ã¯ãŠæ±‚ã‚ã§ã™ã‹ï¼Ÿã€\")\n",
    "                \n",
    "                # æœŸå¾…åŠ¹æœã®è©¦ç®—\n",
    "                antecedent_sales = my_df[my_df['å•†å“å'].isin(antecedent_list)]['å£²ä¸Šé‡‘é¡'].sum()\n",
    "                consequent_price = my_df[my_df['å•†å“å'].isin(consequent_list)]['å£²ä¸Šé‡‘é¡'].mean()\n",
    "                \n",
    "                # ç¾åœ¨ã®ã‚¯ãƒ­ã‚¹ã‚»ãƒ«ç‡\n",
    "                current_rate = rule['support'] / (basket_df[antecedent_list[0]].sum() / len(basket_df))\n",
    "                \n",
    "                # æ–½ç­–å¾Œã®æœŸå¾…ã‚¯ãƒ­ã‚¹ã‚»ãƒ«ç‡ï¼ˆ+20%å‘ä¸Šã‚’æƒ³å®šï¼‰\n",
    "                expected_rate = current_rate * 1.2\n",
    "                \n",
    "                # æœŸå¾…å¢—å\n",
    "                expected_revenue = (antecedent_sales / len(basket_df)) * (expected_rate - current_rate) * consequent_price\n",
    "                \n",
    "                print(f\"\\n   æœŸå¾…åŠ¹æœ:\")\n",
    "                print(f\"      ç¾åœ¨ã®ã‚¯ãƒ­ã‚¹ã‚»ãƒ«ç‡: {current_rate*100:.1f}%\")\n",
    "                print(f\"      æ–½ç­–å¾Œã®ç›®æ¨™:       {expected_rate*100:.1f}% (+20%)\")\n",
    "                print(f\"      æœŸå¾…å¢—å:           Â¥{expected_revenue:,.0f}/æœŸé–“\")\n",
    "                print(f\"      æœˆé–“å¢—åæ¨å®š:       Â¥{expected_revenue * 30:,.0f}\")\n",
    "            \n",
    "            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã®ä½œæˆ\n",
    "            print(f\"\\n\\nğŸ•¸ï¸ å•†å“é–¢é€£ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³\")\n",
    "            print(\"-\" * 120)\n",
    "            \n",
    "            # TOP 30ãƒ«ãƒ¼ãƒ«ã§ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä½œæˆ\n",
    "            top_rules = strong_rules.head(30)\n",
    "            \n",
    "            G = nx.DiGraph()\n",
    "            \n",
    "            for _, rule in top_rules.iterrows():\n",
    "                for ant in rule['antecedents']:\n",
    "                    for con in rule['consequents']:\n",
    "                        G.add_edge(ant[:20], con[:20], weight=rule['lift'])\n",
    "            \n",
    "            if len(G.nodes()) > 0:\n",
    "                plt.figure(figsize=(20, 16))\n",
    "                \n",
    "                # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ\n",
    "                pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "                \n",
    "                # ãƒãƒ¼ãƒ‰ã®æç”»\n",
    "                nx.draw_networkx_nodes(G, pos, node_color='#4ECDC4', node_size=3000, alpha=0.8)\n",
    "                \n",
    "                # ã‚¨ãƒƒã‚¸ã®æç”»ï¼ˆå¤ªã• = Liftå€¤ï¼‰\n",
    "                edges = G.edges()\n",
    "                weights = [G[u][v]['weight'] for u, v in edges]\n",
    "                nx.draw_networkx_edges(G, pos, width=[w*2 for w in weights], \n",
    "                                      alpha=0.5, arrows=True, arrowsize=20,\n",
    "                                      edge_color='#95A5A6')\n",
    "                \n",
    "                # ãƒ©ãƒ™ãƒ«ã®æç”»\n",
    "                nx.draw_networkx_labels(G, pos, font_size=9, font_weight='bold')\n",
    "                \n",
    "                plt.title('Product Association Network (TOP 30 Rules)', \n",
    "                         fontsize=18, pad=20)\n",
    "                plt.axis('off')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                print(\"   ğŸ“ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã®è¦‹æ–¹:\")\n",
    "                print(\"      - ãƒãƒ¼ãƒ‰: å•†å“\")\n",
    "                print(\"      - ã‚¨ãƒƒã‚¸: é–¢é€£æ€§ï¼ˆçŸ¢å°ã®æ–¹å‘ = Aâ†’Bï¼‰\")\n",
    "                print(\"      - å¤ªã•: Liftå€¤ï¼ˆå¤ªã„ã»ã©å¼·ã„é–¢é€£ï¼‰\")\n",
    "        else:\n",
    "            pass\n",
    "            print(\"   âš ï¸ æœ‰æ„ãªã‚¢ã‚½ã‚·ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\")\n",
    "    else:\n",
    "        pass\n",
    "        print(\"   âš ï¸ é »å‡ºã‚¢ã‚¤ãƒ†ãƒ ã‚»ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\")\n",
    "        print(\"   min_supportã‚’ä¸‹ã’ã‚‹ã‹ã€ãƒ‡ãƒ¼ã‚¿é‡ã‚’å¢—ã‚„ã—ã¦ãã ã•ã„\")\n",
    "\n",
    "else:\n",
    "    pass\n",
    "    print(\"\\nâš ï¸ mlxtendãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ã“ã®æ©Ÿèƒ½ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™\")\n",
    "    print(\"   pip install mlxtend ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“ Phase 3å®Ÿè£…ã¾ã¨ã‚\n",
    "\n",
    "## âœ… å®Ÿè£…å®Œäº†ã—ãŸ5ã¤ã®é«˜åº¦ãªåˆ†ææ©Ÿèƒ½\n",
    "\n",
    "1. **PyCaretè‡ªå‹•ç‰¹å¾´é‡é¸æŠ** - SHAPå€¤ã§é‡è¦ç‰¹å¾´é‡ã‚’è‡ªå‹•æŠ½å‡ºã€ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆæ€§å‘ä¸Š\n",
    "2. **ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œçŸ¥ãƒ»æˆé•·ç‡åˆ†æ** - Mann-Kendallæ¤œå®šã§çµ±è¨ˆçš„ã«ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æ¤œè¨¼\n",
    "3. **æ¬ å“æ¤œçŸ¥ãƒ»æ©Ÿä¼šæå¤±å®šé‡åŒ–** - éå»ã®æ¬ å“ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰å¹´é–“æå¤±é¡ã‚’æ¨å®š\n",
    "4. **ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æŠ½å‡º** - ãƒˆãƒƒãƒ—åº—èˆ—ã®æˆåŠŸè¦å› ã‚’åˆ†æãƒ»æ¨ªå±•é–‹æ¨å¥¨\n",
    "5. **ãƒãƒ¼ã‚±ãƒƒãƒˆãƒã‚¹ã‚±ãƒƒãƒˆåˆ†æ** - Aprioriã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§å•†å“é–“ã®é–¢é€£æ€§ã‚’ç™ºè¦‹\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Phase 3ã®æˆæœ\n",
    "\n",
    "### Phase 1ãƒ»2ã¨ã®é•ã„\n",
    "- **Phase 1**: ã€Œç¾çŠ¶ã‚’æŠŠæ¡ã™ã‚‹ã€ï¼ˆè¦‹ãˆã‚‹åŒ–ï¼‰\n",
    "- **Phase 2**: ã€Œå•é¡Œã‚’äºˆé˜²ã—ã€æœ€é©è¡Œå‹•ã‚’å°ãã€ï¼ˆæœ€é©åŒ–ï¼‰\n",
    "- **Phase 3**: ã€ŒAIã§å£²ä¸Šã‚’æœ€å¤§åŒ–ã™ã‚‹ã€ï¼ˆè‡ªå‹•åŒ–ãƒ»é«˜åº¦åŒ–ï¼‰\n",
    "\n",
    "### å°å…¥åŠ¹æœï¼ˆæƒ³å®šï¼‰\n",
    "- ç‰¹å¾´é‡å‰Šæ¸›ã«ã‚ˆã‚‹å­¦ç¿’æ™‚é–“: **-40%**\n",
    "- æ¬ å“ã«ã‚ˆã‚‹æ©Ÿä¼šæå¤±: **å¹´é–“æ•°ç™¾ä¸‡å††ã‚’å¯è¦–åŒ–**\n",
    "- ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æ¨ªå±•é–‹: **+5-10%ã®å£²ä¸Šå‘ä¸Š**\n",
    "- ã‚¯ãƒ­ã‚¹ã‚»ãƒ«æ–½ç­–: **+3-5%ã®å®¢å˜ä¾¡å‘ä¸Š**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆPhase 4ï¼‰\n",
    "\n",
    "1. **C2. æ™‚é–“å¸¯åˆ¥åˆ†æ** - ãƒ”ãƒ¼ã‚¯ã‚¿ã‚¤ãƒ ã®æœ€é©åŒ–\n",
    "2. **G2. ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³åŠ¹æœæ¸¬å®š** - ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ROIåˆ†æ\n",
    "3. **J1. What-ifã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³** - æ–½ç­–ã®äº‹å‰è©•ä¾¡\n",
    "4. **C4. ã‚¯ãƒ­ã‚¹ã‚»ãƒ«ææ¡ˆã®è‡ªå‹•åŒ–** - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "5. **H2. ãƒ¢ãƒã‚¤ãƒ«å¯¾å¿œãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ** - ã‚¹ãƒãƒ›ãƒ»ã‚¿ãƒ–ãƒ¬ãƒƒãƒˆæœ€é©åŒ–\n",
    "\n",
    "---\n",
    "\n",
    "**Phase 3ã«ã‚ˆã‚Šã€åº—èˆ—é‹å–¶ãŒã€ŒçµŒé¨“å‰‡ã€ã‹ã‚‰ã€ŒAIãƒ‰ãƒªãƒ–ãƒ³ã€ã«é€²åŒ–ã—ã¾ã—ãŸï¼** ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}