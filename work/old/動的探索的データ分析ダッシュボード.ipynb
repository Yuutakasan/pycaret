{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f37912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語フォント設定（共通モジュール）\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# よく使うライブラリを先に読み込む\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ウィジェットの有無を通知・フラグ化\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, HTML, clear_output\n",
    "    WIDGETS_AVAILABLE = True\n",
    "    print('✅ ipywidgets利用可能')\n",
    "except Exception:\n",
    "    WIDGETS_AVAILABLE = False\n",
    "    print('⚠️ ipywidgets未インストール - 一部機能制限')\n",
    "\n",
    "import font_setup\n",
    "JP_FP = font_setup.setup_fonts(show_test=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3e95fa8-0821-4e6e-8e71-5d2977044f97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 データ読み込み中...\n",
      "✅ データ読み込み成功（UTF-8）: 13,344行\n",
      "📊 GPU メモリ使用状況: 4.0/12.9 GB\n",
      "✅ クリーニング後: 13,318行\n",
      "   店舗数: 3\n",
      "   商品数: 1112\n",
      "   期間: 2025-07-09 00:00:00 ～ 2025-07-22 00:00:00\n",
      "📊 派生変数作成中...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMixedTypeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:996\u001b[39m, in \u001b[36m_fast_slow_function_call\u001b[39m\u001b[34m(func, *args, **kwargs)\u001b[39m\n\u001b[32m    991\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m nvtx.annotate(\n\u001b[32m    992\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mEXECUTE_FAST\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    993\u001b[39m     color=_CUDF_PANDAS_NVTX_COLORS[\u001b[33m\"\u001b[39m\u001b[33mEXECUTE_FAST\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    994\u001b[39m     domain=\u001b[33m\"\u001b[39m\u001b[33mcudf_pandas\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    995\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m     fast_args, fast_kwargs = \u001b[43m_fast_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m, _fast_arg(kwargs)\n\u001b[32m    997\u001b[39m     result = func(*fast_args, **fast_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:1183\u001b[39m, in \u001b[36m_fast_arg\u001b[39m\u001b[34m(arg)\u001b[39m\n\u001b[32m   1182\u001b[39m seen: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m-> \u001b[39m\u001b[32m1183\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_transform_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_fsproxy_fast\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseen\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:1108\u001b[39m, in \u001b[36m_transform_arg\u001b[39m\u001b[34m(arg, attribute_name, seen)\u001b[39m\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1108\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m   1109\u001b[39m             _transform_arg(a, attribute_name, seen) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arg\n\u001b[32m   1110\u001b[39m         )\n\u001b[32m   1111\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(arg, \u001b[33m\"\u001b[39m\u001b[33m__getnewargs_ex__\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1112\u001b[39m     \u001b[38;5;66;03m# Partial implementation of to reconstruct with\u001b[39;00m\n\u001b[32m   1113\u001b[39m     \u001b[38;5;66;03m# transformed pieces\u001b[39;00m\n\u001b[32m   1114\u001b[39m     \u001b[38;5;66;03m# This handles scipy._lib._bunch._make_tuple_bunch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:1109\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1108\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m-> \u001b[39m\u001b[32m1109\u001b[39m             \u001b[43m_transform_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseen\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arg\n\u001b[32m   1110\u001b[39m         )\n\u001b[32m   1111\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(arg, \u001b[33m\"\u001b[39m\u001b[33m__getnewargs_ex__\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1112\u001b[39m     \u001b[38;5;66;03m# Partial implementation of to reconstruct with\u001b[39;00m\n\u001b[32m   1113\u001b[39m     \u001b[38;5;66;03m# transformed pieces\u001b[39;00m\n\u001b[32m   1114\u001b[39m     \u001b[38;5;66;03m# This handles scipy._lib._bunch._make_tuple_bunch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:1108\u001b[39m, in \u001b[36m_transform_arg\u001b[39m\u001b[34m(arg, attribute_name, seen)\u001b[39m\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1108\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m   1109\u001b[39m             _transform_arg(a, attribute_name, seen) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arg\n\u001b[32m   1110\u001b[39m         )\n\u001b[32m   1111\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(arg, \u001b[33m\"\u001b[39m\u001b[33m__getnewargs_ex__\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1112\u001b[39m     \u001b[38;5;66;03m# Partial implementation of to reconstruct with\u001b[39;00m\n\u001b[32m   1113\u001b[39m     \u001b[38;5;66;03m# transformed pieces\u001b[39;00m\n\u001b[32m   1114\u001b[39m     \u001b[38;5;66;03m# This handles scipy._lib._bunch._make_tuple_bunch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:1109\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1108\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m-> \u001b[39m\u001b[32m1109\u001b[39m             \u001b[43m_transform_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseen\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arg\n\u001b[32m   1110\u001b[39m         )\n\u001b[32m   1111\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(arg, \u001b[33m\"\u001b[39m\u001b[33m__getnewargs_ex__\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1112\u001b[39m     \u001b[38;5;66;03m# Partial implementation of to reconstruct with\u001b[39;00m\n\u001b[32m   1113\u001b[39m     \u001b[38;5;66;03m# transformed pieces\u001b[39;00m\n\u001b[32m   1114\u001b[39m     \u001b[38;5;66;03m# This handles scipy._lib._bunch._make_tuple_bunch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:1064\u001b[39m, in \u001b[36m_transform_arg\u001b[39m\u001b[34m(arg, attribute_name, seen)\u001b[39m\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (_FastSlowProxy, _FastSlowProxyMeta, _FunctionProxy)):\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     typ = \u001b[38;5;28mgetattr\u001b[39m(arg, attribute_name)\n\u001b[32m   1065\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m _Unusable:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:530\u001b[39m, in \u001b[36m_FastSlowProxy._fsproxy_fast\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    525\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    526\u001b[39m \u001b[33;03mReturns the wrapped object. If the wrapped object is of \"slow\"\u001b[39;00m\n\u001b[32m    527\u001b[39m \u001b[33;03mtype, replaces it with the corresponding \"fast\" object before\u001b[39;00m\n\u001b[32m    528\u001b[39m \u001b[33;03mreturning it.\u001b[39;00m\n\u001b[32m    529\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m \u001b[38;5;28mself\u001b[39m._fsproxy_wrapped = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fsproxy_slow_to_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fsproxy_wrapped\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:192\u001b[39m, in \u001b[36mmake_final_proxy_type.<locals>._fsproxy_slow_to_fast\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fsproxy_state \u001b[38;5;129;01mis\u001b[39;00m _State.SLOW:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mslow_to_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fsproxy_wrapped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fsproxy_wrapped\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/cudf/utils/performance_tracking.py:51\u001b[39m, in \u001b[36m_performance_tracking.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m     stack.enter_context(\n\u001b[32m     45\u001b[39m         nvtx.annotate(\n\u001b[32m     46\u001b[39m             message=func.\u001b[34m__qualname__\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m         )\n\u001b[32m     50\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/cudf/core/dataframe.py:8689\u001b[39m, in \u001b[36mfrom_pandas\u001b[39m\u001b[34m(obj, nan_as_null)\u001b[39m\n\u001b[32m   8688\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, pd.Series):\n\u001b[32m-> \u001b[39m\u001b[32m8689\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnan_as_null\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnan_as_null\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   8690\u001b[39m \u001b[38;5;66;03m# This carveout for cudf.pandas is undesirable, but fixes crucial issues\u001b[39;00m\n\u001b[32m   8691\u001b[39m \u001b[38;5;66;03m# for core RAPIDS projects like cuML and cuGraph that rely on\u001b[39;00m\n\u001b[32m   8692\u001b[39m \u001b[38;5;66;03m# `cudf.from_pandas`, so we allow it for now.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/cudf/pandas/_wrappers/pandas.py:2022\u001b[39m, in \u001b[36mwrap_from_pandas_series.<locals>.wrapped_from_pandas_series\u001b[39m\u001b[34m(s, *args, **kwargs)\u001b[39m\n\u001b[32m   2021\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m s\n\u001b[32m-> \u001b[39m\u001b[32m2022\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moriginal_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/cudf/utils/performance_tracking.py:51\u001b[39m, in \u001b[36m_performance_tracking.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m     stack.enter_context(\n\u001b[32m     45\u001b[39m         nvtx.annotate(\n\u001b[32m     46\u001b[39m             message=func.\u001b[34m__qualname__\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m         )\n\u001b[32m     50\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/cudf/core/series.py:658\u001b[39m, in \u001b[36mSeries.from_pandas\u001b[39m\u001b[34m(cls, s, nan_as_null)\u001b[39m\n\u001b[32m    657\u001b[39m     warnings.simplefilter(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m     result = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnan_as_null\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnan_as_null\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/cudf/pandas/_wrappers/pandas.py:1977\u001b[39m, in \u001b[36mwrap_init.<locals>.wrapped_init\u001b[39m\u001b[34m(self, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1976\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1977\u001b[39m \u001b[43moriginal_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/cudf/utils/performance_tracking.py:51\u001b[39m, in \u001b[36m_performance_tracking.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m     stack.enter_context(\n\u001b[32m     45\u001b[39m         nvtx.annotate(\n\u001b[32m     46\u001b[39m             message=func.\u001b[34m__qualname__\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m         )\n\u001b[32m     50\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/cudf/core/series.py:504\u001b[39m, in \u001b[36mSeries.__init__\u001b[39m\u001b[34m(self, data, index, dtype, name, copy, nan_as_null)\u001b[39m\n\u001b[32m    503\u001b[39m name_from_data = data.name\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m column = \u001b[43mas_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnan_as_null\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnan_as_null\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (pd.Series, Series)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/cudf/core/column/column.py:3033\u001b[39m, in \u001b[36mas_column\u001b[39m\u001b[34m(arbitrary, nan_as_null, dtype, length)\u001b[39m\n\u001b[32m   3026\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m   3027\u001b[39m     nan_as_null \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3028\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m inferred_dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mdecimal\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mempty\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   (...)\u001b[39m\u001b[32m   3031\u001b[39m     \u001b[38;5;66;03m# Decimal can hold float(\"nan\")\u001b[39;00m\n\u001b[32m   3032\u001b[39m     \u001b[38;5;66;03m# All np.nan is not restricted by type\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3033\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MixedTypeError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot have NaN with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minferred_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   3035\u001b[39m pyarrow_array = pa.array(\n\u001b[32m   3036\u001b[39m     arbitrary,\n\u001b[32m   3037\u001b[39m     from_pandas=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   3038\u001b[39m )\n",
      "\u001b[31mMixedTypeError\u001b[39m: Cannot have NaN with mixed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m📊 派生変数作成中...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# 単価の計算\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33m単価\u001b[39m\u001b[33m'\u001b[39m] = np.where(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m売上数量\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m, df[\u001b[33m'\u001b[39m\u001b[33m売上金額\u001b[39m\u001b[33m'\u001b[39m] / df[\u001b[33m'\u001b[39m\u001b[33m売上数量\u001b[39m\u001b[33m'\u001b[39m], \u001b[32m0\u001b[39m)\n\u001b[32m     57\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33m単価\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33m単価\u001b[39m\u001b[33m'\u001b[39m].replace([np.inf, -np.inf], np.nan).fillna(\u001b[32m0\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# 販売率の計算\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:722\u001b[39m, in \u001b[36m_CallableProxyMixin.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m722\u001b[39m     result, _ = \u001b[43m_fast_slow_function_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# We cannot directly call self here because we need it to be\u001b[39;49;00m\n\u001b[32m    724\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# converted into either the fast or slow object (by\u001b[39;49;00m\n\u001b[32m    725\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# _fast_slow_function_call) to avoid infinite recursion.\u001b[39;49;00m\n\u001b[32m    726\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# TODO: When Python 3.11 is the minimum supported Python version\u001b[39;49;00m\n\u001b[32m    727\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# this can use operator.call\u001b[39;49;00m\n\u001b[32m    728\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcall_operator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:1049\u001b[39m, in \u001b[36m_fast_slow_function_call\u001b[39m\u001b[34m(func, *args, **kwargs)\u001b[39m\n\u001b[32m   1047\u001b[39m         _slow_function_call()\n\u001b[32m   1048\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m disable_module_accelerator():\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m             result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mslow_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mslow_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _maybe_wrap_result(result, func, *args, **kwargs), fast\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/cudf/pandas/fast_slow_proxy.py:28\u001b[39m, in \u001b[36mcall_operator\u001b[39m\u001b[34m(fn, args, kwargs)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_operator\u001b[39m(fn, args, kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/pandas/core/arraylike.py:56\u001b[39m, in \u001b[36mOpsMixin.__gt__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__gt__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__gt__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/pandas/core/series.py:5803\u001b[39m, in \u001b[36mSeries._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   5800\u001b[39m lvalues = \u001b[38;5;28mself\u001b[39m._values\n\u001b[32m   5801\u001b[39m rvalues = extract_array(other, extract_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m5803\u001b[39m res_values = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5805\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._construct_result(res_values, name=res_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:346\u001b[39m, in \u001b[36mcomparison_op\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m    343\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m lvalues.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     res_values = \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    349\u001b[39m     res_values = _na_arithmetic_op(lvalues, rvalues, op, is_cmp=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:131\u001b[39m, in \u001b[36mcomp_method_OBJECT_ARRAY\u001b[39m\u001b[34m(op, x, y)\u001b[39m\n\u001b[32m    129\u001b[39m     result = libops.vec_compare(x.ravel(), y.ravel(), op)\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     result = \u001b[43mlibops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result.reshape(x.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mops.pyx:107\u001b[39m, in \u001b[36mpandas._libs.ops.scalar_compare\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: '>' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 2. データ読み込みと前処理（GPU高速化）\n",
    "\n",
    "# %%\n",
    "# データパスの設定\n",
    "file_path = 'output/06_【POS情報】店別－商品別実績_TX秋葉原駅_TX六町駅_TXつくば駅_20250709_20250722.csv'\n",
    "\n",
    "# データ読み込み（cudf.pandasにより自動的にGPU加速）\n",
    "print(\"📊 データ読み込み中...\")\n",
    "try:\n",
    "    # UTF-8で試行\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    print(f\"✅ データ読み込み成功（UTF-8）: {len(df):,}行\")\n",
    "except:\n",
    "    try:\n",
    "        # CP932で試行\n",
    "        df = pd.read_csv(file_path, encoding='cp932')\n",
    "        print(f\"✅ データ読み込み成功（CP932）: {len(df):,}行\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ データ読み込みエラー: {e}\")\n",
    "        raise\n",
    "\n",
    "# GPU使用時のメモリ情報表示\n",
    "if GPU_AVAILABLE:\n",
    "    try:\n",
    "        import pynvml\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "        info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        print(f\"📊 GPU メモリ使用状況: {info.used/1e9:.1f}/{info.total/1e9:.1f} GB\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# %%\n",
    "# 基本的なデータクリーニング\n",
    "# 日付型に変換\n",
    "df['日付'] = pd.to_datetime(df['日付'])\n",
    "\n",
    "# 店舗名の正規化\n",
    "df['店舗'] = df['店舗'].str.replace('ファミリーマート　', '').str.replace('店', '')\n",
    "\n",
    "# 不要データの除外\n",
    "df = df[df['商品名'].notna() & (df['商品名'] != '不明') & (df['商品名'].str.strip() != '')]\n",
    "df = df[~df['店舗'].str.contains('合計|小計', na=False)]\n",
    "\n",
    "print(f\"✅ クリーニング後: {len(df):,}行\")\n",
    "print(f\"   店舗数: {df['店舗'].nunique()}\")\n",
    "print(f\"   商品数: {df['商品名'].nunique()}\")\n",
    "print(f\"   期間: {df['日付'].min()} ～ {df['日付'].max()}\")\n",
    "\n",
    "# %%\n",
    "# 派生変数の作成（GPU加速により高速化）\n",
    "print(\"📊 派生変数作成中...\")\n",
    "\n",
    "# 単価の計算\n",
    "df['単価'] = np.where(df['売上数量'] > 0, df['売上金額'] / df['売上数量'], 0)\n",
    "df['単価'] = df['単価'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# 販売率の計算\n",
    "df['販売率'] = np.where(df['納品数量'] > 0, df['売上数量'] / df['納品数量'], 0)\n",
    "df['販売率'] = df['販売率'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# 在庫回転日数の計算\n",
    "df['在庫回転日数'] = np.where(\n",
    "    df['売上数量'] > 0,\n",
    "    df['納品数量'] / df['売上数量'],\n",
    "    30  # 売上がない場合は30日とする\n",
    ")\n",
    "df['在庫回転日数'] = df['在庫回転日数'].clip(upper=30)\n",
    "\n",
    "# 廃棄リスクスコア（販売率が低く、在庫回転日数が長い）\n",
    "df['廃棄リスクスコア'] = (1 - df['販売率']) * df['在庫回転日数'] / 30\n",
    "\n",
    "print(\"✅ 派生変数作成完了\")\n",
    "print(f\"   平均単価: {df['単価'].mean():.0f}円\")\n",
    "print(f\"   平均販売率: {df['販売率'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c81cf9-6644-4238-b481-77dbeaf80469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # 3. 基本統計分析（GPU加速）\n",
    "\n",
    "# %%\n",
    "# 店舗別基本統計（groupby操作がGPU加速される）\n",
    "print(\"📊 店舗別統計を計算中...\")\n",
    "\n",
    "store_stats = df.groupby('店舗').agg({\n",
    "    '売上金額': ['sum', 'mean', 'std'],\n",
    "    '売上数量': ['sum', 'mean'],\n",
    "    '納品数量': ['sum', 'mean'],\n",
    "    '商品名': 'nunique',\n",
    "    '日付': 'nunique',\n",
    "    '販売率': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "# カラム名を整理\n",
    "store_stats.columns = ['_'.join(col).strip() for col in store_stats.columns.values]\n",
    "store_stats = store_stats.reset_index()\n",
    "\n",
    "# 日販の計算\n",
    "store_stats['日販'] = store_stats['売上金額_sum'] / store_stats['日付_nunique']\n",
    "\n",
    "print(\"📊 店舗別基本統計:\")\n",
    "print(store_stats[['店舗', '日販', '売上金額_sum', '販売率_mean', '商品名_nunique']])\n",
    "\n",
    "# %%\n",
    "# 商品別基本統計（上位商品）\n",
    "print(\"\\n📊 商品別統計を計算中...\")\n",
    "\n",
    "product_stats = df.groupby('商品名').agg({\n",
    "    '売上金額': 'sum',\n",
    "    '売上数量': 'sum',\n",
    "    '納品数量': 'sum',\n",
    "    '販売率': 'mean',\n",
    "    '在庫回転日数': 'mean',\n",
    "    'フェイスくくり大分類': 'first',\n",
    "    'PB/NBフラグ': 'first'\n",
    "}).round(2)\n",
    "\n",
    "# 売上順にソート\n",
    "product_stats = product_stats.sort_values('売上金額', ascending=False)\n",
    "\n",
    "print(\"\\n📊 売上上位10商品:\")\n",
    "print(product_stats.head(10)[['売上金額', '販売率', '在庫回転日数']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7855dc27-90c6-4de6-a120-86708532f9c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # 4. 売上パターン分析（GPU高速化）\n",
    "\n",
    "# %%\n",
    "# 曜日別売上分析（大規模集計もGPU加速で高速）\n",
    "print(\"📊 曜日別パターン分析中...\")\n",
    "\n",
    "weekday_analysis = df.groupby(['店舗', '曜日']).agg({\n",
    "    '売上金額': ['sum', 'mean', 'count']\n",
    "}).round(0)\n",
    "\n",
    "# 曜日順序を定義\n",
    "weekday_order = ['月', '火', '水', '木', '金', '土', '日']\n",
    "\n",
    "# 可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 店舗別曜日パターン\n",
    "for store in df['店舗'].unique():\n",
    "    store_weekday = weekday_analysis.loc[store]['売上金額']['mean']\n",
    "    store_weekday = store_weekday.reindex(weekday_order)\n",
    "    axes[0].plot(weekday_order, store_weekday, marker='o', label=store, linewidth=2)\n",
    "\n",
    "axes[0].set_title('店舗別曜日売上パターン', fontsize=14, fontproperties=JP_FP)\n",
    "axes[0].set_xlabel('曜日', fontproperties=JP_FP)\n",
    "axes[0].set_ylabel('平均売上金額', fontproperties=JP_FP)\n",
    "axes[0].legend(prop=JP_FP)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 全体の曜日パターン\n",
    "overall_weekday = df.groupby('曜日')['売上金額'].mean().reindex(weekday_order)\n",
    "bars = axes[1].bar(weekday_order, overall_weekday, color=COLORS[0])\n",
    "\n",
    "# ピークと谷を特定\n",
    "peak_day = overall_weekday.idxmax()\n",
    "low_day = overall_weekday.idxmin()\n",
    "\n",
    "# 色を変更\n",
    "for i, day in enumerate(weekday_order):\n",
    "    if day == peak_day:\n",
    "        bars[i].set_color('#2ECC71')\n",
    "    elif day == low_day:\n",
    "        bars[i].set_color('#E74C3C')\n",
    "\n",
    "axes[1].set_title('全体曜日別売上（ピーク:緑、谷:赤）', fontsize=14, fontproperties=JP_FP)\n",
    "axes[1].set_xlabel('曜日', fontproperties=JP_FP)\n",
    "axes[1].set_ylabel('平均売上金額', fontproperties=JP_FP)\n",
    "axes[1].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x/10000):.0f}万'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 分析結果を辞書に保存\n",
    "weekday_insights = {\n",
    "    'peak_day': peak_day,\n",
    "    'low_day': low_day,\n",
    "    'peak_to_low_ratio': float(overall_weekday.max() / overall_weekday.min()),\n",
    "    'pattern': overall_weekday.to_dict()\n",
    "}\n",
    "\n",
    "print(f\"\\n📊 曜日分析結果:\")\n",
    "print(f\"   ピーク曜日: {peak_day}\")\n",
    "print(f\"   最低曜日: {low_day}\")\n",
    "print(f\"   ピーク/最低比: {weekday_insights['peak_to_low_ratio']:.2f}倍\")\n",
    "\n",
    "# %%\n",
    "# 時間的トレンド分析\n",
    "daily_trend = df.groupby('日付')['売上金額'].sum()\n",
    "\n",
    "# トレンド分析\n",
    "x = np.arange(len(daily_trend))\n",
    "y = daily_trend.values\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "\n",
    "# 可視化\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(daily_trend.index, daily_trend.values, alpha=0.7, label='日次売上')\n",
    "\n",
    "# トレンドライン\n",
    "trend_line = slope * x + intercept\n",
    "plt.plot(daily_trend.index, trend_line, 'r--', linewidth=2, \n",
    "         label=f'トレンド（傾き: {slope:.0f}円/日）')\n",
    "\n",
    "# 移動平均\n",
    "ma7 = daily_trend.rolling(window=7, center=True).mean()\n",
    "plt.plot(daily_trend.index, ma7, 'g-', linewidth=2, label='7日移動平均')\n",
    "\n",
    "plt.title('売上の時系列トレンド', fontsize=14, fontproperties=JP_FP)\n",
    "plt.xlabel('日付', fontproperties=JP_FP)\n",
    "plt.ylabel('売上金額', fontproperties=JP_FP)\n",
    "plt.legend(prop=JP_FP)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "trend_insights = {\n",
    "    'direction': 'increasing' if slope > 0 else 'decreasing',\n",
    "    'daily_change': float(slope),\n",
    "    'statistical_significance': p_value < 0.05,\n",
    "    'r_squared': float(r_value ** 2)\n",
    "}\n",
    "\n",
    "print(f\"\\n📊 トレンド分析結果:\")\n",
    "print(f\"   方向: {trend_insights['direction']}\")\n",
    "print(f\"   日次変化: {trend_insights['daily_change']:.0f}円/日\")\n",
    "print(f\"   統計的有意性: {trend_insights['statistical_significance']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049122e2-a1be-46a6-8f41-b97fc96b31ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # 5. 在庫効率分析（GPU対応）\n",
    "\n",
    "# %%\n",
    "# 商品効率性マトリックス作成（大規模集計もGPU加速）\n",
    "print(\"📊 在庫効率分析中...\")\n",
    "\n",
    "efficiency_data = df.groupby('商品名').agg({\n",
    "    '売上金額': 'sum',\n",
    "    '売上数量': 'sum',\n",
    "    '納品数量': 'sum',\n",
    "    '販売率': 'mean',\n",
    "    '在庫回転日数': 'mean',\n",
    "    '廃棄リスクスコア': 'mean',\n",
    "    'フェイスくくり大分類': 'first',\n",
    "    'PB/NBフラグ': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# 効率スコアの計算（売上貢献度 × 回転率）\n",
    "efficiency_data['売上貢献度'] = efficiency_data['売上金額'] / efficiency_data['売上金額'].sum()\n",
    "efficiency_data['効率スコア'] = (\n",
    "    efficiency_data['売上貢献度'] * 1000 * \n",
    "    (1 / (efficiency_data['在庫回転日数'] + 1))\n",
    ")\n",
    "\n",
    "# %%\n",
    "# 商品分類（4象限分析）\n",
    "# 販売率と売上金額で分類\n",
    "median_rate = efficiency_data['販売率'].median()\n",
    "median_sales = efficiency_data['売上金額'].median()\n",
    "\n",
    "efficiency_data['商品分類'] = ''\n",
    "efficiency_data.loc[\n",
    "    (efficiency_data['販売率'] >= median_rate) & \n",
    "    (efficiency_data['売上金額'] >= median_sales), '商品分類'\n",
    "] = 'スター商品'\n",
    "\n",
    "efficiency_data.loc[\n",
    "    (efficiency_data['販売率'] < median_rate) & \n",
    "    (efficiency_data['売上金額'] >= median_sales), '商品分類'\n",
    "] = '要改善商品'\n",
    "\n",
    "efficiency_data.loc[\n",
    "    (efficiency_data['販売率'] >= median_rate) & \n",
    "    (efficiency_data['売上金額'] < median_sales), '商品分類'\n",
    "] = 'ニッチ商品'\n",
    "\n",
    "efficiency_data.loc[\n",
    "    (efficiency_data['販売率'] < median_rate) & \n",
    "    (efficiency_data['売上金額'] < median_sales), '商品分類'\n",
    "] = '削減候補'\n",
    "\n",
    "# 可視化\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors_map = {\n",
    "    'スター商品': '#2ECC71',\n",
    "    '要改善商品': '#F39C12',\n",
    "    'ニッチ商品': '#3498DB',\n",
    "    '削減候補': '#E74C3C'\n",
    "}\n",
    "\n",
    "for category, color in colors_map.items():\n",
    "    data = efficiency_data[efficiency_data['商品分類'] == category]\n",
    "    plt.scatter(data['販売率'] * 100, data['売上金額'] / 10000,\n",
    "                s=50, alpha=0.6, c=color, label=f'{category}（{len(data)}品）')\n",
    "\n",
    "plt.axvline(x=median_rate * 100, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.axhline(y=median_sales / 10000, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.xlabel('販売率（%）', fontproperties=JP_FP)\n",
    "plt.ylabel('売上金額（万円）', fontproperties=JP_FP)\n",
    "plt.title('商品効率性マトリックス', fontsize=14, fontproperties=JP_FP)\n",
    "plt.legend(prop=JP_FP)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 分類別統計\n",
    "classification_stats = efficiency_data.groupby('商品分類').agg({\n",
    "    '商品名': 'count',\n",
    "    '売上金額': 'sum',\n",
    "    '販売率': 'mean',\n",
    "    '在庫回転日数': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"📊 商品分類別統計:\")\n",
    "print(classification_stats)\n",
    "\n",
    "# %%\n",
    "# 削減候補商品の詳細分析\n",
    "reduction_candidates = efficiency_data[\n",
    "    efficiency_data['商品分類'] == '削減候補'\n",
    "].sort_values('効率スコア')\n",
    "\n",
    "# 削減による影響分析\n",
    "total_reduction_sales = reduction_candidates['売上金額'].sum()\n",
    "total_reduction_items = len(reduction_candidates)\n",
    "\n",
    "print(f\"\\n📊 削減候補分析:\")\n",
    "print(f\"   対象商品数: {total_reduction_items}品\")\n",
    "print(f\"   影響売上額: {total_reduction_sales/10000:.0f}万円\")\n",
    "print(f\"   全体に占める割合: {total_reduction_sales/efficiency_data['売上金額'].sum()*100:.1f}%\")\n",
    "\n",
    "# カテゴリ別削減候補\n",
    "reduction_by_category = reduction_candidates.groupby('フェイスくくり大分類').agg({\n",
    "    '商品名': 'count',\n",
    "    '売上金額': 'sum'\n",
    "}).sort_values('商品名', ascending=False)\n",
    "\n",
    "print(\"\\n📊 カテゴリ別削減候補:\")\n",
    "print(reduction_by_category.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f3f90d-7314-409e-a8cd-c1f3873a4d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # 6. 気象影響分析（GPU加速）\n",
    "\n",
    "# %%\n",
    "# 天候別売上分析\n",
    "print(\"📊 気象影響分析中...\")\n",
    "\n",
    "weather_analysis = df.groupby('天気').agg({\n",
    "    '売上金額': ['sum', 'mean', 'count'],\n",
    "    '売上数量': 'mean',\n",
    "    '商品名': 'nunique'\n",
    "}).round(0)\n",
    "\n",
    "weather_analysis.columns = ['_'.join(col).strip() for col in weather_analysis.columns.values]\n",
    "\n",
    "# 可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# 天候別平均売上\n",
    "weather_avg = weather_analysis['売上金額_mean'].sort_values(ascending=False)\n",
    "bars1 = axes[0].bar(weather_avg.index, weather_avg.values, color=COLORS[:len(weather_avg)])\n",
    "axes[0].set_title('天候別平均売上', fontsize=14, fontproperties=JP_FP)\n",
    "axes[0].set_ylabel('平均売上金額', fontproperties=JP_FP)\n",
    "axes[0].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x/10000):.0f}万'))\n",
    "\n",
    "# 数値表示\n",
    "for i, (weather, value) in enumerate(weather_avg.items()):\n",
    "    axes[0].text(i, value + value * 0.02, f'{int(value/10000, fontproperties=JP_FP)}万', ha='center')\n",
    "\n",
    "# 気温と売上の相関\n",
    "temp_corr_data = df.groupby('日付').agg({\n",
    "    '売上金額': 'sum',\n",
    "    '最高気温': 'first',\n",
    "    '最低気温': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "temp_corr_data['平均気温'] = (temp_corr_data['最高気温'] + temp_corr_data['最低気温']) / 2\n",
    "\n",
    "# 散布図\n",
    "scatter = axes[1].scatter(temp_corr_data['平均気温'], \n",
    "                         temp_corr_data['売上金額'] / 10000,\n",
    "                         c=temp_corr_data.index, cmap='coolwarm', s=50, alpha=0.7)\n",
    "\n",
    "# 相関係数\n",
    "corr_coef = temp_corr_data['平均気温'].corr(temp_corr_data['売上金額'])\n",
    "\n",
    "axes[1].set_title(f'気温と売上の関係（相関: {corr_coef:.3f}）', fontsize=14, fontproperties=JP_FP)\n",
    "axes[1].set_xlabel('平均気温（℃）', fontproperties=JP_FP)\n",
    "axes[1].set_ylabel('売上金額（万円）', fontproperties=JP_FP)\n",
    "\n",
    "# トレンドライン\n",
    "z = np.polyfit(temp_corr_data['平均気温'], temp_corr_data['売上金額'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[1].plot(temp_corr_data['平均気温'].sort_values(), \n",
    "             p(temp_corr_data['平均気温'].sort_values()) / 10000,\n",
    "             \"r--\", linewidth=2, alpha=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# 気温感応商品の特定（並列処理でGPU高速化）\n",
    "print(\"📊 気温感応商品の特定中...\")\n",
    "\n",
    "temp_sensitive_products = {}\n",
    "\n",
    "# 売上上位100商品について気温相関を計算\n",
    "top_products = product_stats.head(100).index\n",
    "\n",
    "for product in top_products:\n",
    "    product_data = df[df['商品名'] == product].groupby('日付').agg({\n",
    "        '売上金額': 'sum',\n",
    "        '最高気温': 'first'\n",
    "    })\n",
    "    \n",
    "    if len(product_data) >= 5:\n",
    "        corr = product_data['売上金額'].corr(product_data['最高気温'])\n",
    "        if abs(corr) > 0.5:  # 相関が強い商品のみ\n",
    "            temp_sensitive_products[product] = float(corr)\n",
    "\n",
    "# ソートして表示\n",
    "temp_sensitive_sorted = sorted(temp_sensitive_products.items(), \n",
    "                              key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print(\"📊 気温感応商品TOP10:\")\n",
    "for i, (product, corr) in enumerate(temp_sensitive_sorted[:10]):\n",
    "    direction = \"高温で売れる\" if corr > 0 else \"低温で売れる\"\n",
    "    print(f\"{i+1}. {product}: {corr:.3f} ({direction})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf18397-c562-425e-9a59-8bb051284757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # 7. カテゴリ・PB/NB分析（GPU高速化）\n",
    "\n",
    "# %%\n",
    "# カテゴリ別パフォーマンス（大規模集計もGPU加速）\n",
    "print(\"📊 カテゴリ・PB/NB分析中...\")\n",
    "\n",
    "category_performance = df.groupby('フェイスくくり大分類').agg({\n",
    "    '売上金額': 'sum',\n",
    "    '売上数量': 'sum',\n",
    "    '販売率': 'mean',\n",
    "    '商品名': 'nunique',\n",
    "    '在庫回転日数': 'mean'\n",
    "}).sort_values('売上金額', ascending=False)\n",
    "\n",
    "# PB/NB別パフォーマンス\n",
    "pb_nb_performance = df.groupby('PB/NBフラグ').agg({\n",
    "    '売上金額': 'sum',\n",
    "    '売上数量': 'sum',\n",
    "    '販売率': 'mean',\n",
    "    '単価': 'mean',\n",
    "    '商品名': 'nunique'\n",
    "})\n",
    "\n",
    "# 可視化\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# カテゴリ別売上構成\n",
    "top_categories = category_performance.head(10)\n",
    "axes[0, 0].pie(top_categories['売上金額'], labels=top_categories.index, \n",
    "               autopct='%1.1f%%', startangle=90)\n",
    "axes[0, 0].set_title('カテゴリ別売上構成（上位10）', fontsize=14, fontproperties=JP_FP)\n",
    "\n",
    "# カテゴリ別効率性\n",
    "axes[0, 1].scatter(top_categories['販売率'] * 100, \n",
    "                   top_categories['在庫回転日数'],\n",
    "                   s=top_categories['売上金額'] / 10000,\n",
    "                   alpha=0.6, c=range(len(top_categories)), cmap='viridis')\n",
    "\n",
    "for i, (idx, row) in enumerate(top_categories.iterrows()):\n",
    "    axes[0, 1].annotate(idx[:10], \n",
    "                        (row['販売率'] * 100, row['在庫回転日数'], fontproperties=JP_FP),\n",
    "                        fontsize=8, alpha=0.7)\n",
    "\n",
    "axes[0, 1].set_xlabel('販売率（%）', fontproperties=JP_FP)\n",
    "axes[0, 1].set_ylabel('在庫回転日数', fontproperties=JP_FP)\n",
    "axes[0, 1].set_title('カテゴリ別効率性（バブルサイズ=売上）', fontsize=14, fontproperties=JP_FP)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# PB/NB比較\n",
    "pb_nb_data = pb_nb_performance.reset_index()\n",
    "x = np.arange(len(pb_nb_data))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[1, 0].bar(x - width/2, pb_nb_data['売上金額'] / 10000, \n",
    "                       width, label='売上（万円）', color=COLORS[0])\n",
    "bars2 = axes[1, 0].bar(x + width/2, pb_nb_data['販売率'] * 100, \n",
    "                       width, label='販売率（%）', color=COLORS[1])\n",
    "\n",
    "axes[1, 0].set_xlabel('商品タイプ', fontproperties=JP_FP)\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(pb_nb_data['PB/NBフラグ'])\n",
    "axes[1, 0].set_title('PB/NB比較', fontsize=14, fontproperties=JP_FP)\n",
    "axes[1, 0].legend(prop=JP_FP)\n",
    "\n",
    "# PB/NB単価比較\n",
    "axes[1, 1].bar(pb_nb_data['PB/NBフラグ'], pb_nb_data['単価'], color=COLORS[2])\n",
    "axes[1, 1].set_ylabel('平均単価（円）', fontproperties=JP_FP)\n",
    "axes[1, 1].set_title('PB/NB平均単価比較', fontsize=14, fontproperties=JP_FP)\n",
    "\n",
    "for i, (idx, row) in enumerate(pb_nb_data.iterrows()):\n",
    "    axes[1, 1].text(i, row['単価'] + 5, f\"{row['単価']:.0f}円\", ha='center', fontproperties=JP_FP)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f0102-22d7-4d9d-9a3f-74b5d41ea67c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # 8. 機会損失分析（GPU並列処理）\n",
    "\n",
    "# %%\n",
    "# 品薄による機会損失\n",
    "print(\"📊 機会損失分析中...\")\n",
    "\n",
    "shortage_products = efficiency_data[efficiency_data['販売率'] >= 0.9].copy()\n",
    "shortage_products['推定損失額'] = shortage_products['売上金額'] * 0.1  # 10%の追加売上可能と仮定\n",
    "\n",
    "total_shortage_loss = shortage_products['推定損失額'].sum()\n",
    "\n",
    "# 過剰在庫コスト\n",
    "excess_products = efficiency_data[efficiency_data['販売率'] < 0.3].copy()\n",
    "excess_products['過剰在庫数'] = excess_products['納品数量'] - excess_products['売上数量']\n",
    "excess_products['推定コスト'] = excess_products['過剰在庫数'] * excess_products['売上金額'] / excess_products['売上数量'] * 0.3\n",
    "\n",
    "total_excess_cost = excess_products['推定コスト'].sum()\n",
    "\n",
    "# 可視化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# 機会損失サマリー\n",
    "loss_data = {\n",
    "    '品薄による機会損失': total_shortage_loss / 10000,\n",
    "    '過剰在庫コスト': total_excess_cost / 10000\n",
    "}\n",
    "\n",
    "bars = axes[0].bar(loss_data.keys(), loss_data.values(), color=['#E74C3C', '#F39C12'])\n",
    "axes[0].set_ylabel('金額（万円）', fontproperties=JP_FP)\n",
    "axes[0].set_title('改善可能な損失額', fontsize=14, fontproperties=JP_FP)\n",
    "\n",
    "for i, (key, value) in enumerate(loss_data.items()):\n",
    "    axes[0].text(i, value + value * 0.05, f'{value:.0f}万円', ha='center', fontproperties=JP_FP)\n",
    "\n",
    "total_opportunity = sum(loss_data.values())\n",
    "axes[0].text(0.5, 0.95, f'合計改善機会: {total_opportunity:.0f}万円', \n",
    "             transform=axes[0].transAxes, ha='center', fontsize=12,\n",
    "             bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5, fontproperties=JP_FP))\n",
    "\n",
    "# 商品数の内訳\n",
    "opportunity_breakdown = {\n",
    "    '品薄商品': len(shortage_products),\n",
    "    '過剰在庫商品': len(excess_products),\n",
    "    '適正商品': len(efficiency_data) - len(shortage_products) - len(excess_products)\n",
    "}\n",
    "\n",
    "axes[1].pie(opportunity_breakdown.values(), labels=opportunity_breakdown.keys(),\n",
    "            autopct='%1.0f%%', startangle=90, colors=['#E74C3C', '#F39C12', '#2ECC71'])\n",
    "axes[1].set_title('商品在庫状況の内訳', fontsize=14, fontproperties=JP_FP)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"📊 機会損失分析結果:\")\n",
    "print(f\"   品薄による損失: {total_shortage_loss/10000:.0f}万円（{len(shortage_products)}商品）\")\n",
    "print(f\"   過剰在庫コスト: {total_excess_cost/10000:.0f}万円（{len(excess_products)}商品）\")\n",
    "print(f\"   合計改善機会: {(total_shortage_loss + total_excess_cost)/10000:.0f}万円\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f73e157-177d-4c3a-a61b-5d836e4d50e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # 9. 動的な発注最適化提案（GPU対応）\n",
    "\n",
    "# %%\n",
    "# 商品クラスタリング（発注戦略用）\n",
    "print(\"📊 発注最適化クラスタリング中...\")\n",
    "\n",
    "# 特徴量の準備\n",
    "cluster_features = efficiency_data[['販売率', '在庫回転日数', '売上貢献度', '効率スコア']].copy()\n",
    "\n",
    "# 欠損値処理\n",
    "cluster_features = cluster_features.fillna(cluster_features.median())\n",
    "\n",
    "# GPU対応のクラスタリング（cuMLが利用可能な場合）\n",
    "if GPU_AVAILABLE:\n",
    "    try:\n",
    "        from cuml.preprocessing import StandardScaler as cuStandardScaler\n",
    "        from cuml.cluster import KMeans as cuKMeans\n",
    "        \n",
    "        # 標準化\n",
    "        scaler = cuStandardScaler()\n",
    "        features_scaled = scaler.fit_transform(cluster_features)\n",
    "        \n",
    "        # K-meansクラスタリング（GPU）\n",
    "        n_clusters = 5\n",
    "        kmeans = cuKMeans(n_clusters=n_clusters, random_state=42)\n",
    "        efficiency_data['発注クラスター'] = kmeans.fit_predict(features_scaled)\n",
    "        \n",
    "        print(\"✅ GPU版K-meansクラスタリング完了\")\n",
    "    except:\n",
    "        # CPU版にフォールバック\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.cluster import KMeans\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        features_scaled = scaler.fit_transform(cluster_features)\n",
    "        \n",
    "        n_clusters = 5\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        efficiency_data['発注クラスター'] = kmeans.fit_predict(features_scaled)\n",
    "        \n",
    "        print(\"⚠️ CPU版K-meansクラスタリング完了\")\n",
    "else:\n",
    "    # CPU版を使用\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(cluster_features)\n",
    "    \n",
    "    n_clusters = 5\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    efficiency_data['発注クラスター'] = kmeans.fit_predict(features_scaled)\n",
    "\n",
    "# クラスター別特性分析\n",
    "cluster_profiles = efficiency_data.groupby('発注クラスター').agg({\n",
    "    '売上金額': ['mean', 'sum'],\n",
    "    '販売率': 'mean',\n",
    "    '在庫回転日数': 'mean',\n",
    "    '商品名': 'count'\n",
    "}).round(2)\n",
    "\n",
    "# %%\n",
    "# クラスターごとの発注戦略を動的に決定\n",
    "ordering_strategies = {}\n",
    "\n",
    "for cluster in range(n_clusters):\n",
    "    cluster_data = cluster_profiles.loc[cluster]\n",
    "    \n",
    "    # 各指標を取得\n",
    "    avg_sales = cluster_data[('売上金額', 'mean')]\n",
    "    avg_rate = cluster_data[('販売率', 'mean')]\n",
    "    avg_turnover = cluster_data[('在庫回転日数', 'mean')]\n",
    "    item_count = cluster_data[('商品名', 'count')]\n",
    "    \n",
    "    # 動的に戦略を決定\n",
    "    if avg_rate > 0.8 and avg_turnover < 3:\n",
    "        strategy = \"発注量20-30%増加\"\n",
    "        reason = \"高回転・高販売率\"\n",
    "        priority = \"高\"\n",
    "    elif avg_rate > 0.6 and avg_sales > cluster_profiles[('売上金額', 'mean')].median():\n",
    "        strategy = \"現状維持・微調整\"\n",
    "        reason = \"安定的なパフォーマンス\"\n",
    "        priority = \"中\"\n",
    "    elif avg_rate < 0.3:\n",
    "        strategy = \"発注量50%削減または廃止\"\n",
    "        reason = \"低販売率\"\n",
    "        priority = \"高\"\n",
    "    elif avg_turnover > 7:\n",
    "        strategy = \"発注量30%削減\"\n",
    "        reason = \"在庫滞留\"\n",
    "        priority = \"高\"\n",
    "    else:\n",
    "        strategy = \"販促強化後に再評価\"\n",
    "        reason = \"改善余地あり\"\n",
    "        priority = \"中\"\n",
    "    \n",
    "    ordering_strategies[f'クラスター{cluster}'] = {\n",
    "        '商品数': int(item_count),\n",
    "        '平均販売率': float(avg_rate),\n",
    "        '平均回転日数': float(avg_turnover),\n",
    "        '戦略': strategy,\n",
    "        '理由': reason,\n",
    "        '優先度': priority\n",
    "    }\n",
    "\n",
    "# 戦略の表示\n",
    "print(\"📊 動的発注戦略（クラスター分析に基づく）:\")\n",
    "for cluster, strategy in ordering_strategies.items():\n",
    "    print(f\"\\n{cluster}（{strategy['商品数']}商品）:\")\n",
    "    print(f\"  販売率: {strategy['平均販売率']:.1%}\")\n",
    "    print(f\"  回転日数: {strategy['平均回転日数']:.1f}日\")\n",
    "    print(f\"  戦略: {strategy['戦略']}\")\n",
    "    print(f\"  理由: {strategy['理由']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78f795e-182b-473c-9863-374338d868be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # 10. 統合ダッシュボード（GPU最適化）\n",
    "\n",
    "# %%\n",
    "# 統合KPIダッシュボード作成\n",
    "print(\"📊 統合ダッシュボード生成中...\")\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. 店舗別KPI\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "store_kpi_data = store_stats[['店舗', '日販', '販売率_mean']]\n",
    "x = np.arange(len(store_kpi_data))\n",
    "width = 0.35\n",
    "\n",
    "ax1_twin = ax1.twinx()\n",
    "bars1 = ax1.bar(x, store_kpi_data['日販'] / 10000, width, label='日販（万円）', color=COLORS[0])\n",
    "line1 = ax1_twin.plot(x, store_kpi_data['販売率_mean'] * 100, 'o-', \n",
    "                      color=COLORS[1], linewidth=2, markersize=10, label='販売率（%）')\n",
    "\n",
    "ax1.set_xlabel('店舗', fontproperties=JP_FP)\n",
    "ax1.set_ylabel('日販（万円）', color=COLORS[0], fontproperties=JP_FP)\n",
    "ax1_twin.set_ylabel('販売率（%）', color=COLORS[1], fontproperties=JP_FP)\n",
    "ax1.set_title('店舗別KPI', fontsize=16, fontproperties=JP_FP)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(store_kpi_data['店舗'])\n",
    "\n",
    "# 2. 機会損失分析\n",
    "ax2 = fig.add_subplot(gs[0, 2:])\n",
    "loss_values = list(loss_data.values())\n",
    "loss_labels = [f'{k}\\n{v:.0f}万円' for k, v in loss_data.items()]\n",
    "bars2 = ax2.bar(range(len(loss_values)), loss_values, color=['#E74C3C', '#F39C12'])\n",
    "ax2.set_ylabel('金額（万円）', fontproperties=JP_FP)\n",
    "ax2.set_title(f'改善機会: 合計{sum(loss_values, fontproperties=JP_FP):.0f}万円', fontsize=16)\n",
    "ax2.set_xticks(range(len(loss_values)))\n",
    "ax2.set_xticklabels(loss_labels)\n",
    "\n",
    "# 3. 商品効率マトリックス（簡略版）\n",
    "ax3 = fig.add_subplot(gs[1, :2])\n",
    "for category, color in colors_map.items():\n",
    "    data = efficiency_data[efficiency_data['商品分類'] == category]\n",
    "    if len(data) > 0:\n",
    "        ax3.scatter(data['販売率'].head(50) * 100, \n",
    "                   data['売上金額'].head(50) / 10000,\n",
    "                   s=30, alpha=0.6, c=color, label=category)\n",
    "\n",
    "ax3.set_xlabel('販売率（%）', fontproperties=JP_FP)\n",
    "ax3.set_ylabel('売上金額（万円）', fontproperties=JP_FP)\n",
    "ax3.set_title('商品分類マトリックス', fontsize=16, fontproperties=JP_FP)\n",
    "ax3.legend(loc='best', fontsize=8, prop=JP_FP)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. 曜日パターン\n",
    "ax4 = fig.add_subplot(gs[1, 2:])\n",
    "weekly_values = [weekday_insights['pattern'][day] for day in weekday_order]\n",
    "bars4 = ax4.bar(weekday_order, weekly_values, color=COLORS[2])\n",
    "\n",
    "# ピークと谷を強調\n",
    "for i, day in enumerate(weekday_order):\n",
    "    if day == weekday_insights['peak_day']:\n",
    "        bars4[i].set_color('#2ECC71')\n",
    "    elif day == weekday_insights['low_day']:\n",
    "        bars4[i].set_color('#E74C3C')\n",
    "\n",
    "ax4.set_xlabel('曜日', fontproperties=JP_FP)\n",
    "ax4.set_ylabel('平均売上', fontproperties=JP_FP)\n",
    "ax4.set_title('曜日別売上パターン', fontsize=16, fontproperties=JP_FP)\n",
    "ax4.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x/10000):.0f}万'))\n",
    "\n",
    "# 5. 動的推奨事項\n",
    "ax5 = fig.add_subplot(gs[2, :])\n",
    "ax5.axis('off')\n",
    "\n",
    "# 推奨事項テキスト\n",
    "recommendations_text = \"【データ分析に基づく動的推奨事項】\\n\\n\"\n",
    "\n",
    "# 優先度の高い施策を抽出\n",
    "high_priority_actions = []\n",
    "\n",
    "# 1. 品薄対策\n",
    "if total_shortage_loss > 100000:\n",
    "    high_priority_actions.append({\n",
    "        'action': f'{len(shortage_products)}商品の発注量を20-30%増加',\n",
    "        'impact': f'{total_shortage_loss/10000:.0f}万円の機会損失削減',\n",
    "        'priority': '高'\n",
    "    })\n",
    "\n",
    "# 2. 過剰在庫対策\n",
    "if total_excess_cost > 50000:\n",
    "    high_priority_actions.append({\n",
    "        'action': f'{len(excess_products)}商品の発注量を段階的に削減',\n",
    "        'impact': f'{total_excess_cost/10000:.0f}万円のコスト削減',\n",
    "        'priority': '高'\n",
    "    })\n",
    "\n",
    "# 3. 曜日対策\n",
    "if weekday_insights['peak_to_low_ratio'] > 1.5:\n",
    "    high_priority_actions.append({\n",
    "        'action': f\"{weekday_insights['peak_day']}の発注を{(weekday_insights['peak_to_low_ratio']-1)*100:.0f}%増加\",\n",
    "        'impact': '売上機会の最大化',\n",
    "        'priority': '中'\n",
    "    })\n",
    "\n",
    "# 4. トレンド対策\n",
    "if trend_insights['direction'] == 'decreasing' and trend_insights['statistical_significance']:\n",
    "    high_priority_actions.append({\n",
    "        'action': '売上減少トレンドへの対策（品揃え・価格見直し）',\n",
    "        'impact': f'日次{abs(trend_insights[\"daily_change\"]):.0f}円の減少を食い止める',\n",
    "        'priority': '高'\n",
    "    })\n",
    "\n",
    "# テキスト生成\n",
    "for i, action in enumerate(high_priority_actions[:5], 1):\n",
    "    recommendations_text += f\"{i}. {action['action']}\\n\"\n",
    "    recommendations_text += f\"   期待効果: {action['impact']}\\n\"\n",
    "    recommendations_text += f\"   優先度: {action['priority']}\\n\\n\"\n",
    "\n",
    "ax5.text(0.05, 0.95, recommendations_text, transform=ax5.transAxes,\n",
    "         fontsize=12, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3, fontproperties=JP_FP))\n",
    "\n",
    "plt.suptitle('動的ビジネスインサイト統合ダッシュボード（GPU加速）', fontsize=20, fontproperties=JP_FP)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# GPUメモリ使用状況の表示\n",
    "if GPU_AVAILABLE:\n",
    "    try:\n",
    "        info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "        print(f\"\\n📊 GPU メモリ使用状況（処理後）: {info.used/1e9:.1f}/{info.total/1e9:.1f} GB\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a70725-0693-4693-8d92-f2dd3d5e20da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # 11. LLM用構造化データ出力\n",
    "\n",
    "# %%\n",
    "# 全ての分析結果を統合\n",
    "print(\"📊 分析結果を構造化データに変換中...\")\n",
    "\n",
    "integrated_insights = {\n",
    "    \"analysis_metadata\": {\n",
    "        \"analysis_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"gpu_enabled\": GPU_AVAILABLE,\n",
    "        \"data_period\": {\n",
    "            \"start\": df['日付'].min().strftime(\"%Y-%m-%d\"),\n",
    "            \"end\": df['日付'].max().strftime(\"%Y-%m-%d\"),\n",
    "            \"days\": int((df['日付'].max() - df['日付'].min()).days) + 1\n",
    "        },\n",
    "        \"data_scope\": {\n",
    "            \"total_records\": int(len(df)),\n",
    "            \"stores\": int(df['店舗'].nunique()),\n",
    "            \"products\": int(df['商品名'].nunique()),\n",
    "            \"categories\": int(df['フェイスくくり大分類'].nunique())\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"key_metrics\": {\n",
    "        \"store_performance\": store_stats[['店舗', '日販', '販売率_mean']].to_dict('records'),\n",
    "        \"total_daily_sales\": float(store_stats['日販'].sum()),\n",
    "        \"average_sales_rate\": float(df['販売率'].mean()),\n",
    "        \"opportunity_analysis\": {\n",
    "            \"shortage_loss\": float(total_shortage_loss),\n",
    "            \"excess_cost\": float(total_excess_cost),\n",
    "            \"total_opportunity\": float(total_shortage_loss + total_excess_cost),\n",
    "            \"shortage_products_count\": int(len(shortage_products)),\n",
    "            \"excess_products_count\": int(len(excess_products))\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"patterns_discovered\": {\n",
    "        \"weekly_pattern\": weekday_insights,\n",
    "        \"time_trend\": trend_insights,\n",
    "        \"weather_impact\": {\n",
    "            \"temperature_correlation\": float(corr_coef),\n",
    "            \"best_weather\": weather_avg.index[0],\n",
    "            \"weather_sales_impact\": float(weather_avg.iloc[0] / weather_avg.iloc[-1])\n",
    "        },\n",
    "        \"temperature_sensitive_products\": temp_sensitive_products\n",
    "    },\n",
    "    \n",
    "    \"product_insights\": {\n",
    "        \"classification_summary\": classification_stats.to_dict(),\n",
    "        \"reduction_candidates\": {\n",
    "            \"count\": int(total_reduction_items),\n",
    "            \"impact_sales\": float(total_reduction_sales),\n",
    "            \"percentage_of_total\": float(total_reduction_sales/efficiency_data['売上金額'].sum()*100)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"category_insights\": {\n",
    "        \"top_categories\": category_performance.head(5)['売上金額'].to_dict(),\n",
    "        \"pb_nb_comparison\": pb_nb_performance.to_dict()\n",
    "    },\n",
    "    \n",
    "    \"ordering_strategies\": ordering_strategies,\n",
    "    \n",
    "    \"priority_actions\": [\n",
    "        {\n",
    "            \"action\": action['action'],\n",
    "            \"expected_impact\": action['impact'],\n",
    "            \"priority\": action['priority']\n",
    "        }\n",
    "        for action in high_priority_actions\n",
    "    ],\n",
    "    \n",
    "    \"processing_info\": {\n",
    "        \"gpu_used\": GPU_AVAILABLE,\n",
    "        \"processing_time\": \"最適化済み（GPU加速）\" if GPU_AVAILABLE else \"標準処理\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# JSON形式で保存（オプション）\n",
    "output_path = Path('analysis_results_gpu.json')\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(integrated_insights, f, ensure_ascii=False, indent=2, default=str)\n",
    "\n",
    "print(\"✅ 分析結果をJSON形式で保存しました\")\n",
    "print(f\"   ファイル: {output_path}\")\n",
    "\n",
    "# %%\n",
    "# 分析サマリーの表示\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 分析結果サマリー（LLM解釈用）\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. 基本情報:\")\n",
    "print(f\"   分析期間: {integrated_insights['analysis_metadata']['data_period']['days']}日間\")\n",
    "print(f\"   総売上: {integrated_insights['key_metrics']['total_daily_sales']/10000:.0f}万円/日\")\n",
    "print(f\"   平均販売率: {integrated_insights['key_metrics']['average_sales_rate']:.1%}\")\n",
    "print(f\"   GPU使用: {'✅ 有効' if GPU_AVAILABLE else '❌ 無効'}\")\n",
    "\n",
    "print(f\"\\n2. 改善機会:\")\n",
    "print(f\"   合計: {integrated_insights['key_metrics']['opportunity_analysis']['total_opportunity']/10000:.0f}万円\")\n",
    "print(f\"   品薄商品: {integrated_insights['key_metrics']['opportunity_analysis']['shortage_products_count']}品\")\n",
    "print(f\"   過剰在庫: {integrated_insights['key_metrics']['opportunity_analysis']['excess_products_count']}品\")\n",
    "\n",
    "print(f\"\\n3. 発見されたパターン:\")\n",
    "print(f\"   曜日ピーク: {integrated_insights['patterns_discovered']['weekly_pattern']['peak_day']}\")\n",
    "print(f\"   売上トレンド: {integrated_insights['patterns_discovered']['time_trend']['direction']}\")\n",
    "print(f\"   気温相関: {integrated_insights['patterns_discovered']['weather_impact']['temperature_correlation']:.3f}\")\n",
    "\n",
    "print(f\"\\n4. 優先アクション数: {len(integrated_insights['priority_actions'])}\")\n",
    "\n",
    "print(\"\\n※このデータをLLMに入力することで、さらに詳細な分析と提案が可能です\")\n",
    "\n",
    "# %%\n",
    "# 分析完了メッセージ\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ 動的探索的データ分析が完了しました！（GPU最適化版）\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n処理の特徴:\")\n",
    "print(\"- cudf.pandasによる透過的なGPU加速\")\n",
    "print(\"- 大規模データの高速集計・分析\")\n",
    "print(\"- 日本語対応の可視化\")\n",
    "print(\"- LLM向け構造化データ出力\")\n",
    "print(\"\\n次のステップ:\")\n",
    "print(\"1. analysis_results_gpu.jsonをLLMに入力して詳細な解釈を得る\")\n",
    "print(\"2. 予測モデルの構築に進む\")\n",
    "print(\"3. 特定の商品や店舗の深堀り分析を実施\")\n",
    "print(\"\\nGPU環境により、より大規模なデータでも高速な分析が可能です！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16326c-2171-4ced-bb83-6481e6dbd731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
