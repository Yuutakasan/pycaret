{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ğŸª åº—èˆ—åˆ¥åŒ…æ‹¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ v6.1 â€” è¤‡æ•°åº—èˆ—é¸æŠãƒ»ç‰¹å¾´é‡ææ¡ˆå¼·åŒ–\n",
    "\n",
    "- æŒ‡å®šCSVã‚’èª­ã¿è¾¼ã¿ã€å£²ä¸Šã«åŠ¹ãç‰¹å¾´é‡ã‚’ç‰¹å®šã—ã€åº—èˆ—é¸æŠã«å¿œã˜ãŸææ¡ˆã‚’è‡ªå‹•ç”Ÿæˆ\n",
    "- åœ¨åº«(on_hand)/åŸä¾¡(cost)/æ£šå®¹é‡ã¯æœªæä¾› â†’ ä¾å­˜åˆ†æã¯é™¤å¤–\n",
    "- ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã¯ KPI / ABC / ç‰¹å¾´é‡ / ææ¡ˆ / ã‚¢ãƒ©ãƒ¼ãƒˆ ã‚’åˆ‡æ›¿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## åº—é•·ã®ä½¿ã„æ–¹ï¼ˆæ„æ€æ±ºå®šã‚¬ã‚¤ãƒ‰)\n",
    "ã“ã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã¯ã€è¦‹ã‚‹â†’åˆ¤æ–­â†’å³ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€ã®é †ã«ä½œã‚‰ã‚Œã¦ã„ã¾ã™ã€‚æ¯æ—¥5ã€œ10åˆ†ã§ä¸‹è¨˜ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "### 1) KPIï¼ˆå…¨ä½“å‚¾å‘ã®æŠŠæ¡ï¼š1åˆ†ï¼‰\n",
    "- ãƒã‚§ãƒƒã‚¯: å£²ä¸Šãƒ»å®¢æ•°ãƒ»å®¢å˜ä¾¡ã®æ¨ç§»ã€‚æ˜¨æ—¥/ä¸€æ˜¨æ—¥ã¨æ¯”ã¹ã¦å¤§ããä¸Šä¸‹ã—ã¦ã„ãªã„ã‹ã€‚\n",
    "- è¡Œå‹•: ä¸‹æŒ¯ã‚Œæ™‚ã¯æ¬¡ã®ã€ã‚¢ãƒ©ãƒ¼ãƒˆã€ã€ææ¡ˆã€ã§åŸå› ã¨æ‰“ã¡æ‰‹ã‚’ç¢ºèªã—ã€è©²å½“ã‚«ãƒ†ã‚´ãƒªã®é¢å‡ºã—/ãƒ•ã‚§ãƒ¼ã‚¹å¢—ã‚„å‰å‡ºã—æ™‚é–“ã®å‰å€’ã—ã‚’å®Ÿæ–½ã€‚\n",
    "\n",
    "### 2) ã‚¢ãƒ©ãƒ¼ãƒˆï¼ˆæ€¥å¤‰ã®æ—©æœŸå¯¾å¿œï¼š1ã€œ2åˆ†ï¼‰\n",
    "- å®šç¾©: å£²ä¸Šã®3æ—¥ç§»å‹•å¹³å‡ã¨7æ—¥ç§»å‹•å¹³å‡ã®ä¹–é›¢ãŒÂ±30%ã‚’è¶…ãˆãŸåº—èˆ—æ—¥ã€‚\n",
    "- è¡Œå‹•: ä¸ŠæŒ¯ã‚Œâ†’è©²å½“ã‚«ãƒ†ã‚´ãƒªã®ãƒ•ã‚§ãƒ¼ã‚¹/åœ¨åº«é¢ï¼ˆå‰å‡ºã—ï¼‰ã‚’å¢—ã‚„ã™ã€‚ä¸‹æŒ¯ã‚Œâ†’é™³åˆ—å ´æ‰€ã‚„å°ç·šã®è¦‹ç›´ã—ãƒ»å€¤é ƒè¨´æ±‚ï¼ˆã®ã¡ã®è¦‹åˆ‡ã‚ŠåŸºæº–ã‚’èª¿æ•´ï¼‰ã€‚\n",
    "\n",
    "### 3) ææ¡ˆï¼ˆä»Šæ—¥ã®æ‰“ã¡æ‰‹ï¼š2ã€œ3åˆ†ï¼‰\n",
    "- ãƒ­ã‚¸ãƒƒã‚¯: å¤©å€™ãƒ»æ›œæ—¥ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆç­‰ã®ãƒ•ãƒ©ã‚°ã”ã¨ã«ã€å£²ä¸ŠãŒä¸ŠæŒ¯ã‚Œã—ã‚„ã™ã„ã‚«ãƒ†ã‚´ãƒªã‚’æç¤ºã€‚\n",
    "- è¡Œå‹•: ä¸Šä½ã‚«ãƒ†ã‚´ãƒªã«å¯¾ã—ã¦ã€æ™‚é–“å¸¯åˆ¥ã®å‰å‡ºã—/ãƒ•ã‚§ãƒ¼ã‚¹å¢—/å°ç·šå¼·åŒ–ã‚’å®Ÿè¡Œã€‚ä¾‹:\n",
    "  - é™é›¨ãƒ•ãƒ©ã‚°: æ¸©ã‹ã„ç·èœ/ã‚«ãƒƒãƒ—éºº/ãƒ›ãƒƒãƒˆé£²æ–™/ä¸­è¯ã¾ã‚“ï¼ˆå…¥å£å´/ãƒ¬ã‚¸æ¨ªã®å°ç·šå¼·åŒ–ï¼‰\n",
    "  - çŒ›æš‘æ—¥/çœŸå¤æ—¥: å†·é£²æ–™/ã‚¢ã‚¤ã‚¹/ãƒãƒ«ãƒ‰ãƒ‡ã‚¶ãƒ¼ãƒˆï¼ˆå†·ã‚±ãƒ¼ã‚¹ã®ãƒ•ã‚§ãƒ¼ã‚¹å¢—ã€æ°·/ä¿å†·å“ã®è¨´æ±‚ï¼‰\n",
    "  - é€±æœ«: å¼å½“/éºº/ãƒ‡ã‚¶ãƒ¼ãƒˆï¼ˆæ˜¼ãƒ»å¤•ãƒ”ãƒ¼ã‚¯å‰ã®å‰å‡ºã—å¼·åŒ–ï¼‰\n",
    "  - çµ¦æ–™æ—¥/çµ¦æ–™æ—¥ç›´å¾Œ: é«˜å˜ä¾¡å¼å½“/ã‚¹ã‚¤ãƒ¼ãƒ„ï¼ˆå¤•æ–¹æ‰‹å‰ã§ã®åšã‚é™³åˆ—ï¼‰\n",
    "  - æœˆåˆ3æ—¥: ä¸»åŠ›å®šç•ªã®é¢ç¢ºä¿ï¼ˆç™ºæ³¨å¼·ã‚ã®åˆ¤æ–­ææ–™ã«ï¼‰\n",
    "  - æœˆæœ«3æ—¥: å€¤é ƒå“/ç¯€ç´„è¨´æ±‚ï¼ˆä¾¡æ ¼ãƒãƒƒãƒ—/ã‚¯ãƒ­ã‚¹MDï¼‰\n",
    "\n",
    "### 4) ABCï¼ˆæ£šã®é…åˆ†èª¿æ•´ï¼š2åˆ†ï¼‰\n",
    "- A: å£²ä¸Š80%ã‚’ä½œã‚‹ä¸»åŠ›ã€‚ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æœ€å„ªå…ˆã§ç¢ºä¿ã—ã€æ¬ å“ã•ã›ãªã„ï¼ˆå‰å‡ºã—æ™‚é–“ã‚’å‰å€’ã—ï¼‰ã€‚\n",
    "- B: è£œå®Œã€‚ãƒ”ãƒ¼ã‚¯å‰è£œå……ã¨è¦‹ã›æ–¹ã§å·®ã‚’å‡ºã™ã€‚\n",
    "- C: åœ§ç¸®/å…¥æ›¿å€™è£œã€‚å£²å ´ã‚¹ãƒšãƒ¼ã‚¹ã‚’A/Bã¸å¯„ã›ã‚‹ã€‚\n",
    "\n",
    "### æ—¥æ¬¡ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼ˆä¾‹ï¼‰\n",
    "- [ ] KPIã§ä¸‹æŒ¯ã‚Œã¯ãªã„ã‹â†’ã‚ã‚‹å ´åˆã¯ã€ã‚¢ãƒ©ãƒ¼ãƒˆã€ã€ææ¡ˆã€ã‚’å„ªå…ˆç¢ºèª\n",
    "- [ ] ä»Šæ—¥ã®ææ¡ˆã‚­ãƒ¼ï¼ˆé™é›¨/çŒ›æš‘/é€±æœ«/çµ¦æ–™æ—¥ãªã©ï¼‰ã¨ä¸Šä½ã‚«ãƒ†ã‚´ãƒªã‚’ãƒ¡ãƒ¢\n",
    "- [ ] é–‹åº—å‰: ä¸Šä½ã‚«ãƒ†ã‚´ãƒªã®é¢å‡ºã—/ãƒ•ã‚§ãƒ¼ã‚¹å¢—ï¼ˆå…¥å£/ãƒ¬ã‚¸æ¨ª/ç«¯å°ç·šï¼‰\n",
    "- [ ] æ˜¼ãƒ”ãƒ¼ã‚¯å‰ï¼ˆã€œ11æ™‚ï¼‰: è£œå……/å‰å‡ºã—å¼·åŒ–\n",
    "- [ ] å¤•ãƒ”ãƒ¼ã‚¯å‰ï¼ˆã€œ17æ™‚ï¼‰: è£œå……/å‰å‡ºã—å¼·åŒ–\n",
    "- [ ] é–‰åº—å‰: å€¤é ƒè¨´æ±‚/è¦‹åˆ‡ã‚Šï¼ˆåœ¨åº«é€£å‹•ã¯æœªå®Ÿè£…ã®ãŸã‚åˆ¤æ–­åŸºæº–ã‚’åº—èˆ—é‹ç”¨ã§ï¼‰\n",
    "\n",
    "### è£œè¶³\n",
    "- æœ¬ã‚·ãƒ¼ãƒˆã¯åœ¨åº«ãƒ»åŸä¾¡ãªã—ã®å‰æã§ã™ã€‚æ¬ å“/å»ƒæ£„/ç²—åˆ©ã®æœ€é©åŒ–ã¯åˆ¥ãƒãƒ¼ãƒˆï¼ˆåœ¨åº«é€£å‹•ï¼‰ã¨é€£æºã‚’æ¤œè¨ã€‚\n",
    "- ç‰¹å¾´é‡ï¼ˆè¦å› ï¼‰ã®ä¸Šä½ã‚’ã€æ˜æ—¥ã€ã®æ‰“ã¡æ‰‹ï¼ˆé™³åˆ—/è²©ä¿ƒ/å°ç·šï¼‰ã¸åæ˜ ã—ã€ç¿Œæ—¥ã®KPIæ”¹å–„ã«ç¹‹ã’ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "import font_setup  # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ/Plotlyãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ\n",
    "\n",
    "try:\n",
    "    import plotly.express as px; import plotly.graph_objects as go\n",
    "    PLOTLY=True\n",
    "except Exception:\n",
    "    PLOTLY=False\n",
    "try:\n",
    "    import ipywidgets as widgets; from IPython.display import display, clear_output\n",
    "    WIDGETS=True\n",
    "except Exception:\n",
    "    WIDGETS=False\n",
    "try:\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    SK_OK=True\n",
    "except Exception:\n",
    "    SK_OK=False\n",
    "\n",
    "# ========================================\n",
    "# ğŸš€ GPUæ¤œå‡ºã¨è¨­å®š\n",
    "# ========================================\n",
    "print('='*60)\n",
    "print('ğŸ–¥ï¸ GPUæ¤œå‡º')\n",
    "print('='*60)\n",
    "\n",
    "GPU_AVAILABLE = False\n",
    "GPU_DEVICE = 'cpu'\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        GPU_AVAILABLE = True\n",
    "        GPU_DEVICE = 'cuda'\n",
    "        print(f'âœ… NVIDIA GPUæ¤œå‡º: {torch.cuda.get_device_name(0)}')\n",
    "        print(f'   CUDA Version: {torch.version.cuda}')\n",
    "        print(f'   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')\n",
    "    else:\n",
    "        print('âš ï¸ PyTorch installed but no CUDA GPU found')\n",
    "except ImportError:\n",
    "    print('â„¹ï¸ PyTorch not installed (GPU detection skipped)')\n",
    "\n",
    "\n",
    "# cuDF (GPU-accelerated Pandas) ãƒã‚§ãƒƒã‚¯\n",
    "CUDF_AVAILABLE = False\n",
    "try:\n",
    "    import cudf\n",
    "    if GPU_AVAILABLE:\n",
    "        CUDF_AVAILABLE = True\n",
    "        print('âœ… cuDF (GPU Pandas) å¯¾å¿œ: å¯èƒ½')\n",
    "        print('   ğŸ’¡ å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãŒ10ï½50å€é«˜é€ŸåŒ–ã•ã‚Œã¾ã™')\n",
    "    else:\n",
    "        print('â„¹ï¸ cuDF: GPUæœªæ¤œå‡ºã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—')\n",
    "except ImportError:\n",
    "    print('â„¹ï¸ cuDF not installed (pip install cudf-cu12)')\n",
    "\n",
    "# GPUä½¿ç”¨ãƒ•ãƒ©ã‚°\n",
    "USE_GPU = GPU_AVAILABLE\n",
    "\n",
    "if USE_GPU:\n",
    "    print(f'ğŸš€ GPUä½¿ç”¨: æœ‰åŠ¹ï¼ˆãƒ‡ãƒ¼ã‚¿å‡¦ç†é«˜é€ŸåŒ–ï¼‰')\n",
    "    print(f'   Device: {GPU_DEVICE}')\n",
    "else:\n",
    "    print(f'\\nâ„¹ï¸ GPUä½¿ç”¨: ç„¡åŠ¹ï¼ˆCPUãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œï¼‰')\n",
    "\n",
    "print('='*60)\n",
    "\n",
    "print(f\"\\nEnv: pandas={pd.__version__}, plotly={'OK' if PLOTLY else 'N/A'}, widgets={'OK' if WIDGETS else 'N/A'}, sklearn={'OK' if SK_OK else 'N/A'}, GPU={'YES' if GPU_AVAILABLE else 'NO'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ğŸ”§ GPU/CPU ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å¤‰æ›ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£\n",
    "# ========================================\n",
    "\n",
    "def to_gpu(df):\n",
    "    \"\"\"pandasãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’GPU (cuDF) ã«å¤‰æ›ï¼ˆUSE_GPU=Trueã®å ´åˆã®ã¿ï¼‰\"\"\"\n",
    "    if USE_GPU and CUDF_AVAILABLE and df is not None and not df.empty:\n",
    "        try:\n",
    "            import cudf\n",
    "            return cudf.from_pandas(df)\n",
    "        except Exception as e:\n",
    "            print(f'âš ï¸ GPUå¤‰æ›å¤±æ•—ã€CPUãƒ¢ãƒ¼ãƒ‰ç¶™ç¶š: {e}')\n",
    "            return df\n",
    "    return df\n",
    "\n",
    "def to_cpu(df):\n",
    "    \"\"\"cuDFãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’pandasã«å¤‰æ›\"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "    try:\n",
    "        import cudf\n",
    "        if isinstance(df, cudf.DataFrame):\n",
    "            return df.to_pandas()\n",
    "    except:\n",
    "        pass\n",
    "    return df\n",
    "\n",
    "# GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³è¡¨ç¤º\n",
    "def show_gpu_memory():\n",
    "    if GPU_AVAILABLE:\n",
    "        try:\n",
    "            import torch\n",
    "            allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "            reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "            print(f'ğŸ“Š GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if USE_GPU and CUDF_AVAILABLE:\n",
    "    print('âœ… GPUé«˜é€ŸåŒ–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£: æº–å‚™å®Œäº†')\n",
    "    print('   to_gpu(df) ã§GPUå‡¦ç†ã€to_cpu(df) ã§CPUæˆ»ã—')\n",
    "    show_gpu_memory()\n",
    "else:\n",
    "    print('â„¹ï¸ CPUå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ï¼ˆGPUãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã¯ç„¡åŠ¹ï¼‰')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. æŒ‡å®šCSVèª­ã¿è¾¼ã¿ã¨æ•´å½¢\n",
    "- ç”Ÿæˆ: `sales`(å¿…é ˆ), `product`(SKUãƒã‚¹ã‚¿), `df`(è§£æç”¨ãƒ™ãƒ¼ã‚¹), `weather`(ä»£è¡¨åˆ—)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆç’°å¢ƒå¤‰æ•°å„ªå…ˆâ†’æ—¢å®šãƒ‘ã‚¹ï¼‰\n",
    "import os\n",
    "cand_env = [os.environ.get('DATA_PATH'), os.environ.get('ENRICHED_CSV')]\n",
    "DATA_PATH = None\n",
    "for p in cand_env:\n",
    "    if p and Path(p).exists():\n",
    "        DATA_PATH = Path(p); break\n",
    "if DATA_PATH is None:\n",
    "    DATA_PATH = Path('output/06_final_enriched_20250701_20250930.csv')\n",
    "\n",
    "raw = pd.read_csv(DATA_PATH, encoding='utf-8-sig')\n",
    "df = raw.copy()\n",
    "# åˆ—åæ­£è¦åŒ–ï¼ˆå­˜åœ¨ã™ã‚Œã°ï¼‰\n",
    "rename_map = {\n",
    "    'åº—èˆ—':'store_id','å•†å“å':'sku_id','æ—¥ä»˜':'date','å£²ä¸Šæ•°é‡':'qty','å£²ä¸Šé‡‘é¡':'sales_amt',\n",
    "    'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šå¤§åˆ†é¡':'category_l','ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šä¸­åˆ†é¡':'category_m','ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šå°åˆ†é¡':'category_s'\n",
    "}\n",
    "df = df.rename(columns={k:v for k,v in rename_map.items() if k in df.columns})\n",
    "if 'date' in df.columns: df['date'] = pd.to_datetime(df['date'])\n",
    "if 'qty' in df.columns: df['qty'] = pd.to_numeric(df['qty'], errors='coerce').fillna(0)\n",
    "if 'sales_amt' in df.columns: df['sales_amt'] = pd.to_numeric(df['sales_amt'], errors='coerce').fillna(0)\n",
    "if {'sales_amt','qty'}.issubset(df.columns): df['price'] = np.where(df['qty']>0, df['sales_amt']/df['qty'], np.nan)\n",
    "\n",
    "sales = df[[c for c in ['date','store_id','sku_id','qty','price','sales_amt','category_l','category_m','category_s'] if c in df.columns]].copy()\n",
    "product = df[[c for c in ['sku_id','category_l','category_m','category_s'] if c in df.columns]].drop_duplicates().copy() if 'sku_id' in df.columns else pd.DataFrame()\n",
    "\n",
    "# weatherï¼ˆä»£è¡¨åˆ—ãŒã‚ã‚Œã°é›†ç´„ï¼‰\n",
    "wcols = [c for c in ['å¤©æ°—','æœ€é«˜æ°—æ¸©','æœ€ä½æ°—æ¸©','é™æ°´é‡','å¹³å‡æ°—æ¸©','æ°—æ¸©å·®'] if c in df.columns]\n",
    "if wcols and {'date','store_id'}.issubset(df.columns):\n",
    "    wdf = df[['date','store_id']+wcols].copy()\n",
    "    num_cols = [c for c in wcols if c!='å¤©æ°—']\n",
    "    agg = {c:'mean' for c in num_cols}\n",
    "    if 'å¤©æ°—' in wcols:\n",
    "        # æœ€é »å€¤ã‚’ä»£è¡¨ã¨ã™ã‚‹\n",
    "        def _mode(x):\n",
    "            m = x.mode()\n",
    "            return m.iloc[0] if not m.empty else x.iloc[0]\n",
    "        agg['å¤©æ°—'] = _mode\n",
    "    weather = wdf.groupby(['date','store_id'], as_index=False).agg(agg)\n",
    "    if 'å¤©æ°—' in weather.columns: weather = weather.rename(columns={'å¤©æ°—':'weather'})\n",
    "else:\n",
    "    weather = pd.DataFrame()\n",
    "\n",
    "print('èª­ã¿è¾¼ã¿:', {'raw': raw.shape, 'sales': sales.shape, 'product': product.shape, 'weather': getattr(weather,'shape',None)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. KPI/ABC ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_daily_kpi(sales_df, prod_df):\n",
    "    if sales_df is None or sales_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    df0 = sales_df.copy()\n",
    "    if 'sales_amt' not in df0.columns:\n",
    "        if {'price','qty'}.issubset(df0.columns):\n",
    "            df0['sales_amt'] = df0['price'] * df0['qty']\n",
    "        else:\n",
    "            df0['sales_amt'] = 0\n",
    "    if not prod_df.empty and 'cost' in prod_df.columns:\n",
    "        df0 = df0.merge(prod_df[['sku_id','cost']], on='sku_id', how='left')\n",
    "        df0['gross_profit'] = df0['sales_amt'] - df0['cost'].fillna(0) * df0['qty'].fillna(0)\n",
    "    agg_map = dict(sales_amt=('sales_amt','sum'), qty=('qty','sum'))\n",
    "    if 'gross_profit' in df0.columns:\n",
    "        agg_map['gross_profit'] = ('gross_profit','sum')\n",
    "    gp = df0.groupby(['date','store_id'], as_index=False).agg(**agg_map)\n",
    "    return gp\n",
    "\n",
    "def abc_analysis(sales_df, prod_df, top_ratio=(0.8,0.95)):\n",
    "    if sales_df is None or sales_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    d = sales_df.copy()\n",
    "    if 'sales_amt' not in d.columns:\n",
    "        if {'price','qty'}.issubset(d.columns):\n",
    "            d['sales_amt'] = d['price'] * d['qty']\n",
    "        else:\n",
    "            d['sales_amt'] = 0\n",
    "    by = d.groupby('sku_id', as_index=False)['sales_amt'].sum().sort_values('sales_amt', ascending=False)\n",
    "    if by['sales_amt'].sum() == 0:\n",
    "        by['cum_ratio'] = 0\n",
    "    else:\n",
    "        by['cum_ratio'] = by['sales_amt'].cumsum()/by['sales_amt'].sum()\n",
    "    a_th, b_th = top_ratio\n",
    "    by['ABC'] = np.where(by['cum_ratio']<=a_th, 'A', np.where(by['cum_ratio']<=b_th, 'B','C'))\n",
    "    if not prod_df.empty:\n",
    "        by = by.merge(prod_df, on='sku_id', how='left')\n",
    "    return by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. åº—èˆ—é¸æŠï¼ˆè¤‡æ•°å¯ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åº—èˆ—é¸æŠã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆï¼ˆã“ã®æ™‚ç‚¹ã§ã¯è¡¨ç¤ºã®ã¿ã€é¸æŠã¯ cell-14 ã§è¡Œã†ï¼‰\n",
    "store_opts = sorted(sales['store_id'].dropna().unique().tolist()) if not sales.empty and 'store_id' in sales.columns else []\n",
    "if WIDGETS and store_opts:\n",
    "    print(f'âœ… åº—èˆ—é¸æŠã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã‚’æº–å‚™ã—ã¾ã—ãŸï¼ˆ{len(store_opts)}åº—èˆ—ï¼‰')\n",
    "    print('ğŸ‘‰ ä¸‹ã®ã€Œ6. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§åº—èˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„')\n",
    "else:\n",
    "    print('âš ï¸ widgetsç„¡åŠ¹ã€ã¾ãŸã¯åº—èˆ—ãƒ‡ãƒ¼ã‚¿ãªã—')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. éœ€è¦äºˆæ¸¬ï¼ˆPyCaretï¼‰\n",
    "- Exogenous: å¤©å€™/ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼/ãƒ—ãƒ­ãƒ¢ãªã©ï¼ˆæœ¬ã‚µãƒ³ãƒ—ãƒ«ã§ã¯æœªçµåˆï¼‰\n",
    "- SKUÃ—åº—èˆ—ã®å˜ä¸€ç³»åˆ—ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pycaret.time_series import setup as ts_setup, compare_models as ts_compare, finalize_model as ts_finalize, predict_model as ts_predict\n",
    "    PYCaret_TS=True\n",
    "except Exception as e:\n",
    "    PYCaret_TS=False\n",
    "    print(f'PyCaret TS ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}')\n",
    "\n",
    "# PyCaret TSã¯ cell-14 ã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§å®Ÿè¡Œï¼ˆåº—èˆ—é¸æŠå¾Œï¼‰\n",
    "if PYCaret_TS:\n",
    "    print('âœ… PyCaret TS åˆ©ç”¨å¯èƒ½ï¼ˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§å®Ÿè¡Œã•ã‚Œã¾ã™ï¼‰')\n",
    "elif not PYCaret_TS:\n",
    "    print('âš ï¸ PyCaret TS ç„¡åŠ¹ï¼ˆã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ï¼‰')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5. ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆå£²ä¸Šé‡‘é¡ï¼‰ã¨ææ¡ˆã®è‡ªå‹•ç”Ÿæˆ\n",
    "- é¸æŠåº—èˆ—ã®åˆç®—å£²ä¸Šï¼ˆå£²ä¸Šé‡‘é¡ï¼‰ã‚’ç›®çš„å¤‰æ•°ã«ã€æä¾›ç‰¹å¾´é‡ã‹ã‚‰é‡è¦åº¦ã‚’æ¨å®š\n",
    "- sklearnãŒç„¡ã„å ´åˆã¯ç›¸é–¢ä¿‚æ•°ã§ä»£æ›¿\n",
    "- ä¸Šä½ç‰¹å¾´é‡ã«å¿œã˜ã¦ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®å£²ä¸Šå·®åˆ†ã‚’ç®—å‡ºã—ã€å®Ÿè¡Œææ¡ˆã‚’å‡ºåŠ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç‰¹å¾´é‡ä½œæˆ\n",
    "def build_feature_dataset(df_all, stores):\n",
    "    d = df_all.copy()\n",
    "    if stores: d = d[d['store_id'].isin(stores)]\n",
    "    cand = [\n",
    "        'ç¥æ—¥ãƒ•ãƒ©ã‚°','åœŸæ›œãƒ•ãƒ©ã‚°','æ—¥æ›œãƒ•ãƒ©ã‚°','é€±æœ«ãƒ•ãƒ©ã‚°','å¹³æ—¥ãƒ•ãƒ©ã‚°','ä¼‘æ—¥ãƒ•ãƒ©ã‚°','é™é›¨ãƒ•ãƒ©ã‚°','å¼±é›¨','æ™®é€šé›¨','å¼·é›¨','è±ªé›¨',\n",
    "        'æœ€é«˜æ°—æ¸©','æœ€ä½æ°—æ¸©','å¹³å‡æ°—æ¸©','æ°—æ¸©å·®','é™æ°´é‡','æœ€é«˜æ°—æ¸©_t-1','æœ€é«˜æ°—æ¸©_t-7','æœ€ä½æ°—æ¸©_t-1','æœ€ä½æ°—æ¸©_t-7','å¹³å‡æ°—æ¸©_t-1','å¹³å‡æ°—æ¸©_t-7',\n",
    "        'æœ€é«˜æ°—æ¸©_MA3','æœ€é«˜æ°—æ¸©_MA7','å¹³å‡æ°—æ¸©_MA3','å¹³å‡æ°—æ¸©_MA7','æ°—æ¸©ãƒˆãƒ¬ãƒ³ãƒ‰_7d','å­£ç¯€å¤‰å‹•æŒ‡æ•°_æœˆ','å­£ç¯€å¤‰å‹•æŒ‡æ•°_é€±','é€±'\n",
    "    ]\n",
    "    present = [c for c in cand if c in d.columns]\n",
    "    keep = ['date','store_id','sales_amt'] + present\n",
    "    t = d[keep].copy()\n",
    "    # æ—¥ä»˜Ã—åº—èˆ—ã§ç‰¹å¾´é‡ã‚’ä»£è¡¨åŒ–ï¼ˆæ•°å€¤ã¯å¹³å‡ï¼‰ã€å£²ä¸Šã¯åˆè¨ˆ\n",
    "    Xdf = t.groupby(['date','store_id'], as_index=False)[present].mean() if present else t[['date','store_id']].drop_duplicates()\n",
    "    ydf = t.groupby(['date','store_id'], as_index=False)['sales_amt'].sum().rename(columns={'sales_amt':'y'})\n",
    "    feat = Xdf.merge(ydf, on=['date','store_id'], how='inner')\n",
    "    return feat, present\n",
    "\n",
    "# é‡è¦åº¦\n",
    "def rank_features(df_all, stores):\n",
    "    feat, cols = build_feature_dataset(df_all, stores)\n",
    "    if len(feat)<10 or not cols:\n",
    "        return pd.DataFrame(columns=['feature','importance'])\n",
    "    if SK_OK:\n",
    "        try:\n",
    "            model = RandomForestRegressor(n_estimators=300, random_state=42)\n",
    "            X = feat[cols].fillna(0).values\n",
    "            y = feat['y'].values\n",
    "            model.fit(X, y)\n",
    "            imp = model.feature_importances_\n",
    "            return pd.DataFrame({'feature': cols, 'importance': imp}).sort_values('importance', ascending=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # fallback: ç›¸é–¢\n",
    "    dfm = feat[cols+['y']].copy()\n",
    "    cor = dfm.corr(numeric_only=True)['y'].abs().drop('y', errors='ignore').sort_values(ascending=False)\n",
    "    return cor.reset_index().rename(columns={'index':'feature'})\n",
    "\n",
    "# ææ¡ˆï¼ˆãƒ•ãƒ©ã‚°åˆ¥upliftï¼‰\n",
    "def uplift_by_flag(df_all, stores, flag_col, top_n=5):\n",
    "    if flag_col not in df_all.columns: return pd.DataFrame()\n",
    "    d = df_all.copy()\n",
    "    if stores: d = d[d['store_id'].isin(stores)]\n",
    "    if 'category_l' not in d.columns: return pd.DataFrame()\n",
    "    base = d.groupby(['date','store_id','category_l'], as_index=False)['sales_amt'].sum()\n",
    "    flags = d[['date','store_id','category_l', flag_col]].drop_duplicates()\n",
    "    m = base.merge(flags, on=['date','store_id','category_l'], how='left')\n",
    "    a = m[m[flag_col]==1].groupby('category_l', as_index=False)['sales_amt'].mean()\n",
    "    b = m[m[flag_col]!=1].groupby('category_l', as_index=False)['sales_amt'].mean()\n",
    "    if a.empty or b.empty: return pd.DataFrame()\n",
    "    u = a.merge(b, on='category_l', suffixes=('_flag1','_flag0'))\n",
    "    u['uplift'] = (u['sales_amt_flag1']-u['sales_amt_flag0'])/(u['sales_amt_flag0']+1e-6)\n",
    "    return u.sort_values('uplift', ascending=False).head(top_n)\n",
    "\n",
    "# å®Ÿè¡Œ\n",
    "fi = rank_features(df, selected_stores if 'selected_stores' in globals() else [])\n",
    "display(fi.head(20))\n",
    "print(\"\\n--- ææ¡ˆï¼ˆã‚«ãƒ†ã‚´ãƒªåˆ¥ã®å£²ä¸Šä¸ŠæŒ¯ã‚ŒãŒå¤§ãã„é †ï¼‰---\")\n",
    "for f in [c for c in ['é™é›¨ãƒ•ãƒ©ã‚°','é€±æœ«ãƒ•ãƒ©ã‚°','çŒ›æš‘æ—¥','çœŸå¤æ—¥','å¤æ—¥','å†¬æ—¥','çœŸå†¬æ—¥','çµ¦æ–™æ—¥','çµ¦æ–™æ—¥ç›´å¾Œ','æœˆåˆ3æ—¥','æœˆæœ«3æ—¥'] if c in df.columns]:\n",
    "    top_u = uplift_by_flag(df, selected_stores if 'selected_stores' in globals() else [], f, top_n=5)\n",
    "    if not top_u.empty:\n",
    "        print(f\"[ææ¡ˆã‚­ãƒ¼: {f}]\")\n",
    "        display(top_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downlift_analysis_cell",
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# ğŸ“‰ Downliftåˆ†æï¼šå£²ä¸Šæ¸›å°‘è¦å› ã®ç‰¹å®š\n# ========================================\n\nprint('\\n' + '='*80)\nprint('ğŸ“‰ Downliftåˆ†æï¼šãƒˆãƒªã‚¬ãƒ¼åˆ¥ã®å£²ä¸Šæ¸›å°‘ã‚«ãƒ†ã‚´ãƒª')\nprint('='*80)\n\n# å£²ä¸Šåˆ—ã®ç¢ºèªï¼ˆå¤‰æ›å¾Œã®åˆ—åï¼‰\nif 'sales_amt' in df.columns:\n    sales_col = 'sales_amt'\nelif 'å£²ä¸Šé‡‘é¡' in df.columns:\n    sales_col = 'å£²ä¸Šé‡‘é¡'\nelif 'å£²ä¸Šæ•°é‡' in df.columns:\n    sales_col = 'å£²ä¸Šæ•°é‡'\nelse:\n    print('âš ï¸ å£²ä¸Šåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')\n    sales_col = None\n\n# ã‚«ãƒ†ã‚´ãƒªåˆ—ã®ç¢ºèª\nif 'category_l' in df.columns:\n    category_col = 'category_l'\nelif 'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šå¤§åˆ†é¡' in df.columns:\n    category_col = 'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šå¤§åˆ†é¡'\nelif 'category_m' in df.columns:\n    category_col = 'category_m'\nelse:\n    print('âš ï¸ ã‚«ãƒ†ã‚´ãƒªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')\n    category_col = None\n\nprint(f'ğŸ“Š ä½¿ç”¨ã™ã‚‹å£²ä¸Šåˆ—: {sales_col}')\nprint(f'ğŸ“Š ä½¿ç”¨ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªåˆ—: {category_col}')\n\nif sales_col is None or category_col is None:\n    print('\\nâš ï¸ å¿…è¦ãªåˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€Downliftåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™')\nelse:\n    # é™é›¨æ™‚ã«å£²ä¸ŠãŒæ¸›å°‘ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªï¼ˆdownliftï¼‰\n    downlift_results = {}\n\n    for trigger_col in ['é™é›¨ãƒ•ãƒ©ã‚°', 'é€±æœ«ãƒ•ãƒ©ã‚°', 'çŒ›æš‘æ—¥', 'çœŸå¤æ—¥', 'å¤æ—¥', 'çµ¦æ–™æ—¥', 'çµ¦æ–™æ—¥ç›´å¾Œ', 'æœˆåˆ3æ—¥', 'æœˆæœ«3æ—¥']:\n        if trigger_col not in df.columns:\n            continue\n\n        # ãƒˆãƒªã‚¬ãƒ¼ON/OFFæ™‚ã®å£²ä¸Šæ¯”è¼ƒï¼ˆdownlift: flag1 < flag0ï¼‰\n        try:\n            comparison = df.groupby([trigger_col, category_col], as_index=False)[sales_col].mean()\n\n            pivot = comparison.pivot_table(\n                index=category_col,\n                columns=trigger_col,\n                values=sales_col,\n                aggfunc='mean'\n            )\n\n            if 1.0 not in pivot.columns or 0.0 not in pivot.columns:\n                continue\n\n            # Downliftè¨ˆç®—ï¼š(OFF - ON) / OFF ï¼ˆæ­£ã®å€¤ = å£²ä¸Šæ¸›å°‘ï¼‰\n            pivot['downlift'] = (pivot[0.0] - pivot[1.0]) / (pivot[0.0] + 1e-6)\n\n            # å£²ä¸Šæ¸›å°‘ãŒå¤§ãã„é †ï¼ˆdownlift > 0.1 = 10%ä»¥ä¸Šæ¸›å°‘ï¼‰\n            significant_down = pivot[pivot['downlift'] > 0.1].copy()\n\n            if len(significant_down) > 0:\n                significant_down = significant_down.sort_values('downlift', ascending=False)\n                significant_down = significant_down.rename(columns={\n                    0.0: f'{sales_col}_flag0',\n                    1.0: f'{sales_col}_flag1'\n                })\n\n                downlift_results[trigger_col] = significant_down[[f'{sales_col}_flag1', f'{sales_col}_flag0', 'downlift']].head(5)\n        except Exception as e:\n            print(f'âš ï¸ {trigger_col} ã®åˆ†æã§ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}')\n            continue\n\n    # ========================================\n    # çµæœè¡¨ç¤º\n    # ========================================\n\n    if downlift_results:\n        print('\\n--- ææ¡ˆï¼ˆã‚«ãƒ†ã‚´ãƒªåˆ¥ã®å£²ä¸Šæ¸›å°‘ãŒå¤§ãã„é †ï¼‰---')\n        for trigger_key, result_df in downlift_results.items():\n            if len(result_df) > 0:\n                print(f'\\n[ææ¡ˆã‚­ãƒ¼: {trigger_key}]')\n                print(result_df.reset_index().to_string(index=False))\n\n        print('\\n' + '='*80)\n        print('ğŸ’¡ Downliftæ´»ç”¨æ–¹æ³•')\n        print('='*80)\n        print('''\nã€åœ¨åº«æœ€é©åŒ–ã€‘\n- é™é›¨æ™‚ã«å£²ä¸ŠãŒæ¸›ã‚‹ã‚«ãƒ†ã‚´ãƒª â†’ é›¨äºˆå ±ã®æ—¥ã¯ç™ºæ³¨ã‚’æ§ãˆã‚‹\n- é€±æœ«ã«å£²ä¸ŠãŒæ¸›ã‚‹ã‚«ãƒ†ã‚´ãƒª â†’ å¹³æ—¥ã«åœ¨åº«ã‚’åšãã™ã‚‹\n\nã€ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ä¼ç”»ã€‘\n- å£²ä¸Šæ¸›å°‘ã‚«ãƒ†ã‚´ãƒªã«å¯¾ã—ã¦ã€Œé›¨ã®æ—¥å‰²å¼•ã€ã€Œé€±æœ«ç‰¹å£²ã€ãªã©ã§éœ€è¦å–šèµ·\n- ä¾‹ï¼šé™é›¨æ™‚10%æ¸›ã®ã‚«ãƒ†ã‚´ãƒª â†’ é›¨ã®æ—¥10%å‰²å¼•ã§éœ€è¦ã‚’ç¶­æŒ\n\nã€è²©å£²è¨ˆç”»ã€‘\n- Downliftç‡ã‚’è€ƒæ…®ã—ãŸå£²ä¸Šäºˆæ¸¬ï¼ˆä¿å®ˆçš„è¦‹ç©ã‚‚ã‚Šï¼‰\n- ä¾‹ï¼šçµ¦æ–™æ—¥ç›´å¾Œ-15%ã€æœˆæœ«-20%ãªã©å­£ç¯€å¤‰å‹•ã‚’åæ˜ \n        ''')\n    else:\n        print('\\nâš ï¸ æœ‰æ„ãªdownliftï¼ˆå£²ä¸Šæ¸›å°‘10%ä»¥ä¸Šï¼‰ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ')\n\n    # ========================================\n    # ğŸ“Š Uplift vs Downliftã®çµ±åˆãƒ“ãƒ¥ãƒ¼\n    # ========================================\n\n    print('\\n' + '='*80)\n    print('ğŸ“Š çµ±åˆåˆ†æï¼šUplift + Downlift')\n    print('='*80)\n\n    # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®å£²ä¸Šå¤‰å‹•å¹…ã‚’è¨ˆç®—\n    category_volatility = []\n\n    for category in df[category_col].dropna().unique():\n        cat_data = df[df[category_col] == category]\n\n        max_uplifts = []\n        max_downlifts = []\n\n        for trigger_col in ['é™é›¨ãƒ•ãƒ©ã‚°', 'é€±æœ«ãƒ•ãƒ©ã‚°', 'çŒ›æš‘æ—¥', 'çœŸå¤æ—¥', 'å¤æ—¥']:\n            if trigger_col not in df.columns:\n                continue\n\n            try:\n                comparison = cat_data.groupby(trigger_col)[sales_col].mean()\n\n                if 1.0 in comparison.index and 0.0 in comparison.index:\n                    uplift = (comparison[1.0] - comparison[0.0]) / (comparison[0.0] + 1e-6)\n                    downlift = (comparison[0.0] - comparison[1.0]) / (comparison[0.0] + 1e-6)\n\n                    max_uplifts.append(max(0, uplift))\n                    max_downlifts.append(max(0, downlift))\n            except:\n                continue\n\n        if max_uplifts and max_downlifts:\n            category_volatility.append({\n                'ã‚«ãƒ†ã‚´ãƒª': category,\n                'æœ€å¤§Uplift': max(max_uplifts),\n                'æœ€å¤§Downlift': max(max_downlifts),\n                'å£²ä¸Šå¤‰å‹•å¹…': max(max_uplifts) + max(max_downlifts),\n                'äºˆæ¸¬é›£æ˜“åº¦': 'A:é«˜é›£æ˜“åº¦' if (max(max_uplifts) + max(max_downlifts)) > 1.5 else\n                             'B:ä¸­é›£æ˜“åº¦' if (max(max_uplifts) + max(max_downlifts)) > 0.5 else\n                             'C:ä½é›£æ˜“åº¦'\n            })\n\n    if category_volatility:\n        volatility_df = pd.DataFrame(category_volatility).sort_values('å£²ä¸Šå¤‰å‹•å¹…', ascending=False)\n\n        print('\\nã€ã‚«ãƒ†ã‚´ãƒªåˆ¥å£²ä¸Šå¤‰å‹•å¹…ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆTop 10ï¼‰ã€‘')\n        print(volatility_df.head(10).to_string(index=False))\n\n        print('\\nğŸ’¡ äºˆæ¸¬ãƒ¢ãƒ‡ãƒªãƒ³ã‚°æ¨å¥¨æˆ¦ç•¥:')\n        print('  A:é«˜é›£æ˜“åº¦ï¼ˆå¤‰å‹•å¹…1.5å€ä»¥ä¸Šï¼‰ â†’ å€‹åˆ¥ãƒ¢ãƒ‡ãƒ« + ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å¼·åŒ–')\n        print('  B:ä¸­é›£æ˜“åº¦ï¼ˆå¤‰å‹•å¹…0.5-1.5å€ï¼‰ â†’ ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ¢ãƒ‡ãƒ«')\n        print('  C:ä½é›£æ˜“åº¦ï¼ˆå¤‰å‹•å¹…0.5å€æœªæº€ï¼‰ â†’ çµ±åˆãƒ¢ãƒ‡ãƒ«ã§OK')\n\n        # CSVä¿å­˜\n        output_path = Path('output/category_uplift_downlift_analysis.csv')\n        volatility_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n        print(f'\\nâœ… åˆ†æçµæœã‚’ä¿å­˜: {output_path}')\n    else:\n        print('\\nâš ï¸ ã‚«ãƒ†ã‚´ãƒªåˆ¥å¤‰å‹•å¹…ã®è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸ')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆåº—èˆ—é¸æŠ + KPI/ABC/ç‰¹å¾´é‡/ææ¡ˆ/ã‚¢ãƒ©ãƒ¼ãƒˆï¼‰\n",
    "- åº—èˆ—ã®è¤‡æ•°é¸æŠã«å¯¾å¿œ\n",
    "- åœ¨åº«/åŸä¾¡ä¾å­˜ã®ãƒ“ãƒ¥ãƒ¼ã¯é™¤å¤–ï¼ˆæœªæä¾›ã®ãŸã‚ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åº—èˆ—è¤‡æ•°é¸æŠ â†’ 5ãƒ“ãƒ¥ãƒ¼ã‚’åŒæ™‚å±•é–‹ï¼ˆKPI/ABC/ç‰¹å¾´é‡/ææ¡ˆ/ã‚¢ãƒ©ãƒ¼ãƒˆï¼‰\n",
    "if WIDGETS and (not sales.empty):\n",
    "    store_opts = sorted(sales['store_id'].dropna().unique().tolist())\n",
    "    stores_ms = widgets.SelectMultiple(\n",
    "        options=store_opts, \n",
    "        value=tuple(store_opts[:1]) if store_opts else tuple(), \n",
    "        description='åº—èˆ—(è¤‡æ•°)',\n",
    "        rows=min(10, len(store_opts))\n",
    "    )\n",
    "    out = widgets.Output()\n",
    "    \n",
    "    print('='*60)\n",
    "    print('ğŸª åº—èˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆè¤‡æ•°é¸æŠå¯ï¼šCtrl/Cmd + ã‚¯ãƒªãƒƒã‚¯ï¼‰')\n",
    "    print('='*60)\n",
    "    display(stores_ms)\n",
    "    display(out)\n",
    "\n",
    "    def render_all(sel):\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            if not sel:\n",
    "                print('âš ï¸ åº—èˆ—ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ä¸Šã®ãƒªã‚¹ãƒˆã‹ã‚‰åº—èˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚')\n",
    "                return\n",
    "            \n",
    "            print('='*60)\n",
    "            print(f\"ğŸ“Š é¸æŠåº—èˆ—: {', '.join(map(str, sel))}\")\n",
    "            print('='*60)\n",
    "            \n",
    "            ss = sales[sales['store_id'].isin(sel)].copy() if sel else sales.copy()\n",
    "            \n",
    "            # 1) KPI\n",
    "            print('\\n' + '='*60)\n",
    "            print('[ãƒ“ãƒ¥ãƒ¼ 1/5] KPIï¼ˆæ—¥æ¬¡å£²ä¸Šï¼‰')\n",
    "            print('='*60)\n",
    "            print('ğŸ“Œ ä½•ã‚’è¦‹ã‚‹: å£²ä¸Šãƒ»å®¢æ•°ãƒ»å®¢å˜ä¾¡ã®æ—¥æ¬¡æ¨ç§»')\n",
    "            print('ğŸ“Œ åˆ¤æ–­åŸºæº–: å‰æ—¥æ¯”ã§å¤§ããä¸‹æŒ¯ã‚Œã—ã¦ã„ãŸã‚‰ã€æ¬¡ã®ã€Œã‚¢ãƒ©ãƒ¼ãƒˆã€ã€Œææ¡ˆã€ã§åŸå› ã‚’ç¢ºèª')\n",
    "            print('-'*60)\n",
    "            kpi = compute_daily_kpi(ss, product)\n",
    "            if kpi is not None and not kpi.empty and PLOTLY:\n",
    "                fig = px.line(kpi, x='date', y='sales_amt', color='store_id', title='å£²ä¸Šé‡‘é¡(æ—¥æ¬¡)')\n",
    "                fig.show()\n",
    "            else:\n",
    "                display(kpi.head() if kpi is not None else 'KPIãƒ‡ãƒ¼ã‚¿ç„¡ã—')\n",
    "            \n",
    "            # 2) ABC\n",
    "            print('\\n' + '='*60)\n",
    "            print('[ãƒ“ãƒ¥ãƒ¼ 2/5] ABCï¼ˆSKUå¯„ä¸ï¼‰')\n",
    "            print('='*60)\n",
    "            print('ğŸ“Œ ä½•ã‚’è¦‹ã‚‹: Aï¼ˆå£²ä¸Š80%ï¼‰ã€Bï¼ˆ80-95%ï¼‰ã€Cï¼ˆ95-100%ï¼‰ã®SKUåˆ†é¡')\n",
    "            print('ğŸ“Œ åˆ¤æ–­åŸºæº–: Aã¯çµ¶å¯¾ã«æ¬ å“ã•ã›ãªã„ã€Cã¯ç¸®å°ãƒ»å…¥æ›¿å€™è£œ')\n",
    "            print('-'*60)\n",
    "            ab = abc_analysis(ss, product)\n",
    "            if not ab.empty:\n",
    "                print(f'âœ… A: {len(ab[ab[\"ABC\"]==\"A\"])} SKU, B: {len(ab[ab[\"ABC\"]==\"B\"])} SKU, C: {len(ab[ab[\"ABC\"]==\"C\"])} SKU')\n",
    "                display(ab.head(30))\n",
    "            else:\n",
    "                print('ãƒ‡ãƒ¼ã‚¿ç„¡ã—')\n",
    "            \n",
    "            # 3) ç‰¹å¾´é‡é‡è¦åº¦\n",
    "            print('\\n' + '='*60)\n",
    "            print('[ãƒ“ãƒ¥ãƒ¼ 3/5] ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆå£²ä¸Šé‡‘é¡ï¼‰')\n",
    "            print('='*60)\n",
    "            print('ğŸ“Œ ä½•ã‚’è¦‹ã‚‹: å£²ä¸Šã«å½±éŸ¿ã™ã‚‹è¦å› ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆå¤©å€™ãƒ»æ›œæ—¥ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆç­‰ï¼‰')\n",
    "            print('ğŸ“Œ åˆ¤æ–­åŸºæº–: ä¸Šä½ã®è¦å› ã‚’ã€Œææ¡ˆã€ã¨çµ„ã¿åˆã‚ã›ã¦ã€é™³åˆ—ãƒ»å°ç·šã‚’èª¿æ•´')\n",
    "            print('-'*60)\n",
    "            fi = rank_features(df, sel)\n",
    "            if not fi.empty:\n",
    "                display(fi.head(20))\n",
    "            else:\n",
    "                print('ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼ˆ10è¡Œæœªæº€ï¼‰ã®ãŸã‚ã€ç‰¹å¾´é‡é‡è¦åº¦ã¯è¨ˆç®—ã§ãã¾ã›ã‚“')\n",
    "            \n",
    "            # 4) ææ¡ˆ\n",
    "            print('\\n' + '='*60)\n",
    "            print('[ãƒ“ãƒ¥ãƒ¼ 4/5] ææ¡ˆï¼ˆã‚«ãƒ†ã‚´ãƒªåˆ¥ã®å£²ä¸Šä¸ŠæŒ¯ã‚Œå€™è£œï¼‰')\n",
    "            print('='*60)\n",
    "            print('ğŸ“Œ ä½•ã‚’è¦‹ã‚‹: å„ãƒ•ãƒ©ã‚°ï¼ˆé™é›¨/é€±æœ«/çŒ›æš‘/çµ¦æ–™æ—¥ç­‰ï¼‰ã§å£²ä¸ŠãŒä¼¸ã³ã‚‹ã‚«ãƒ†ã‚´ãƒª')\n",
    "            print('ğŸ“Œ åˆ¤æ–­åŸºæº–: ä»Šæ—¥è©²å½“ã™ã‚‹ãƒ•ãƒ©ã‚°ã®ä¸Šä½ã‚«ãƒ†ã‚´ãƒªã‚’ã€é–‹åº—å‰ãƒ»ãƒ”ãƒ¼ã‚¯å‰ã«å‰å‡ºã—/ãƒ•ã‚§ãƒ¼ã‚¹å¢—')\n",
    "            print('-'*60)\n",
    "            flag_found = False\n",
    "            for f in [c for c in ['é™é›¨ãƒ•ãƒ©ã‚°','é€±æœ«ãƒ•ãƒ©ã‚°','çŒ›æš‘æ—¥','çœŸå¤æ—¥','å¤æ—¥','å†¬æ—¥','çœŸå†¬æ—¥','çµ¦æ–™æ—¥','çµ¦æ–™æ—¥ç›´å¾Œ','æœˆåˆ3æ—¥','æœˆæœ«3æ—¥'] if c in df.columns]:\n",
    "                u = uplift_by_flag(df, sel, f, top_n=5)\n",
    "                if not u.empty:\n",
    "                    flag_found = True\n",
    "                    print(f\"\\nğŸ”‘ [ææ¡ˆã‚­ãƒ¼: {f}]\")\n",
    "                    display(u)\n",
    "            if not flag_found:\n",
    "                print('ææ¡ˆãƒ‡ãƒ¼ã‚¿ãªã—ï¼ˆè©²å½“ãƒ•ãƒ©ã‚°åˆ—ãŒå­˜åœ¨ã—ãªã„ã€ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼‰')\n",
    "            \n",
    "            # 5) ã‚¢ãƒ©ãƒ¼ãƒˆ\n",
    "            print('\\n' + '='*60)\n",
    "            print('[ãƒ“ãƒ¥ãƒ¼ 5/5] ã‚¢ãƒ©ãƒ¼ãƒˆï¼ˆ3æ—¥MA vs 7æ—¥MAã®ä¹–é›¢ï¼‰')\n",
    "            print('='*60)\n",
    "            print('ğŸ“Œ ä½•ã‚’è¦‹ã‚‹: çŸ­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆ3æ—¥ï¼‰ã¨ä¸­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆ7æ—¥ï¼‰ã®å·®ãŒÂ±30%ã‚’è¶…ãˆãŸæ—¥')\n",
    "            print('ğŸ“Œ åˆ¤æ–­åŸºæº–: ä¸ŠæŒ¯ã‚Œâ†’è©²å½“ã‚«ãƒ†ã‚´ãƒªã®ãƒ•ã‚§ãƒ¼ã‚¹å¢—ã€ä¸‹æŒ¯ã‚Œâ†’é™³åˆ—ãƒ»å°ç·šãƒ»å€¤é ƒè¨´æ±‚ã®è¦‹ç›´ã—')\n",
    "            print('-'*60)\n",
    "            tmp = ss.groupby(['date','store_id'], as_index=False)['sales_amt'].sum().sort_values('date')\n",
    "            if tmp.empty:\n",
    "                print('ãƒ‡ãƒ¼ã‚¿ç„¡ã—')\n",
    "            else:\n",
    "                tmp['ma3'] = tmp.groupby('store_id')['sales_amt'].transform(lambda x: x.rolling(3, min_periods=1).mean())\n",
    "                tmp['ma7'] = tmp.groupby('store_id')['sales_amt'].transform(lambda x: x.rolling(7, min_periods=1).mean())\n",
    "                tmp['delta'] = (tmp['ma3'] - tmp['ma7'])/(tmp['ma7']+1e-6)\n",
    "                alert = tmp[tmp['delta'].abs()>0.3].copy()\n",
    "                if not alert.empty:\n",
    "                    print(f'âš ï¸ ã‚¢ãƒ©ãƒ¼ãƒˆç™ºç”Ÿ: {len(alert)} ä»¶ï¼ˆæœ€æ–°20ä»¶ã‚’è¡¨ç¤ºï¼‰')\n",
    "                    display(alert.tail(20))\n",
    "                else:\n",
    "                    print('âœ… ã‚¢ãƒ©ãƒ¼ãƒˆãªã—ï¼ˆä¹–é›¢ãŒ30%æœªæº€ï¼‰')\n",
    "            \n",
    "            print('\\n' + '='*60)\n",
    "            print('âœ… å…¨5ãƒ“ãƒ¥ãƒ¼ã®è¡¨ç¤ºå®Œäº†')\n",
    "            print('='*60)\n",
    "\n",
    "    def _on_change(change):\n",
    "        if change['name']=='value':\n",
    "            render_all(list(change['owner'].value))\n",
    "\n",
    "    stores_ms.observe(_on_change, names='value')\n",
    "    # åˆå›æç”»\n",
    "    render_all(list(stores_ms.value))\n",
    "else:\n",
    "    print('âš ï¸ widgetsæœªåˆ©ç”¨ã€ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ç„¡ã—ã®ãŸã‚ã€ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å±•é–‹ã¯ã‚¹ã‚­ãƒƒãƒ—')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive_impact_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ğŸ“Š åŒ…æ‹¬çš„å£²ä¸Šã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆåˆ†æï¼ˆå…¨ç‰¹å¾´é‡ï¼‰\n",
    "# ========================================\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('ğŸ“Š åŒ…æ‹¬çš„å£²ä¸Šã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆåˆ†æï¼ˆæ ¼ç´ç‡80%ä»¥ä¸Šã®å…¨ç‰¹å¾´é‡ï¼‰')\n",
    "print('='*80)\n",
    "\n",
    "# å£²ä¸Šåˆ—ã®ç¢ºèª\n",
    "sales_col = 'å£²ä¸Šé‡‘é¡' if 'å£²ä¸Šé‡‘é¡' in df.columns else 'å£²ä¸Šæ•°é‡'\n",
    "print(f'\\nğŸ“Š å£²ä¸ŠæŒ‡æ¨™: {sales_col}')\n",
    "\n",
    "# ========================================\n",
    "# ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æ\n",
    "# ========================================\n",
    "\n",
    "# å„åˆ—ã®æ ¼ç´ç‡è¨ˆç®—\n",
    "completeness = {}\n",
    "for col in df.columns:\n",
    "    non_null_count = df[col].notna().sum()\n",
    "    completeness[col] = non_null_count / len(df)\n",
    "\n",
    "# æ ¼ç´ç‡80%ä»¥ä¸Šã®ã‚«ãƒ©ãƒ æŠ½å‡º\n",
    "high_quality_cols = [col for col, rate in completeness.items() if rate >= 0.8]\n",
    "\n",
    "print(f'âœ… æ ¼ç´ç‡80%ä»¥ä¸Šã®ã‚«ãƒ©ãƒ : {len(high_quality_cols)}å€‹')\n",
    "\n",
    "# åˆ†æå¯¾è±¡ã‚«ãƒ©ãƒ ï¼ˆé™¤å¤–åˆ—ã‚’é™¤ãï¼‰\n",
    "exclude_cols = ['åº—èˆ—', 'å•†å“å', 'æ—¥ä»˜', sales_col, 'date', 'store_id', 'sku_id',\n",
    "               'category_l', 'category_m', 'category_s',\n",
    "               'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šå¤§åˆ†é¡', 'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šä¸­åˆ†é¡', 'ãƒ•ã‚§ã‚¤ã‚¹ããã‚Šå°åˆ†é¡']\n",
    "\n",
    "analysis_cols = [col for col in high_quality_cols if col not in exclude_cols]\n",
    "print(f'   åˆ†æå¯¾è±¡ã‚«ãƒ©ãƒ : {len(analysis_cols)}å€‹')\n",
    "\n",
    "# ========================================\n",
    "# å£²ä¸Šã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆè¨ˆç®—\n",
    "# ========================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "impact_results = []\n",
    "\n",
    "for col in analysis_cols:\n",
    "    try:\n",
    "        col_data = df[col].dropna()\n",
    "\n",
    "        if len(col_data) < 100:\n",
    "            continue\n",
    "\n",
    "        # æ•°å€¤å‹ã®å ´åˆ\n",
    "        if pd.api.types.is_numeric_dtype(col_data):\n",
    "            unique_vals = col_data.unique()\n",
    "\n",
    "            # ãƒã‚¤ãƒŠãƒªãƒ•ãƒ©ã‚°ï¼ˆ0/1ï¼‰\n",
    "            if len(unique_vals) == 2 and set(unique_vals).issubset({0, 1, 0.0, 1.0}):\n",
    "                flag_on = df[df[col] == 1][sales_col].mean()\n",
    "                flag_off = df[df[col] == 0][sales_col].mean()\n",
    "\n",
    "                if flag_off > 0:\n",
    "                    impact = (flag_on - flag_off) / flag_off\n",
    "                    impact_abs = flag_on - flag_off\n",
    "\n",
    "                    impact_results.append({\n",
    "                        'ç‰¹å¾´é‡': col,\n",
    "                        'åˆ†æã‚¿ã‚¤ãƒ—': 'ãƒã‚¤ãƒŠãƒªãƒ•ãƒ©ã‚°',\n",
    "                        'å£²ä¸Š_ON': flag_on,\n",
    "                        'å£²ä¸Š_OFF': flag_off,\n",
    "                        'ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡': impact,\n",
    "                        'ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆçµ¶å¯¾å€¤': impact_abs,\n",
    "                        'ã‚µãƒ³ãƒ—ãƒ«æ•°_ON': (df[col] == 1).sum(),\n",
    "                        'ã‚µãƒ³ãƒ—ãƒ«æ•°_OFF': (df[col] == 0).sum()\n",
    "                    })\n",
    "\n",
    "            # é€£ç¶šå€¤ï¼ˆç›¸é–¢åˆ†æï¼‰\n",
    "            elif len(unique_vals) > 10:\n",
    "                correlation = df[[col, sales_col]].corr().iloc[0, 1]\n",
    "\n",
    "                if not np.isnan(correlation):\n",
    "                    q75 = col_data.quantile(0.75)\n",
    "                    q25 = col_data.quantile(0.25)\n",
    "\n",
    "                    sales_high = df[df[col] >= q75][sales_col].mean()\n",
    "                    sales_low = df[df[col] <= q25][sales_col].mean()\n",
    "\n",
    "                    if sales_low > 0:\n",
    "                        impact = (sales_high - sales_low) / sales_low\n",
    "                        impact_abs = sales_high - sales_low\n",
    "\n",
    "                        impact_results.append({\n",
    "                            'ç‰¹å¾´é‡': col,\n",
    "                            'åˆ†æã‚¿ã‚¤ãƒ—': 'é€£ç¶šå€¤ï¼ˆQ75 vs Q25ï¼‰',\n",
    "                            'å£²ä¸Š_é«˜': sales_high,\n",
    "                            'å£²ä¸Š_ä½': sales_low,\n",
    "                            'ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡': impact,\n",
    "                            'ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆçµ¶å¯¾å€¤': impact_abs,\n",
    "                            'ç›¸é–¢ä¿‚æ•°': correlation\n",
    "                        })\n",
    "\n",
    "            # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«æ•°å€¤ï¼ˆ3-10ç¨®é¡ï¼‰\n",
    "            elif 3 <= len(unique_vals) <= 10:\n",
    "                category_sales = df.groupby(col)[sales_col].agg(['mean', 'count'])\n",
    "\n",
    "                if len(category_sales) >= 2:\n",
    "                    max_sales = category_sales['mean'].max()\n",
    "                    min_sales = category_sales['mean'].min()\n",
    "\n",
    "                    if min_sales > 0:\n",
    "                        impact = (max_sales - min_sales) / min_sales\n",
    "                        impact_abs = max_sales - min_sales\n",
    "\n",
    "                        impact_results.append({\n",
    "                            'ç‰¹å¾´é‡': col,\n",
    "                            'åˆ†æã‚¿ã‚¤ãƒ—': 'ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«',\n",
    "                            'å£²ä¸Š_æœ€å¤§': max_sales,\n",
    "                            'å£²ä¸Š_æœ€å°': min_sales,\n",
    "                            'ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡': impact,\n",
    "                            'ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆçµ¶å¯¾å€¤': impact_abs,\n",
    "                            'ã‚«ãƒ†ã‚´ãƒªæ•°': len(unique_vals)\n",
    "                        })\n",
    "\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "print(f'\\nâœ… åˆ†æå®Œäº†: {len(impact_results)}å€‹ã®ç‰¹å¾´é‡ã‚’åˆ†æ')\n",
    "\n",
    "# ========================================\n",
    "# çµæœé›†è¨ˆ\n",
    "# ========================================\n",
    "\n",
    "if len(impact_results) > 0:\n",
    "    impact_df = pd.DataFrame(impact_results)\n",
    "    impact_df['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡_çµ¶å¯¾å€¤'] = impact_df['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡'].abs()\n",
    "    impact_df = impact_df.sort_values('ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡_çµ¶å¯¾å€¤', ascending=False)\n",
    "\n",
    "    print('\\n' + '='*80)\n",
    "    print('ğŸ† å£²ä¸Šã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆãƒ©ãƒ³ã‚­ãƒ³ã‚° Top 20')\n",
    "    print('='*80)\n",
    "\n",
    "    for idx, row in impact_df.head(20).iterrows():\n",
    "        impact_sign = '+' if row['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡'] > 0 else ''\n",
    "        print(f\"{row['ç‰¹å¾´é‡']:30s} {impact_sign}{row['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡']:.2%} \"              f\"({impact_sign}{row['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆçµ¶å¯¾å€¤']:,.0f}å††) [{row['åˆ†æã‚¿ã‚¤ãƒ—']}]\")\n",
    "\n",
    "    # æ­£ã®ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆTop 5\n",
    "    positive_impact = impact_df[impact_df['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡'] > 0].head(5)\n",
    "    print('\\n' + '='*80)\n",
    "    print('âœ… å£²ä¸Šå¢—åŠ è¦å›  Top 5')\n",
    "    print('='*80)\n",
    "    for idx, row in positive_impact.iterrows():\n",
    "        print(f\"  {row['ç‰¹å¾´é‡']}: {row['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡']:+.2%} ({row['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆçµ¶å¯¾å€¤']:+,.0f}å††)\")\n",
    "\n",
    "    # è² ã®ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆTop 5\n",
    "    negative_impact = impact_df[impact_df['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡'] < 0].sort_values('ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡').head(5)\n",
    "    print('\\n' + '='*80)\n",
    "    print('âš ï¸ å£²ä¸Šæ¸›å°‘è¦å›  Top 5')\n",
    "    print('='*80)\n",
    "    for idx, row in negative_impact.iterrows():\n",
    "        print(f\"  {row['ç‰¹å¾´é‡']}: {row['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡']:+.2%} ({row['ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆçµ¶å¯¾å€¤']:+,.0f}å††)\")\n",
    "\n",
    "    print('\\n' + '='*80)\n",
    "    print('ğŸ¯ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³')\n",
    "    print('='*80)\n",
    "    print('''\n",
    "ã€å£²ä¸Šæœ€å¤§åŒ–æ–½ç­–ã€‘\n",
    "1. æ­£ã®ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆè¦å› ã‚’æœ€å¤§åŒ–\n",
    "   - è©²å½“æ¡ä»¶ã§ã®åœ¨åº«ç¢ºä¿\n",
    "   - ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³å¼·åŒ–\n",
    "   - ä¾¡æ ¼æœ€é©åŒ–\n",
    "\n",
    "2. è² ã®ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆè¦å› ã‚’ç·©å’Œ\n",
    "   - å£²ä¸Šæ¸›å°‘æ™‚ã®ç‰¹åˆ¥æ–½ç­–\n",
    "   - ä»£æ›¿å•†å“ã®ææ¡ˆ\n",
    "   - å‰²å¼•ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³\n",
    "\n",
    "3. äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã¸ã®æ´»ç”¨\n",
    "   - ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆç‡ä¸Šä½ã®ç‰¹å¾´é‡ã‚’é‡ç‚¹ä½¿ç”¨\n",
    "   - ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã§ç›¸äº’ä½œç”¨é …ä½œæˆ\n",
    "   - ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«é‡è¦ç‰¹å¾´é‡ã‚’é¸å®š\n",
    "    ''')\n",
    "\n",
    "else:\n",
    "    print('\\nâš ï¸ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆåˆ†æçµæœãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸ')\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}